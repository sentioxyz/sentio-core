diff -ruN a/ecc/bls12-377/fp/element_amd64.s b/ecc/bls12-377/fp/element_amd64.s
--- a/ecc/bls12-377/fp/element_amd64.s
+++ b/ecc/bls12-377/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 18408829383245254329
-#include "../../../field/asm/element_6w/element_6w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-377/fp/element_arm64.s b/ecc/bls12-377/fp/element_arm64.s
--- a/ecc/bls12-377/fp/element_arm64.s
+++ b/ecc/bls12-377/fp/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 15397482240260640864
-#include "../../../field/asm/element_6w/element_6w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-377/fr/element_amd64.s b/ecc/bls12-377/fr/element_amd64.s
--- a/ecc/bls12-377/fr/element_amd64.s
+++ b/ecc/bls12-377/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-377/fr/element_arm64.s b/ecc/bls12-377/fr/element_arm64.s
--- a/ecc/bls12-377/fr/element_arm64.s
+++ b/ecc/bls12-377/fr/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-377/internal/fptower/e2_amd64.s b/ecc/bls12-377/internal/fptower/e2_amd64.s
--- a/ecc/bls12-377/internal/fptower/e2_amd64.s
+++ b/ecc/bls12-377/internal/fptower/e2_amd64.s
@@ -1,297 +1,1 @@
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define REDUCE(ra0, ra1, ra2, ra3, ra4, ra5, rb0, rb1, rb2, rb3, rb4, rb5) \
-	MOVQ    ra0, rb0;              \
-	SUBQ    ·qElement(SB), ra0;    \
-	MOVQ    ra1, rb1;              \
-	SBBQ    ·qElement+8(SB), ra1;  \
-	MOVQ    ra2, rb2;              \
-	SBBQ    ·qElement+16(SB), ra2; \
-	MOVQ    ra3, rb3;              \
-	SBBQ    ·qElement+24(SB), ra3; \
-	MOVQ    ra4, rb4;              \
-	SBBQ    ·qElement+32(SB), ra4; \
-	MOVQ    ra5, rb5;              \
-	SBBQ    ·qElement+40(SB), ra5; \
-	CMOVQCS rb0, ra0;              \
-	CMOVQCS rb1, ra1;              \
-	CMOVQCS rb2, ra2;              \
-	CMOVQCS rb3, ra3;              \
-	CMOVQCS rb4, ra4;              \
-	CMOVQCS rb5, ra5;              \
-
-TEXT ·addE2(SB), NOSPLIT, $0-24
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), BX
-	MOVQ 8(AX), SI
-	MOVQ 16(AX), DI
-	MOVQ 24(AX), R8
-	MOVQ 32(AX), R9
-	MOVQ 40(AX), R10
-	MOVQ y+16(FP), DX
-	ADDQ 0(DX), BX
-	ADCQ 8(DX), SI
-	ADCQ 16(DX), DI
-	ADCQ 24(DX), R8
-	ADCQ 32(DX), R9
-	ADCQ 40(DX), R10
-
-	// reduce element(BX,SI,DI,R8,R9,R10) using temp registers (R11,R12,R13,R14,R15,s0-8(SP))
-	REDUCE(BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP))
-
-	MOVQ res+0(FP), CX
-	MOVQ BX, 0(CX)
-	MOVQ SI, 8(CX)
-	MOVQ DI, 16(CX)
-	MOVQ R8, 24(CX)
-	MOVQ R9, 32(CX)
-	MOVQ R10, 40(CX)
-	MOVQ 48(AX), BX
-	MOVQ 56(AX), SI
-	MOVQ 64(AX), DI
-	MOVQ 72(AX), R8
-	MOVQ 80(AX), R9
-	MOVQ 88(AX), R10
-	ADDQ 48(DX), BX
-	ADCQ 56(DX), SI
-	ADCQ 64(DX), DI
-	ADCQ 72(DX), R8
-	ADCQ 80(DX), R9
-	ADCQ 88(DX), R10
-
-	// reduce element(BX,SI,DI,R8,R9,R10) using temp registers (R11,R12,R13,R14,R15,s0-8(SP))
-	REDUCE(BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP))
-
-	MOVQ BX, 48(CX)
-	MOVQ SI, 56(CX)
-	MOVQ DI, 64(CX)
-	MOVQ R8, 72(CX)
-	MOVQ R9, 80(CX)
-	MOVQ R10, 88(CX)
-	RET
-
-TEXT ·doubleE2(SB), NOSPLIT, $0-16
-	MOVQ res+0(FP), DX
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), CX
-	MOVQ 8(AX), BX
-	MOVQ 16(AX), SI
-	MOVQ 24(AX), DI
-	MOVQ 32(AX), R8
-	MOVQ 40(AX), R9
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-
-	// reduce element(CX,BX,SI,DI,R8,R9) using temp registers (R10,R11,R12,R13,R14,R15)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15)
-
-	MOVQ CX, 0(DX)
-	MOVQ BX, 8(DX)
-	MOVQ SI, 16(DX)
-	MOVQ DI, 24(DX)
-	MOVQ R8, 32(DX)
-	MOVQ R9, 40(DX)
-	MOVQ 48(AX), CX
-	MOVQ 56(AX), BX
-	MOVQ 64(AX), SI
-	MOVQ 72(AX), DI
-	MOVQ 80(AX), R8
-	MOVQ 88(AX), R9
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-
-	// reduce element(CX,BX,SI,DI,R8,R9) using temp registers (R10,R11,R12,R13,R14,R15)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15)
-
-	MOVQ CX, 48(DX)
-	MOVQ BX, 56(DX)
-	MOVQ SI, 64(DX)
-	MOVQ DI, 72(DX)
-	MOVQ R8, 80(DX)
-	MOVQ R9, 88(DX)
-	RET
-
-TEXT ·subE2(SB), NOSPLIT, $0-24
-	XORQ    R9, R9
-	MOVQ    x+8(FP), R8
-	MOVQ    0(R8), AX
-	MOVQ    8(R8), DX
-	MOVQ    16(R8), CX
-	MOVQ    24(R8), BX
-	MOVQ    32(R8), SI
-	MOVQ    40(R8), DI
-	MOVQ    y+16(FP), R8
-	SUBQ    0(R8), AX
-	SBBQ    8(R8), DX
-	SBBQ    16(R8), CX
-	SBBQ    24(R8), BX
-	SBBQ    32(R8), SI
-	SBBQ    40(R8), DI
-	MOVQ    x+8(FP), R8
-	MOVQ    $0x8508c00000000001, R10
-	MOVQ    $0x170b5d4430000000, R11
-	MOVQ    $0x1ef3622fba094800, R12
-	MOVQ    $0x1a22d9f300f5138f, R13
-	MOVQ    $0xc63b05c06ca1493b, R14
-	MOVQ    $0x01ae3a4617c510ea, R15
-	CMOVQCC R9, R10
-	CMOVQCC R9, R11
-	CMOVQCC R9, R12
-	CMOVQCC R9, R13
-	CMOVQCC R9, R14
-	CMOVQCC R9, R15
-	ADDQ    R10, AX
-	ADCQ    R11, DX
-	ADCQ    R12, CX
-	ADCQ    R13, BX
-	ADCQ    R14, SI
-	ADCQ    R15, DI
-	MOVQ    res+0(FP), R10
-	MOVQ    AX, 0(R10)
-	MOVQ    DX, 8(R10)
-	MOVQ    CX, 16(R10)
-	MOVQ    BX, 24(R10)
-	MOVQ    SI, 32(R10)
-	MOVQ    DI, 40(R10)
-	MOVQ    48(R8), AX
-	MOVQ    56(R8), DX
-	MOVQ    64(R8), CX
-	MOVQ    72(R8), BX
-	MOVQ    80(R8), SI
-	MOVQ    88(R8), DI
-	MOVQ    y+16(FP), R8
-	SUBQ    48(R8), AX
-	SBBQ    56(R8), DX
-	SBBQ    64(R8), CX
-	SBBQ    72(R8), BX
-	SBBQ    80(R8), SI
-	SBBQ    88(R8), DI
-	MOVQ    $0x8508c00000000001, R11
-	MOVQ    $0x170b5d4430000000, R12
-	MOVQ    $0x1ef3622fba094800, R13
-	MOVQ    $0x1a22d9f300f5138f, R14
-	MOVQ    $0xc63b05c06ca1493b, R15
-	MOVQ    $0x01ae3a4617c510ea, R10
-	CMOVQCC R9, R11
-	CMOVQCC R9, R12
-	CMOVQCC R9, R13
-	CMOVQCC R9, R14
-	CMOVQCC R9, R15
-	CMOVQCC R9, R10
-	ADDQ    R11, AX
-	ADCQ    R12, DX
-	ADCQ    R13, CX
-	ADCQ    R14, BX
-	ADCQ    R15, SI
-	ADCQ    R10, DI
-	MOVQ    res+0(FP), R8
-	MOVQ    AX, 48(R8)
-	MOVQ    DX, 56(R8)
-	MOVQ    CX, 64(R8)
-	MOVQ    BX, 72(R8)
-	MOVQ    SI, 80(R8)
-	MOVQ    DI, 88(R8)
-	RET
-
-TEXT ·negE2(SB), NOSPLIT, $0-16
-	MOVQ  res+0(FP), DX
-	MOVQ  x+8(FP), AX
-	MOVQ  0(AX), BX
-	MOVQ  8(AX), SI
-	MOVQ  16(AX), DI
-	MOVQ  24(AX), R8
-	MOVQ  32(AX), R9
-	MOVQ  40(AX), R10
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	ORQ   R9, AX
-	ORQ   R10, AX
-	TESTQ AX, AX
-	JNE   l1
-	MOVQ  AX, 0(DX)
-	MOVQ  AX, 8(DX)
-	MOVQ  AX, 16(DX)
-	MOVQ  AX, 24(DX)
-	MOVQ  AX, 32(DX)
-	MOVQ  AX, 40(DX)
-	JMP   l3
-
-l1:
-	MOVQ $0x8508c00000000001, CX
-	SUBQ BX, CX
-	MOVQ CX, 0(DX)
-	MOVQ $0x170b5d4430000000, CX
-	SBBQ SI, CX
-	MOVQ CX, 8(DX)
-	MOVQ $0x1ef3622fba094800, CX
-	SBBQ DI, CX
-	MOVQ CX, 16(DX)
-	MOVQ $0x1a22d9f300f5138f, CX
-	SBBQ R8, CX
-	MOVQ CX, 24(DX)
-	MOVQ $0xc63b05c06ca1493b, CX
-	SBBQ R9, CX
-	MOVQ CX, 32(DX)
-	MOVQ $0x01ae3a4617c510ea, CX
-	SBBQ R10, CX
-	MOVQ CX, 40(DX)
-
-l3:
-	MOVQ  x+8(FP), AX
-	MOVQ  48(AX), BX
-	MOVQ  56(AX), SI
-	MOVQ  64(AX), DI
-	MOVQ  72(AX), R8
-	MOVQ  80(AX), R9
-	MOVQ  88(AX), R10
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	ORQ   R9, AX
-	ORQ   R10, AX
-	TESTQ AX, AX
-	JNE   l2
-	MOVQ  AX, 48(DX)
-	MOVQ  AX, 56(DX)
-	MOVQ  AX, 64(DX)
-	MOVQ  AX, 72(DX)
-	MOVQ  AX, 80(DX)
-	MOVQ  AX, 88(DX)
-	RET
-
-l2:
-	MOVQ $0x8508c00000000001, CX
-	SUBQ BX, CX
-	MOVQ CX, 48(DX)
-	MOVQ $0x170b5d4430000000, CX
-	SBBQ SI, CX
-	MOVQ CX, 56(DX)
-	MOVQ $0x1ef3622fba094800, CX
-	SBBQ DI, CX
-	MOVQ CX, 64(DX)
-	MOVQ $0x1a22d9f300f5138f, CX
-	SBBQ R8, CX
-	MOVQ CX, 72(DX)
-	MOVQ $0xc63b05c06ca1493b, CX
-	SBBQ R9, CX
-	MOVQ CX, 80(DX)
-	MOVQ $0x01ae3a4617c510ea, CX
-	SBBQ R10, CX
-	MOVQ CX, 88(DX)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-381/fp/element_amd64.s b/ecc/bls12-381/fp/element_amd64.s
--- a/ecc/bls12-381/fp/element_amd64.s
+++ b/ecc/bls12-381/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 18408829383245254329
-#include "../../../field/asm/element_6w/element_6w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-381/fp/element_arm64.s b/ecc/bls12-381/fp/element_arm64.s
--- a/ecc/bls12-381/fp/element_arm64.s
+++ b/ecc/bls12-381/fp/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 15397482240260640864
-#include "../../../field/asm/element_6w/element_6w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-381/fr/element_amd64.s b/ecc/bls12-381/fr/element_amd64.s
--- a/ecc/bls12-381/fr/element_amd64.s
+++ b/ecc/bls12-381/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-381/fr/element_arm64.s b/ecc/bls12-381/fr/element_arm64.s
--- a/ecc/bls12-381/fr/element_arm64.s
+++ b/ecc/bls12-381/fr/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls12-381/internal/fptower/e2_amd64.s b/ecc/bls12-381/internal/fptower/e2_amd64.s
--- a/ecc/bls12-381/internal/fptower/e2_amd64.s
+++ b/ecc/bls12-381/internal/fptower/e2_amd64.s
@@ -1,867 +1,1 @@
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define REDUCE(ra0, ra1, ra2, ra3, ra4, ra5, rb0, rb1, rb2, rb3, rb4, rb5) \
-	MOVQ    ra0, rb0;              \
-	SUBQ    ·qElement(SB), ra0;    \
-	MOVQ    ra1, rb1;              \
-	SBBQ    ·qElement+8(SB), ra1;  \
-	MOVQ    ra2, rb2;              \
-	SBBQ    ·qElement+16(SB), ra2; \
-	MOVQ    ra3, rb3;              \
-	SBBQ    ·qElement+24(SB), ra3; \
-	MOVQ    ra4, rb4;              \
-	SBBQ    ·qElement+32(SB), ra4; \
-	MOVQ    ra5, rb5;              \
-	SBBQ    ·qElement+40(SB), ra5; \
-	CMOVQCS rb0, ra0;              \
-	CMOVQCS rb1, ra1;              \
-	CMOVQCS rb2, ra2;              \
-	CMOVQCS rb3, ra3;              \
-	CMOVQCS rb4, ra4;              \
-	CMOVQCS rb5, ra5;              \
-
-TEXT ·addE2(SB), NOSPLIT, $0-24
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), BX
-	MOVQ 8(AX), SI
-	MOVQ 16(AX), DI
-	MOVQ 24(AX), R8
-	MOVQ 32(AX), R9
-	MOVQ 40(AX), R10
-	MOVQ y+16(FP), DX
-	ADDQ 0(DX), BX
-	ADCQ 8(DX), SI
-	ADCQ 16(DX), DI
-	ADCQ 24(DX), R8
-	ADCQ 32(DX), R9
-	ADCQ 40(DX), R10
-
-	// reduce element(BX,SI,DI,R8,R9,R10) using temp registers (R11,R12,R13,R14,R15,s0-8(SP))
-	REDUCE(BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP))
-
-	MOVQ res+0(FP), CX
-	MOVQ BX, 0(CX)
-	MOVQ SI, 8(CX)
-	MOVQ DI, 16(CX)
-	MOVQ R8, 24(CX)
-	MOVQ R9, 32(CX)
-	MOVQ R10, 40(CX)
-	MOVQ 48(AX), BX
-	MOVQ 56(AX), SI
-	MOVQ 64(AX), DI
-	MOVQ 72(AX), R8
-	MOVQ 80(AX), R9
-	MOVQ 88(AX), R10
-	ADDQ 48(DX), BX
-	ADCQ 56(DX), SI
-	ADCQ 64(DX), DI
-	ADCQ 72(DX), R8
-	ADCQ 80(DX), R9
-	ADCQ 88(DX), R10
-
-	// reduce element(BX,SI,DI,R8,R9,R10) using temp registers (R11,R12,R13,R14,R15,s0-8(SP))
-	REDUCE(BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP))
-
-	MOVQ BX, 48(CX)
-	MOVQ SI, 56(CX)
-	MOVQ DI, 64(CX)
-	MOVQ R8, 72(CX)
-	MOVQ R9, 80(CX)
-	MOVQ R10, 88(CX)
-	RET
-
-TEXT ·doubleE2(SB), NOSPLIT, $0-16
-	MOVQ res+0(FP), DX
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), CX
-	MOVQ 8(AX), BX
-	MOVQ 16(AX), SI
-	MOVQ 24(AX), DI
-	MOVQ 32(AX), R8
-	MOVQ 40(AX), R9
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-
-	// reduce element(CX,BX,SI,DI,R8,R9) using temp registers (R10,R11,R12,R13,R14,R15)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15)
-
-	MOVQ CX, 0(DX)
-	MOVQ BX, 8(DX)
-	MOVQ SI, 16(DX)
-	MOVQ DI, 24(DX)
-	MOVQ R8, 32(DX)
-	MOVQ R9, 40(DX)
-	MOVQ 48(AX), CX
-	MOVQ 56(AX), BX
-	MOVQ 64(AX), SI
-	MOVQ 72(AX), DI
-	MOVQ 80(AX), R8
-	MOVQ 88(AX), R9
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-
-	// reduce element(CX,BX,SI,DI,R8,R9) using temp registers (R10,R11,R12,R13,R14,R15)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15)
-
-	MOVQ CX, 48(DX)
-	MOVQ BX, 56(DX)
-	MOVQ SI, 64(DX)
-	MOVQ DI, 72(DX)
-	MOVQ R8, 80(DX)
-	MOVQ R9, 88(DX)
-	RET
-
-TEXT ·subE2(SB), NOSPLIT, $0-24
-	XORQ    R9, R9
-	MOVQ    x+8(FP), R8
-	MOVQ    0(R8), AX
-	MOVQ    8(R8), DX
-	MOVQ    16(R8), CX
-	MOVQ    24(R8), BX
-	MOVQ    32(R8), SI
-	MOVQ    40(R8), DI
-	MOVQ    y+16(FP), R8
-	SUBQ    0(R8), AX
-	SBBQ    8(R8), DX
-	SBBQ    16(R8), CX
-	SBBQ    24(R8), BX
-	SBBQ    32(R8), SI
-	SBBQ    40(R8), DI
-	MOVQ    x+8(FP), R8
-	MOVQ    $0xb9feffffffffaaab, R10
-	MOVQ    $0x1eabfffeb153ffff, R11
-	MOVQ    $0x6730d2a0f6b0f624, R12
-	MOVQ    $0x64774b84f38512bf, R13
-	MOVQ    $0x4b1ba7b6434bacd7, R14
-	MOVQ    $0x1a0111ea397fe69a, R15
-	CMOVQCC R9, R10
-	CMOVQCC R9, R11
-	CMOVQCC R9, R12
-	CMOVQCC R9, R13
-	CMOVQCC R9, R14
-	CMOVQCC R9, R15
-	ADDQ    R10, AX
-	ADCQ    R11, DX
-	ADCQ    R12, CX
-	ADCQ    R13, BX
-	ADCQ    R14, SI
-	ADCQ    R15, DI
-	MOVQ    res+0(FP), R10
-	MOVQ    AX, 0(R10)
-	MOVQ    DX, 8(R10)
-	MOVQ    CX, 16(R10)
-	MOVQ    BX, 24(R10)
-	MOVQ    SI, 32(R10)
-	MOVQ    DI, 40(R10)
-	MOVQ    48(R8), AX
-	MOVQ    56(R8), DX
-	MOVQ    64(R8), CX
-	MOVQ    72(R8), BX
-	MOVQ    80(R8), SI
-	MOVQ    88(R8), DI
-	MOVQ    y+16(FP), R8
-	SUBQ    48(R8), AX
-	SBBQ    56(R8), DX
-	SBBQ    64(R8), CX
-	SBBQ    72(R8), BX
-	SBBQ    80(R8), SI
-	SBBQ    88(R8), DI
-	MOVQ    $0xb9feffffffffaaab, R11
-	MOVQ    $0x1eabfffeb153ffff, R12
-	MOVQ    $0x6730d2a0f6b0f624, R13
-	MOVQ    $0x64774b84f38512bf, R14
-	MOVQ    $0x4b1ba7b6434bacd7, R15
-	MOVQ    $0x1a0111ea397fe69a, R10
-	CMOVQCC R9, R11
-	CMOVQCC R9, R12
-	CMOVQCC R9, R13
-	CMOVQCC R9, R14
-	CMOVQCC R9, R15
-	CMOVQCC R9, R10
-	ADDQ    R11, AX
-	ADCQ    R12, DX
-	ADCQ    R13, CX
-	ADCQ    R14, BX
-	ADCQ    R15, SI
-	ADCQ    R10, DI
-	MOVQ    res+0(FP), R8
-	MOVQ    AX, 48(R8)
-	MOVQ    DX, 56(R8)
-	MOVQ    CX, 64(R8)
-	MOVQ    BX, 72(R8)
-	MOVQ    SI, 80(R8)
-	MOVQ    DI, 88(R8)
-	RET
-
-TEXT ·negE2(SB), NOSPLIT, $0-16
-	MOVQ  res+0(FP), DX
-	MOVQ  x+8(FP), AX
-	MOVQ  0(AX), BX
-	MOVQ  8(AX), SI
-	MOVQ  16(AX), DI
-	MOVQ  24(AX), R8
-	MOVQ  32(AX), R9
-	MOVQ  40(AX), R10
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	ORQ   R9, AX
-	ORQ   R10, AX
-	TESTQ AX, AX
-	JNE   l1
-	MOVQ  AX, 0(DX)
-	MOVQ  AX, 8(DX)
-	MOVQ  AX, 16(DX)
-	MOVQ  AX, 24(DX)
-	MOVQ  AX, 32(DX)
-	MOVQ  AX, 40(DX)
-	JMP   l3
-
-l1:
-	MOVQ $0xb9feffffffffaaab, CX
-	SUBQ BX, CX
-	MOVQ CX, 0(DX)
-	MOVQ $0x1eabfffeb153ffff, CX
-	SBBQ SI, CX
-	MOVQ CX, 8(DX)
-	MOVQ $0x6730d2a0f6b0f624, CX
-	SBBQ DI, CX
-	MOVQ CX, 16(DX)
-	MOVQ $0x64774b84f38512bf, CX
-	SBBQ R8, CX
-	MOVQ CX, 24(DX)
-	MOVQ $0x4b1ba7b6434bacd7, CX
-	SBBQ R9, CX
-	MOVQ CX, 32(DX)
-	MOVQ $0x1a0111ea397fe69a, CX
-	SBBQ R10, CX
-	MOVQ CX, 40(DX)
-
-l3:
-	MOVQ  x+8(FP), AX
-	MOVQ  48(AX), BX
-	MOVQ  56(AX), SI
-	MOVQ  64(AX), DI
-	MOVQ  72(AX), R8
-	MOVQ  80(AX), R9
-	MOVQ  88(AX), R10
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	ORQ   R9, AX
-	ORQ   R10, AX
-	TESTQ AX, AX
-	JNE   l2
-	MOVQ  AX, 48(DX)
-	MOVQ  AX, 56(DX)
-	MOVQ  AX, 64(DX)
-	MOVQ  AX, 72(DX)
-	MOVQ  AX, 80(DX)
-	MOVQ  AX, 88(DX)
-	RET
-
-l2:
-	MOVQ $0xb9feffffffffaaab, CX
-	SUBQ BX, CX
-	MOVQ CX, 48(DX)
-	MOVQ $0x1eabfffeb153ffff, CX
-	SBBQ SI, CX
-	MOVQ CX, 56(DX)
-	MOVQ $0x6730d2a0f6b0f624, CX
-	SBBQ DI, CX
-	MOVQ CX, 64(DX)
-	MOVQ $0x64774b84f38512bf, CX
-	SBBQ R8, CX
-	MOVQ CX, 72(DX)
-	MOVQ $0x4b1ba7b6434bacd7, CX
-	SBBQ R9, CX
-	MOVQ CX, 80(DX)
-	MOVQ $0x1a0111ea397fe69a, CX
-	SBBQ R10, CX
-	MOVQ CX, 88(DX)
-	RET
-
-TEXT ·mulNonResE2(SB), NOSPLIT, $0-16
-	XORQ    R15, R15
-	MOVQ    x+8(FP), R14
-	MOVQ    0(R14), AX
-	MOVQ    8(R14), DX
-	MOVQ    16(R14), CX
-	MOVQ    24(R14), BX
-	MOVQ    32(R14), SI
-	MOVQ    40(R14), DI
-	SUBQ    48(R14), AX
-	SBBQ    56(R14), DX
-	SBBQ    64(R14), CX
-	SBBQ    72(R14), BX
-	SBBQ    80(R14), SI
-	SBBQ    88(R14), DI
-	MOVQ    $0xb9feffffffffaaab, R8
-	MOVQ    $0x1eabfffeb153ffff, R9
-	MOVQ    $0x6730d2a0f6b0f624, R10
-	MOVQ    $0x64774b84f38512bf, R11
-	MOVQ    $0x4b1ba7b6434bacd7, R12
-	MOVQ    $0x1a0111ea397fe69a, R13
-	CMOVQCC R15, R8
-	CMOVQCC R15, R9
-	CMOVQCC R15, R10
-	CMOVQCC R15, R11
-	CMOVQCC R15, R12
-	CMOVQCC R15, R13
-	ADDQ    R8, AX
-	ADCQ    R9, DX
-	ADCQ    R10, CX
-	ADCQ    R11, BX
-	ADCQ    R12, SI
-	ADCQ    R13, DI
-	MOVQ    48(R14), R8
-	MOVQ    56(R14), R9
-	MOVQ    64(R14), R10
-	MOVQ    72(R14), R11
-	MOVQ    80(R14), R12
-	MOVQ    88(R14), R13
-	ADDQ    0(R14), R8
-	ADCQ    8(R14), R9
-	ADCQ    16(R14), R10
-	ADCQ    24(R14), R11
-	ADCQ    32(R14), R12
-	ADCQ    40(R14), R13
-	MOVQ    res+0(FP), R15
-	MOVQ    AX, 0(R15)
-	MOVQ    DX, 8(R15)
-	MOVQ    CX, 16(R15)
-	MOVQ    BX, 24(R15)
-	MOVQ    SI, 32(R15)
-	MOVQ    DI, 40(R15)
-
-	// reduce element(R8,R9,R10,R11,R12,R13) using temp registers (AX,DX,CX,BX,SI,DI)
-	REDUCE(R8,R9,R10,R11,R12,R13,AX,DX,CX,BX,SI,DI)
-
-	MOVQ R8, 48(R15)
-	MOVQ R9, 56(R15)
-	MOVQ R10, 64(R15)
-	MOVQ R11, 72(R15)
-	MOVQ R12, 80(R15)
-	MOVQ R13, 88(R15)
-	RET
-
-TEXT ·squareAdxE2(SB), $48-16
-	NO_LOCAL_POINTERS
-
-	// z.A0 = (x.A0 + x.A1) * (x.A0 - x.A1)
-	// z.A1 = 2 * x.A0 * x.A1
-
-	CMPB ·supportAdx(SB), $1
-	JNE  l4
-
-	// 2 * x.A0 * x.A1
-	MOVQ x+8(FP), AX
-
-	// 2 * x.A1[0] -> R14
-	// 2 * x.A1[1] -> R15
-	// 2 * x.A1[2] -> CX
-	// 2 * x.A1[3] -> BX
-	// 2 * x.A1[4] -> SI
-	// 2 * x.A1[5] -> DI
-	MOVQ 48(AX), R14
-	MOVQ 56(AX), R15
-	MOVQ 64(AX), CX
-	MOVQ 72(AX), BX
-	MOVQ 80(AX), SI
-	MOVQ 88(AX), DI
-	ADDQ R14, R14
-	ADCQ R15, R15
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// A -> BP
-	// t[0] -> R8
-	// t[1] -> R9
-	// t[2] -> R10
-	// t[3] -> R11
-	// t[4] -> R12
-	// t[5] -> R13
-#define MACC(in0, in1, in2) \
-	ADCXQ in0, in1     \
-	MULXQ in2, AX, in0 \
-	ADOXQ AX, in1      \
-
-#define DIV_SHIFT() \
-	PUSHQ BP                         \
-	MOVQ  $const_qInvNeg, DX         \
-	IMULQ R8, DX                     \
-	XORQ  AX, AX                     \
-	MULXQ ·qElement+0(SB), AX, BP    \
-	ADCXQ R8, AX                     \
-	MOVQ  BP, R8                     \
-	POPQ  BP                         \
-	MACC(R9, R8, ·qElement+8(SB))    \
-	MACC(R10, R9, ·qElement+16(SB))  \
-	MACC(R11, R10, ·qElement+24(SB)) \
-	MACC(R12, R11, ·qElement+32(SB)) \
-	MACC(R13, R12, ·qElement+40(SB)) \
-	MOVQ  $0, AX                     \
-	ADCXQ AX, R13                    \
-	ADOXQ BP, R13                    \
-
-#define MUL_WORD_0() \
-	XORQ  AX, AX       \
-	MULXQ R14, R8, R9  \
-	MULXQ R15, AX, R10 \
-	ADOXQ AX, R9       \
-	MULXQ CX, AX, R11  \
-	ADOXQ AX, R10      \
-	MULXQ BX, AX, R12  \
-	ADOXQ AX, R11      \
-	MULXQ SI, AX, R13  \
-	ADOXQ AX, R12      \
-	MULXQ DI, AX, BP   \
-	ADOXQ AX, R13      \
-	MOVQ  $0, AX       \
-	ADOXQ AX, BP       \
-	DIV_SHIFT()        \
-
-#define MUL_WORD_N() \
-	XORQ  AX, AX      \
-	MULXQ R14, AX, BP \
-	ADOXQ AX, R8      \
-	MACC(BP, R9, R15) \
-	MACC(BP, R10, CX) \
-	MACC(BP, R11, BX) \
-	MACC(BP, R12, SI) \
-	MACC(BP, R13, DI) \
-	MOVQ  $0, AX      \
-	ADCXQ AX, BP      \
-	ADOXQ AX, BP      \
-	DIV_SHIFT()       \
-
-	// mul body
-	MOVQ x+8(FP), DX
-	MOVQ 0(DX), DX
-	MUL_WORD_0()
-	MOVQ x+8(FP), DX
-	MOVQ 8(DX), DX
-	MUL_WORD_N()
-	MOVQ x+8(FP), DX
-	MOVQ 16(DX), DX
-	MUL_WORD_N()
-	MOVQ x+8(FP), DX
-	MOVQ 24(DX), DX
-	MUL_WORD_N()
-	MOVQ x+8(FP), DX
-	MOVQ 32(DX), DX
-	MUL_WORD_N()
-	MOVQ x+8(FP), DX
-	MOVQ 40(DX), DX
-	MUL_WORD_N()
-
-	// reduce element(R8,R9,R10,R11,R12,R13) using temp registers (R14,R15,CX,BX,SI,DI)
-	REDUCE(R8,R9,R10,R11,R12,R13,R14,R15,CX,BX,SI,DI)
-
-	MOVQ x+8(FP), AX
-
-	// x.A1[0] -> R14
-	// x.A1[1] -> R15
-	// x.A1[2] -> CX
-	// x.A1[3] -> BX
-	// x.A1[4] -> SI
-	// x.A1[5] -> DI
-	MOVQ 48(AX), R14
-	MOVQ 56(AX), R15
-	MOVQ 64(AX), CX
-	MOVQ 72(AX), BX
-	MOVQ 80(AX), SI
-	MOVQ 88(AX), DI
-	MOVQ res+0(FP), DX
-	MOVQ R8, 48(DX)
-	MOVQ R9, 56(DX)
-	MOVQ R10, 64(DX)
-	MOVQ R11, 72(DX)
-	MOVQ R12, 80(DX)
-	MOVQ R13, 88(DX)
-	MOVQ R14, R8
-	MOVQ R15, R9
-	MOVQ CX, R10
-	MOVQ BX, R11
-	MOVQ SI, R12
-	MOVQ DI, R13
-
-	// Add(&x.A0, &x.A1)
-	ADDQ 0(AX), R14
-	ADCQ 8(AX), R15
-	ADCQ 16(AX), CX
-	ADCQ 24(AX), BX
-	ADCQ 32(AX), SI
-	ADCQ 40(AX), DI
-	MOVQ R14, s0-8(SP)
-	MOVQ R15, s1-16(SP)
-	MOVQ CX, s2-24(SP)
-	MOVQ BX, s3-32(SP)
-	MOVQ SI, s4-40(SP)
-	MOVQ DI, s5-48(SP)
-	XORQ BP, BP
-
-	// Sub(&x.A0, &x.A1)
-	MOVQ    0(AX), R14
-	MOVQ    8(AX), R15
-	MOVQ    16(AX), CX
-	MOVQ    24(AX), BX
-	MOVQ    32(AX), SI
-	MOVQ    40(AX), DI
-	SUBQ    R8, R14
-	SBBQ    R9, R15
-	SBBQ    R10, CX
-	SBBQ    R11, BX
-	SBBQ    R12, SI
-	SBBQ    R13, DI
-	MOVQ    $0xb9feffffffffaaab, R8
-	MOVQ    $0x1eabfffeb153ffff, R9
-	MOVQ    $0x6730d2a0f6b0f624, R10
-	MOVQ    $0x64774b84f38512bf, R11
-	MOVQ    $0x4b1ba7b6434bacd7, R12
-	MOVQ    $0x1a0111ea397fe69a, R13
-	CMOVQCC BP, R8
-	CMOVQCC BP, R9
-	CMOVQCC BP, R10
-	CMOVQCC BP, R11
-	CMOVQCC BP, R12
-	CMOVQCC BP, R13
-	ADDQ    R8, R14
-	ADCQ    R9, R15
-	ADCQ    R10, CX
-	ADCQ    R11, BX
-	ADCQ    R12, SI
-	ADCQ    R13, DI
-
-	// A -> BP
-	// t[0] -> R8
-	// t[1] -> R9
-	// t[2] -> R10
-	// t[3] -> R11
-	// t[4] -> R12
-	// t[5] -> R13
-	// mul body
-	MOVQ s0-8(SP), DX
-	MUL_WORD_0()
-	MOVQ s1-16(SP), DX
-	MUL_WORD_N()
-	MOVQ s2-24(SP), DX
-	MUL_WORD_N()
-	MOVQ s3-32(SP), DX
-	MUL_WORD_N()
-	MOVQ s4-40(SP), DX
-	MUL_WORD_N()
-	MOVQ s5-48(SP), DX
-	MUL_WORD_N()
-
-	// reduce element(R8,R9,R10,R11,R12,R13) using temp registers (R14,R15,CX,BX,SI,DI)
-	REDUCE(R8,R9,R10,R11,R12,R13,R14,R15,CX,BX,SI,DI)
-
-	MOVQ res+0(FP), AX
-	MOVQ R8, 0(AX)
-	MOVQ R9, 8(AX)
-	MOVQ R10, 16(AX)
-	MOVQ R11, 24(AX)
-	MOVQ R12, 32(AX)
-	MOVQ R13, 40(AX)
-	RET
-
-l4:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	CALL ·squareGenericE2(SB)
-	RET
-
-TEXT ·mulAdxE2(SB), $96-24
-	NO_LOCAL_POINTERS
-
-	// var a, b, c fp.Element
-	// a.Add(&x.A0, &x.A1)
-	// b.Add(&y.A0, &y.A1)
-	// a.Mul(&a, &b)
-	// b.Mul(&x.A0, &y.A0)
-	// c.Mul(&x.A1, &y.A1)
-	// z.A1.Sub(&a, &b).Sub(&z.A1, &c)
-	// z.A0.Sub(&b, &c)
-
-	CMPB ·supportAdx(SB), $1
-	JNE  l5
-	MOVQ x+8(FP), AX
-	MOVQ 48(AX), R14
-	MOVQ 56(AX), R15
-	MOVQ 64(AX), CX
-	MOVQ 72(AX), BX
-	MOVQ 80(AX), SI
-	MOVQ 88(AX), DI
-
-	// A -> BP
-	// t[0] -> R8
-	// t[1] -> R9
-	// t[2] -> R10
-	// t[3] -> R11
-	// t[4] -> R12
-	// t[5] -> R13
-	// mul body
-	MOVQ y+16(FP), DX
-	MOVQ 48(DX), DX
-	MUL_WORD_0()
-	MOVQ y+16(FP), DX
-	MOVQ 56(DX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), DX
-	MOVQ 64(DX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), DX
-	MOVQ 72(DX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), DX
-	MOVQ 80(DX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), DX
-	MOVQ 88(DX), DX
-	MUL_WORD_N()
-
-	// reduce element(R8,R9,R10,R11,R12,R13) using temp registers (R14,R15,CX,BX,SI,DI)
-	REDUCE(R8,R9,R10,R11,R12,R13,R14,R15,CX,BX,SI,DI)
-
-	MOVQ R8, s6-56(SP)
-	MOVQ R9, s7-64(SP)
-	MOVQ R10, s8-72(SP)
-	MOVQ R11, s9-80(SP)
-	MOVQ R12, s10-88(SP)
-	MOVQ R13, s11-96(SP)
-	MOVQ x+8(FP), AX
-	MOVQ y+16(FP), DX
-	MOVQ 48(AX), R14
-	MOVQ 56(AX), R15
-	MOVQ 64(AX), CX
-	MOVQ 72(AX), BX
-	MOVQ 80(AX), SI
-	MOVQ 88(AX), DI
-	ADDQ 0(AX), R14
-	ADCQ 8(AX), R15
-	ADCQ 16(AX), CX
-	ADCQ 24(AX), BX
-	ADCQ 32(AX), SI
-	ADCQ 40(AX), DI
-	MOVQ R14, s0-8(SP)
-	MOVQ R15, s1-16(SP)
-	MOVQ CX, s2-24(SP)
-	MOVQ BX, s3-32(SP)
-	MOVQ SI, s4-40(SP)
-	MOVQ DI, s5-48(SP)
-	MOVQ 0(DX), R14
-	MOVQ 8(DX), R15
-	MOVQ 16(DX), CX
-	MOVQ 24(DX), BX
-	MOVQ 32(DX), SI
-	MOVQ 40(DX), DI
-	ADDQ 48(DX), R14
-	ADCQ 56(DX), R15
-	ADCQ 64(DX), CX
-	ADCQ 72(DX), BX
-	ADCQ 80(DX), SI
-	ADCQ 88(DX), DI
-
-	// A -> BP
-	// t[0] -> R8
-	// t[1] -> R9
-	// t[2] -> R10
-	// t[3] -> R11
-	// t[4] -> R12
-	// t[5] -> R13
-	// mul body
-	MOVQ s0-8(SP), DX
-	MUL_WORD_0()
-	MOVQ s1-16(SP), DX
-	MUL_WORD_N()
-	MOVQ s2-24(SP), DX
-	MUL_WORD_N()
-	MOVQ s3-32(SP), DX
-	MUL_WORD_N()
-	MOVQ s4-40(SP), DX
-	MUL_WORD_N()
-	MOVQ s5-48(SP), DX
-	MUL_WORD_N()
-
-	// reduce element(R8,R9,R10,R11,R12,R13) using temp registers (R14,R15,CX,BX,SI,DI)
-	REDUCE(R8,R9,R10,R11,R12,R13,R14,R15,CX,BX,SI,DI)
-
-	MOVQ R8, s0-8(SP)
-	MOVQ R9, s1-16(SP)
-	MOVQ R10, s2-24(SP)
-	MOVQ R11, s3-32(SP)
-	MOVQ R12, s4-40(SP)
-	MOVQ R13, s5-48(SP)
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), R14
-	MOVQ 8(AX), R15
-	MOVQ 16(AX), CX
-	MOVQ 24(AX), BX
-	MOVQ 32(AX), SI
-	MOVQ 40(AX), DI
-
-	// A -> BP
-	// t[0] -> R8
-	// t[1] -> R9
-	// t[2] -> R10
-	// t[3] -> R11
-	// t[4] -> R12
-	// t[5] -> R13
-	// mul body
-	MOVQ y+16(FP), DX
-	MOVQ 0(DX), DX
-	MUL_WORD_0()
-	MOVQ y+16(FP), DX
-	MOVQ 8(DX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), DX
-	MOVQ 16(DX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), DX
-	MOVQ 24(DX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), DX
-	MOVQ 32(DX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), DX
-	MOVQ 40(DX), DX
-	MUL_WORD_N()
-
-	// reduce element(R8,R9,R10,R11,R12,R13) using temp registers (R14,R15,CX,BX,SI,DI)
-	REDUCE(R8,R9,R10,R11,R12,R13,R14,R15,CX,BX,SI,DI)
-
-	XORQ    DX, DX
-	MOVQ    s0-8(SP), R14
-	MOVQ    s1-16(SP), R15
-	MOVQ    s2-24(SP), CX
-	MOVQ    s3-32(SP), BX
-	MOVQ    s4-40(SP), SI
-	MOVQ    s5-48(SP), DI
-	SUBQ    R8, R14
-	SBBQ    R9, R15
-	SBBQ    R10, CX
-	SBBQ    R11, BX
-	SBBQ    R12, SI
-	SBBQ    R13, DI
-	MOVQ    R8, s0-8(SP)
-	MOVQ    R9, s1-16(SP)
-	MOVQ    R10, s2-24(SP)
-	MOVQ    R11, s3-32(SP)
-	MOVQ    R12, s4-40(SP)
-	MOVQ    R13, s5-48(SP)
-	MOVQ    $0xb9feffffffffaaab, R8
-	MOVQ    $0x1eabfffeb153ffff, R9
-	MOVQ    $0x6730d2a0f6b0f624, R10
-	MOVQ    $0x64774b84f38512bf, R11
-	MOVQ    $0x4b1ba7b6434bacd7, R12
-	MOVQ    $0x1a0111ea397fe69a, R13
-	CMOVQCC DX, R8
-	CMOVQCC DX, R9
-	CMOVQCC DX, R10
-	CMOVQCC DX, R11
-	CMOVQCC DX, R12
-	CMOVQCC DX, R13
-	ADDQ    R8, R14
-	ADCQ    R9, R15
-	ADCQ    R10, CX
-	ADCQ    R11, BX
-	ADCQ    R12, SI
-	ADCQ    R13, DI
-	SUBQ    s6-56(SP), R14
-	SBBQ    s7-64(SP), R15
-	SBBQ    s8-72(SP), CX
-	SBBQ    s9-80(SP), BX
-	SBBQ    s10-88(SP), SI
-	SBBQ    s11-96(SP), DI
-	MOVQ    $0xb9feffffffffaaab, R8
-	MOVQ    $0x1eabfffeb153ffff, R9
-	MOVQ    $0x6730d2a0f6b0f624, R10
-	MOVQ    $0x64774b84f38512bf, R11
-	MOVQ    $0x4b1ba7b6434bacd7, R12
-	MOVQ    $0x1a0111ea397fe69a, R13
-	CMOVQCC DX, R8
-	CMOVQCC DX, R9
-	CMOVQCC DX, R10
-	CMOVQCC DX, R11
-	CMOVQCC DX, R12
-	CMOVQCC DX, R13
-	ADDQ    R8, R14
-	ADCQ    R9, R15
-	ADCQ    R10, CX
-	ADCQ    R11, BX
-	ADCQ    R12, SI
-	ADCQ    R13, DI
-	MOVQ    z+0(FP), AX
-	MOVQ    R14, 48(AX)
-	MOVQ    R15, 56(AX)
-	MOVQ    CX, 64(AX)
-	MOVQ    BX, 72(AX)
-	MOVQ    SI, 80(AX)
-	MOVQ    DI, 88(AX)
-	MOVQ    s0-8(SP), R8
-	MOVQ    s1-16(SP), R9
-	MOVQ    s2-24(SP), R10
-	MOVQ    s3-32(SP), R11
-	MOVQ    s4-40(SP), R12
-	MOVQ    s5-48(SP), R13
-	SUBQ    s6-56(SP), R8
-	SBBQ    s7-64(SP), R9
-	SBBQ    s8-72(SP), R10
-	SBBQ    s9-80(SP), R11
-	SBBQ    s10-88(SP), R12
-	SBBQ    s11-96(SP), R13
-	MOVQ    $0xb9feffffffffaaab, R14
-	MOVQ    $0x1eabfffeb153ffff, R15
-	MOVQ    $0x6730d2a0f6b0f624, CX
-	MOVQ    $0x64774b84f38512bf, BX
-	MOVQ    $0x4b1ba7b6434bacd7, SI
-	MOVQ    $0x1a0111ea397fe69a, DI
-	CMOVQCC DX, R14
-	CMOVQCC DX, R15
-	CMOVQCC DX, CX
-	CMOVQCC DX, BX
-	CMOVQCC DX, SI
-	CMOVQCC DX, DI
-	ADDQ    R14, R8
-	ADCQ    R15, R9
-	ADCQ    CX, R10
-	ADCQ    BX, R11
-	ADCQ    SI, R12
-	ADCQ    DI, R13
-	MOVQ    R8, 0(AX)
-	MOVQ    R9, 8(AX)
-	MOVQ    R10, 16(AX)
-	MOVQ    R11, 24(AX)
-	MOVQ    R12, 32(AX)
-	MOVQ    R13, 40(AX)
-	RET
-
-l5:
-	MOVQ z+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	MOVQ y+16(FP), AX
-	MOVQ AX, 16(SP)
-	CALL ·mulGenericE2(SB)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls24-315/fp/element_amd64.s b/ecc/bls24-315/fp/element_amd64.s
--- a/ecc/bls24-315/fp/element_amd64.s
+++ b/ecc/bls24-315/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 4600622797032586825
-#include "../../../field/asm/element_5w/element_5w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls24-315/fr/element_amd64.s b/ecc/bls24-315/fr/element_amd64.s
--- a/ecc/bls24-315/fr/element_amd64.s
+++ b/ecc/bls24-315/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls24-315/fr/element_arm64.s b/ecc/bls24-315/fr/element_arm64.s
--- a/ecc/bls24-315/fr/element_arm64.s
+++ b/ecc/bls24-315/fr/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls24-315/internal/fptower/e2_amd64.s b/ecc/bls24-315/internal/fptower/e2_amd64.s
--- a/ecc/bls24-315/internal/fptower/e2_amd64.s
+++ b/ecc/bls24-315/internal/fptower/e2_amd64.s
@@ -1,1633 +1,1 @@
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-#include "textflag.h"
-#include "funcdata.h"
-
-// modulus q
-DATA q<>+0(SB)/8, $0x6fe802ff40300001
-DATA q<>+8(SB)/8, $0x421ee5da52bde502
-DATA q<>+16(SB)/8, $0xdec1d01aa27a1ae0
-DATA q<>+24(SB)/8, $0xd3f7498be97c5eaf
-DATA q<>+32(SB)/8, $0x04c23a02b586d650
-GLOBL q<>(SB), (RODATA+NOPTR), $40
-
-// qInv0 q'[0]
-DATA qInv0<>(SB)/8, $0x702ff9ff402fffff
-GLOBL qInv0<>(SB), (RODATA+NOPTR), $8
-
-#define REDUCE(ra0, ra1, ra2, ra3, ra4, rb0, rb1, rb2, rb3, rb4) \
-	MOVQ    ra0, rb0;        \
-	SUBQ    q<>(SB), ra0;    \
-	MOVQ    ra1, rb1;        \
-	SBBQ    q<>+8(SB), ra1;  \
-	MOVQ    ra2, rb2;        \
-	SBBQ    q<>+16(SB), ra2; \
-	MOVQ    ra3, rb3;        \
-	SBBQ    q<>+24(SB), ra3; \
-	MOVQ    ra4, rb4;        \
-	SBBQ    q<>+32(SB), ra4; \
-	CMOVQCS rb0, ra0;        \
-	CMOVQCS rb1, ra1;        \
-	CMOVQCS rb2, ra2;        \
-	CMOVQCS rb3, ra3;        \
-	CMOVQCS rb4, ra4;        \
-
-TEXT ·addE2(SB), NOSPLIT, $0-24
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), BX
-	MOVQ 8(AX), SI
-	MOVQ 16(AX), DI
-	MOVQ 24(AX), R8
-	MOVQ 32(AX), R9
-	MOVQ y+16(FP), DX
-	ADDQ 0(DX), BX
-	ADCQ 8(DX), SI
-	ADCQ 16(DX), DI
-	ADCQ 24(DX), R8
-	ADCQ 32(DX), R9
-
-	// reduce element(BX,SI,DI,R8,R9) using temp registers (R10,R11,R12,R13,R14)
-	REDUCE(BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	MOVQ res+0(FP), CX
-	MOVQ BX, 0(CX)
-	MOVQ SI, 8(CX)
-	MOVQ DI, 16(CX)
-	MOVQ R8, 24(CX)
-	MOVQ R9, 32(CX)
-	MOVQ 40(AX), BX
-	MOVQ 48(AX), SI
-	MOVQ 56(AX), DI
-	MOVQ 64(AX), R8
-	MOVQ 72(AX), R9
-	ADDQ 40(DX), BX
-	ADCQ 48(DX), SI
-	ADCQ 56(DX), DI
-	ADCQ 64(DX), R8
-	ADCQ 72(DX), R9
-
-	// reduce element(BX,SI,DI,R8,R9) using temp registers (R15,R10,R11,R12,R13)
-	REDUCE(BX,SI,DI,R8,R9,R15,R10,R11,R12,R13)
-
-	MOVQ BX, 40(CX)
-	MOVQ SI, 48(CX)
-	MOVQ DI, 56(CX)
-	MOVQ R8, 64(CX)
-	MOVQ R9, 72(CX)
-	RET
-
-TEXT ·doubleE2(SB), NOSPLIT, $0-16
-	MOVQ res+0(FP), DX
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), CX
-	MOVQ 8(AX), BX
-	MOVQ 16(AX), SI
-	MOVQ 24(AX), DI
-	MOVQ 32(AX), R8
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13)
-
-	MOVQ CX, 0(DX)
-	MOVQ BX, 8(DX)
-	MOVQ SI, 16(DX)
-	MOVQ DI, 24(DX)
-	MOVQ R8, 32(DX)
-	MOVQ 40(AX), CX
-	MOVQ 48(AX), BX
-	MOVQ 56(AX), SI
-	MOVQ 64(AX), DI
-	MOVQ 72(AX), R8
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(CX,BX,SI,DI,R8) using temp registers (R14,R15,R9,R10,R11)
-	REDUCE(CX,BX,SI,DI,R8,R14,R15,R9,R10,R11)
-
-	MOVQ CX, 40(DX)
-	MOVQ BX, 48(DX)
-	MOVQ SI, 56(DX)
-	MOVQ DI, 64(DX)
-	MOVQ R8, 72(DX)
-	RET
-
-TEXT ·subE2(SB), NOSPLIT, $0-24
-	XORQ    R8, R8
-	MOVQ    x+8(FP), DI
-	MOVQ    0(DI), AX
-	MOVQ    8(DI), DX
-	MOVQ    16(DI), CX
-	MOVQ    24(DI), BX
-	MOVQ    32(DI), SI
-	MOVQ    y+16(FP), DI
-	SUBQ    0(DI), AX
-	SBBQ    8(DI), DX
-	SBBQ    16(DI), CX
-	SBBQ    24(DI), BX
-	SBBQ    32(DI), SI
-	MOVQ    x+8(FP), DI
-	MOVQ    $0x6fe802ff40300001, R9
-	MOVQ    $0x421ee5da52bde502, R10
-	MOVQ    $0xdec1d01aa27a1ae0, R11
-	MOVQ    $0xd3f7498be97c5eaf, R12
-	MOVQ    $0x04c23a02b586d650, R13
-	CMOVQCC R8, R9
-	CMOVQCC R8, R10
-	CMOVQCC R8, R11
-	CMOVQCC R8, R12
-	CMOVQCC R8, R13
-	ADDQ    R9, AX
-	ADCQ    R10, DX
-	ADCQ    R11, CX
-	ADCQ    R12, BX
-	ADCQ    R13, SI
-	MOVQ    res+0(FP), R14
-	MOVQ    AX, 0(R14)
-	MOVQ    DX, 8(R14)
-	MOVQ    CX, 16(R14)
-	MOVQ    BX, 24(R14)
-	MOVQ    SI, 32(R14)
-	MOVQ    40(DI), AX
-	MOVQ    48(DI), DX
-	MOVQ    56(DI), CX
-	MOVQ    64(DI), BX
-	MOVQ    72(DI), SI
-	MOVQ    y+16(FP), DI
-	SUBQ    40(DI), AX
-	SBBQ    48(DI), DX
-	SBBQ    56(DI), CX
-	SBBQ    64(DI), BX
-	SBBQ    72(DI), SI
-	MOVQ    $0x6fe802ff40300001, R15
-	MOVQ    $0x421ee5da52bde502, R9
-	MOVQ    $0xdec1d01aa27a1ae0, R10
-	MOVQ    $0xd3f7498be97c5eaf, R11
-	MOVQ    $0x04c23a02b586d650, R12
-	CMOVQCC R8, R15
-	CMOVQCC R8, R9
-	CMOVQCC R8, R10
-	CMOVQCC R8, R11
-	CMOVQCC R8, R12
-	ADDQ    R15, AX
-	ADCQ    R9, DX
-	ADCQ    R10, CX
-	ADCQ    R11, BX
-	ADCQ    R12, SI
-	MOVQ    res+0(FP), DI
-	MOVQ    AX, 40(DI)
-	MOVQ    DX, 48(DI)
-	MOVQ    CX, 56(DI)
-	MOVQ    BX, 64(DI)
-	MOVQ    SI, 72(DI)
-	RET
-
-TEXT ·negE2(SB), NOSPLIT, $0-16
-	MOVQ  res+0(FP), DX
-	MOVQ  x+8(FP), AX
-	MOVQ  0(AX), BX
-	MOVQ  8(AX), SI
-	MOVQ  16(AX), DI
-	MOVQ  24(AX), R8
-	MOVQ  32(AX), R9
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	ORQ   R9, AX
-	TESTQ AX, AX
-	JNE   l1
-	MOVQ  AX, 0(DX)
-	MOVQ  AX, 8(DX)
-	MOVQ  AX, 16(DX)
-	MOVQ  AX, 24(DX)
-	MOVQ  AX, 32(DX)
-	JMP   l3
-
-l1:
-	MOVQ $0x6fe802ff40300001, CX
-	SUBQ BX, CX
-	MOVQ CX, 0(DX)
-	MOVQ $0x421ee5da52bde502, CX
-	SBBQ SI, CX
-	MOVQ CX, 8(DX)
-	MOVQ $0xdec1d01aa27a1ae0, CX
-	SBBQ DI, CX
-	MOVQ CX, 16(DX)
-	MOVQ $0xd3f7498be97c5eaf, CX
-	SBBQ R8, CX
-	MOVQ CX, 24(DX)
-	MOVQ $0x04c23a02b586d650, CX
-	SBBQ R9, CX
-	MOVQ CX, 32(DX)
-
-l3:
-	MOVQ  x+8(FP), AX
-	MOVQ  40(AX), BX
-	MOVQ  48(AX), SI
-	MOVQ  56(AX), DI
-	MOVQ  64(AX), R8
-	MOVQ  72(AX), R9
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	ORQ   R9, AX
-	TESTQ AX, AX
-	JNE   l2
-	MOVQ  AX, 40(DX)
-	MOVQ  AX, 48(DX)
-	MOVQ  AX, 56(DX)
-	MOVQ  AX, 64(DX)
-	MOVQ  AX, 72(DX)
-	RET
-
-l2:
-	MOVQ $0x6fe802ff40300001, CX
-	SUBQ BX, CX
-	MOVQ CX, 40(DX)
-	MOVQ $0x421ee5da52bde502, CX
-	SBBQ SI, CX
-	MOVQ CX, 48(DX)
-	MOVQ $0xdec1d01aa27a1ae0, CX
-	SBBQ DI, CX
-	MOVQ CX, 56(DX)
-	MOVQ $0xd3f7498be97c5eaf, CX
-	SBBQ R8, CX
-	MOVQ CX, 64(DX)
-	MOVQ $0x04c23a02b586d650, CX
-	SBBQ R9, CX
-	MOVQ CX, 72(DX)
-	RET
-
-TEXT ·mulNonResE2(SB), NOSPLIT, $0-16
-	MOVQ x+8(FP), AX
-	MOVQ 40(AX), DX
-	MOVQ 48(AX), CX
-	MOVQ 56(AX), BX
-	MOVQ 64(AX), SI
-	MOVQ 72(AX), DI
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP))
-	REDUCE(DX,CX,BX,SI,DI,R13,R14,R15,s0-8(SP),s1-16(SP))
-
-	MOVQ DX, R13
-	MOVQ CX, R14
-	MOVQ BX, R15
-	MOVQ SI, s0-8(SP)
-	MOVQ DI, s1-16(SP)
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	ADDQ R13, DX
-	ADCQ R14, CX
-	ADCQ R15, BX
-	ADCQ s0-8(SP), SI
-	ADCQ s1-16(SP), DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	ADDQ 40(AX), DX
-	ADCQ 48(AX), CX
-	ADCQ 56(AX), BX
-	ADCQ 64(AX), SI
-	ADCQ 72(AX), DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	MOVQ res+0(FP), R13
-	MOVQ 0(AX), R8
-	MOVQ 8(AX), R9
-	MOVQ 16(AX), R10
-	MOVQ 24(AX), R11
-	MOVQ 32(AX), R12
-	MOVQ R8, 40(R13)
-	MOVQ R9, 48(R13)
-	MOVQ R10, 56(R13)
-	MOVQ R11, 64(R13)
-	MOVQ R12, 72(R13)
-	MOVQ DX, 0(R13)
-	MOVQ CX, 8(R13)
-	MOVQ BX, 16(R13)
-	MOVQ SI, 24(R13)
-	MOVQ DI, 32(R13)
-	RET
-
-TEXT ·mulAdxE2(SB), $80-24
-	NO_LOCAL_POINTERS
-
-	// 	var a, b, c fp.Element
-	// 	a.Add(&x.A0, &x.A1)
-	// 	b.Add(&y.A0, &y.A1)
-	// 	a.Mul(&a, &b)
-	// 	b.Mul(&x.A0, &y.A0)
-	// 	c.Mul(&x.A1, &y.A1)
-	// 	z.A1.Sub(&a, &b).Sub(&z.A1, &c)
-	// 	fp.MulBy13(&c)
-	// 	z.A0.Add(&c, &b)
-
-	CMPB ·supportAdx(SB), $1
-	JNE  l4
-	MOVQ x+8(FP), AX
-	MOVQ 40(AX), R14
-	MOVQ 48(AX), R15
-	MOVQ 56(AX), CX
-	MOVQ 64(AX), BX
-	MOVQ 72(AX), SI
-
-	// A -> BP
-	// t[0] -> DI
-	// t[1] -> R8
-	// t[2] -> R9
-	// t[3] -> R10
-	// t[4] -> R11
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 40(DX), DX
-
-	// (A,t[0])  := x[0]*y[0] + A
-	MULXQ R14, DI, R8
-
-	// (A,t[1])  := x[1]*y[0] + A
-	MULXQ R15, AX, R9
-	ADOXQ AX, R8
-
-	// (A,t[2])  := x[2]*y[0] + A
-	MULXQ CX, AX, R10
-	ADOXQ AX, R9
-
-	// (A,t[3])  := x[3]*y[0] + A
-	MULXQ BX, AX, R11
-	ADOXQ AX, R10
-
-	// (A,t[4])  := x[4]*y[0] + A
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 48(DX), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[1] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[1] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[1] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[1] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[1] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 56(DX), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[2] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[2] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[2] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[2] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[2] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 64(DX), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[3] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[3] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[3] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[3] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[3] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 72(DX), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[4] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[4] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[4] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[4] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[4] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// reduce element(DI,R8,R9,R10,R11) using temp registers (R14,R15,CX,BX,SI)
-	REDUCE(DI,R8,R9,R10,R11,R14,R15,CX,BX,SI)
-
-	MOVQ DI, s5-48(SP)
-	MOVQ R8, s6-56(SP)
-	MOVQ R9, s7-64(SP)
-	MOVQ R10, s8-72(SP)
-	MOVQ R11, s9-80(SP)
-	MOVQ x+8(FP), AX
-	MOVQ y+16(FP), DX
-	MOVQ 40(AX), R14
-	MOVQ 48(AX), R15
-	MOVQ 56(AX), CX
-	MOVQ 64(AX), BX
-	MOVQ 72(AX), SI
-	ADDQ 0(AX), R14
-	ADCQ 8(AX), R15
-	ADCQ 16(AX), CX
-	ADCQ 24(AX), BX
-	ADCQ 32(AX), SI
-	MOVQ R14, s0-8(SP)
-	MOVQ R15, s1-16(SP)
-	MOVQ CX, s2-24(SP)
-	MOVQ BX, s3-32(SP)
-	MOVQ SI, s4-40(SP)
-	MOVQ 0(DX), R14
-	MOVQ 8(DX), R15
-	MOVQ 16(DX), CX
-	MOVQ 24(DX), BX
-	MOVQ 32(DX), SI
-	ADDQ 40(DX), R14
-	ADCQ 48(DX), R15
-	ADCQ 56(DX), CX
-	ADCQ 64(DX), BX
-	ADCQ 72(DX), SI
-
-	// A -> BP
-	// t[0] -> DI
-	// t[1] -> R8
-	// t[2] -> R9
-	// t[3] -> R10
-	// t[4] -> R11
-	// clear the flags
-	XORQ AX, AX
-	MOVQ s0-8(SP), DX
-
-	// (A,t[0])  := x[0]*y[0] + A
-	MULXQ R14, DI, R8
-
-	// (A,t[1])  := x[1]*y[0] + A
-	MULXQ R15, AX, R9
-	ADOXQ AX, R8
-
-	// (A,t[2])  := x[2]*y[0] + A
-	MULXQ CX, AX, R10
-	ADOXQ AX, R9
-
-	// (A,t[3])  := x[3]*y[0] + A
-	MULXQ BX, AX, R11
-	ADOXQ AX, R10
-
-	// (A,t[4])  := x[4]*y[0] + A
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R13
-	ADCXQ DI, AX
-	MOVQ  R13, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ s1-16(SP), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[1] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[1] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[1] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[1] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[1] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R13
-	ADCXQ DI, AX
-	MOVQ  R13, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ s2-24(SP), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[2] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[2] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[2] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[2] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[2] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R13
-	ADCXQ DI, AX
-	MOVQ  R13, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ s3-32(SP), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[3] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[3] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[3] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[3] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[3] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R13
-	ADCXQ DI, AX
-	MOVQ  R13, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ s4-40(SP), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[4] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[4] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[4] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[4] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[4] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R13
-	ADCXQ DI, AX
-	MOVQ  R13, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// reduce element(DI,R8,R9,R10,R11) using temp registers (R14,R15,CX,BX,SI)
-	REDUCE(DI,R8,R9,R10,R11,R14,R15,CX,BX,SI)
-
-	MOVQ DI, s0-8(SP)
-	MOVQ R8, s1-16(SP)
-	MOVQ R9, s2-24(SP)
-	MOVQ R10, s3-32(SP)
-	MOVQ R11, s4-40(SP)
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), R14
-	MOVQ 8(AX), R15
-	MOVQ 16(AX), CX
-	MOVQ 24(AX), BX
-	MOVQ 32(AX), SI
-
-	// A -> BP
-	// t[0] -> DI
-	// t[1] -> R8
-	// t[2] -> R9
-	// t[3] -> R10
-	// t[4] -> R11
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 0(DX), DX
-
-	// (A,t[0])  := x[0]*y[0] + A
-	MULXQ R14, DI, R8
-
-	// (A,t[1])  := x[1]*y[0] + A
-	MULXQ R15, AX, R9
-	ADOXQ AX, R8
-
-	// (A,t[2])  := x[2]*y[0] + A
-	MULXQ CX, AX, R10
-	ADOXQ AX, R9
-
-	// (A,t[3])  := x[3]*y[0] + A
-	MULXQ BX, AX, R11
-	ADOXQ AX, R10
-
-	// (A,t[4])  := x[4]*y[0] + A
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 8(DX), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[1] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[1] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[1] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[1] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[1] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 16(DX), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[2] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[2] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[2] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[2] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[2] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 24(DX), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[3] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[3] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[3] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[3] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[3] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// clear the flags
-	XORQ AX, AX
-	MOVQ y+16(FP), DX
-	MOVQ 32(DX), DX
-
-	// (A,t[0])  := t[0] + x[0]*y[4] + A
-	MULXQ R14, AX, BP
-	ADOXQ AX, DI
-
-	// (A,t[1])  := t[1] + x[1]*y[4] + A
-	ADCXQ BP, R8
-	MULXQ R15, AX, BP
-	ADOXQ AX, R8
-
-	// (A,t[2])  := t[2] + x[2]*y[4] + A
-	ADCXQ BP, R9
-	MULXQ CX, AX, BP
-	ADOXQ AX, R9
-
-	// (A,t[3])  := t[3] + x[3]*y[4] + A
-	ADCXQ BP, R10
-	MULXQ BX, AX, BP
-	ADOXQ AX, R10
-
-	// (A,t[4])  := t[4] + x[4]*y[4] + A
-	ADCXQ BP, R11
-	MULXQ SI, AX, BP
-	ADOXQ AX, R11
-
-	// A += carries from ADCXQ and ADOXQ
-	MOVQ  $0, AX
-	ADCXQ AX, BP
-	ADOXQ AX, BP
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  qInv0<>(SB), DX
-	IMULQ DI, DX
-
-	// clear the flags
-	XORQ AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ q<>+0(SB), AX, R12
-	ADCXQ DI, AX
-	MOVQ  R12, DI
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R8, DI
-	MULXQ q<>+8(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ R9, R8
-	MULXQ q<>+16(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ R10, R9
-	MULXQ q<>+24(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ R11, R10
-	MULXQ q<>+32(SB), AX, R11
-	ADOXQ AX, R10
-
-	// t[4] = C + A
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ BP, R11
-
-	// reduce element(DI,R8,R9,R10,R11) using temp registers (R14,R15,CX,BX,SI)
-	REDUCE(DI,R8,R9,R10,R11,R14,R15,CX,BX,SI)
-
-	XORQ    DX, DX
-	MOVQ    s0-8(SP), R14
-	MOVQ    s1-16(SP), R15
-	MOVQ    s2-24(SP), CX
-	MOVQ    s3-32(SP), BX
-	MOVQ    s4-40(SP), SI
-	SUBQ    DI, R14
-	SBBQ    R8, R15
-	SBBQ    R9, CX
-	SBBQ    R10, BX
-	SBBQ    R11, SI
-	MOVQ    DI, s0-8(SP)
-	MOVQ    R8, s1-16(SP)
-	MOVQ    R9, s2-24(SP)
-	MOVQ    R10, s3-32(SP)
-	MOVQ    R11, s4-40(SP)
-	MOVQ    $0x6fe802ff40300001, DI
-	MOVQ    $0x421ee5da52bde502, R8
-	MOVQ    $0xdec1d01aa27a1ae0, R9
-	MOVQ    $0xd3f7498be97c5eaf, R10
-	MOVQ    $0x04c23a02b586d650, R11
-	CMOVQCC DX, DI
-	CMOVQCC DX, R8
-	CMOVQCC DX, R9
-	CMOVQCC DX, R10
-	CMOVQCC DX, R11
-	ADDQ    DI, R14
-	ADCQ    R8, R15
-	ADCQ    R9, CX
-	ADCQ    R10, BX
-	ADCQ    R11, SI
-	SUBQ    s5-48(SP), R14
-	SBBQ    s6-56(SP), R15
-	SBBQ    s7-64(SP), CX
-	SBBQ    s8-72(SP), BX
-	SBBQ    s9-80(SP), SI
-	MOVQ    $0x6fe802ff40300001, DI
-	MOVQ    $0x421ee5da52bde502, R8
-	MOVQ    $0xdec1d01aa27a1ae0, R9
-	MOVQ    $0xd3f7498be97c5eaf, R10
-	MOVQ    $0x04c23a02b586d650, R11
-	CMOVQCC DX, DI
-	CMOVQCC DX, R8
-	CMOVQCC DX, R9
-	CMOVQCC DX, R10
-	CMOVQCC DX, R11
-	ADDQ    DI, R14
-	ADCQ    R8, R15
-	ADCQ    R9, CX
-	ADCQ    R10, BX
-	ADCQ    R11, SI
-	MOVQ    res+0(FP), AX
-	MOVQ    R14, 40(AX)
-	MOVQ    R15, 48(AX)
-	MOVQ    CX, 56(AX)
-	MOVQ    BX, 64(AX)
-	MOVQ    SI, 72(AX)
-	MOVQ    s5-48(SP), DI
-	MOVQ    s6-56(SP), R8
-	MOVQ    s7-64(SP), R9
-	MOVQ    s8-72(SP), R10
-	MOVQ    s9-80(SP), R11
-	MOVQ    s0-8(SP), R14
-	MOVQ    s1-16(SP), R15
-	MOVQ    s2-24(SP), CX
-	MOVQ    s3-32(SP), BX
-	MOVQ    s4-40(SP), SI
-	ADDQ    DI, R14
-	ADCQ    R8, R15
-	ADCQ    R9, CX
-	ADCQ    R10, BX
-	ADCQ    R11, SI
-
-	// reduce element(R14,R15,CX,BX,SI) using temp registers (DI,R8,R9,R10,R11)
-	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11)
-
-	MOVQ s5-48(SP), DI
-	MOVQ s6-56(SP), R8
-	MOVQ s7-64(SP), R9
-	MOVQ s8-72(SP), R10
-	MOVQ s9-80(SP), R11
-	MOVQ R14, s5-48(SP)
-	MOVQ R15, s6-56(SP)
-	MOVQ CX, s7-64(SP)
-	MOVQ BX, s8-72(SP)
-	MOVQ SI, s9-80(SP)
-	ADDQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-
-	// reduce element(DI,R8,R9,R10,R11) using temp registers (R14,R15,CX,BX,SI)
-	REDUCE(DI,R8,R9,R10,R11,R14,R15,CX,BX,SI)
-
-	ADDQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-
-	// reduce element(DI,R8,R9,R10,R11) using temp registers (R14,R15,CX,BX,SI)
-	REDUCE(DI,R8,R9,R10,R11,R14,R15,CX,BX,SI)
-
-	MOVQ DI, s0-8(SP)
-	MOVQ R8, s1-16(SP)
-	MOVQ R9, s2-24(SP)
-	MOVQ R10, s3-32(SP)
-	MOVQ R11, s4-40(SP)
-	ADDQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-
-	// reduce element(DI,R8,R9,R10,R11) using temp registers (R14,R15,CX,BX,SI)
-	REDUCE(DI,R8,R9,R10,R11,R14,R15,CX,BX,SI)
-
-	ADDQ s0-8(SP), DI
-	ADCQ s1-16(SP), R8
-	ADCQ s2-24(SP), R9
-	ADCQ s3-32(SP), R10
-	ADCQ s4-40(SP), R11
-
-	// reduce element(DI,R8,R9,R10,R11) using temp registers (R14,R15,CX,BX,SI)
-	REDUCE(DI,R8,R9,R10,R11,R14,R15,CX,BX,SI)
-
-	ADDQ s5-48(SP), DI
-	ADCQ s6-56(SP), R8
-	ADCQ s7-64(SP), R9
-	ADCQ s8-72(SP), R10
-	ADCQ s9-80(SP), R11
-
-	// reduce element(DI,R8,R9,R10,R11) using temp registers (R14,R15,CX,BX,SI)
-	REDUCE(DI,R8,R9,R10,R11,R14,R15,CX,BX,SI)
-
-	MOVQ DI, 0(AX)
-	MOVQ R8, 8(AX)
-	MOVQ R9, 16(AX)
-	MOVQ R10, 24(AX)
-	MOVQ R11, 32(AX)
-	RET
-
-l4:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	MOVQ y+16(FP), AX
-	MOVQ AX, 16(SP)
-	CALL ·mulGenericE2(SB)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls24-317/fp/element_amd64.s b/ecc/bls24-317/fp/element_amd64.s
--- a/ecc/bls24-317/fp/element_amd64.s
+++ b/ecc/bls24-317/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 4600622797032586825
-#include "../../../field/asm/element_5w/element_5w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls24-317/fr/element_amd64.s b/ecc/bls24-317/fr/element_amd64.s
--- a/ecc/bls24-317/fr/element_amd64.s
+++ b/ecc/bls24-317/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls24-317/fr/element_arm64.s b/ecc/bls24-317/fr/element_arm64.s
--- a/ecc/bls24-317/fr/element_arm64.s
+++ b/ecc/bls24-317/fr/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bls24-317/internal/fptower/e2_amd64.s b/ecc/bls24-317/internal/fptower/e2_amd64.s
--- a/ecc/bls24-317/internal/fptower/e2_amd64.s
+++ b/ecc/bls24-317/internal/fptower/e2_amd64.s
@@ -1,269 +1,1 @@
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-#include "textflag.h"
-#include "funcdata.h"
-
-// modulus q
-DATA q<>+0(SB)/8, $0x8d512e565dab2aab
-DATA q<>+8(SB)/8, $0xd6f339e43424bf7e
-DATA q<>+16(SB)/8, $0x169a61e684c73446
-DATA q<>+24(SB)/8, $0xf28fc5a0b7f9d039
-DATA q<>+32(SB)/8, $0x1058ca226f60892c
-GLOBL q<>(SB), (RODATA+NOPTR), $40
-
-// qInv0 q'[0]
-DATA qInv0<>(SB)/8, $0x55b5e0028b047ffd
-GLOBL qInv0<>(SB), (RODATA+NOPTR), $8
-
-#define REDUCE(ra0, ra1, ra2, ra3, ra4, rb0, rb1, rb2, rb3, rb4) \
-	MOVQ    ra0, rb0;        \
-	SUBQ    q<>(SB), ra0;    \
-	MOVQ    ra1, rb1;        \
-	SBBQ    q<>+8(SB), ra1;  \
-	MOVQ    ra2, rb2;        \
-	SBBQ    q<>+16(SB), ra2; \
-	MOVQ    ra3, rb3;        \
-	SBBQ    q<>+24(SB), ra3; \
-	MOVQ    ra4, rb4;        \
-	SBBQ    q<>+32(SB), ra4; \
-	CMOVQCS rb0, ra0;        \
-	CMOVQCS rb1, ra1;        \
-	CMOVQCS rb2, ra2;        \
-	CMOVQCS rb3, ra3;        \
-	CMOVQCS rb4, ra4;        \
-
-TEXT ·addE2(SB), NOSPLIT, $0-24
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), BX
-	MOVQ 8(AX), SI
-	MOVQ 16(AX), DI
-	MOVQ 24(AX), R8
-	MOVQ 32(AX), R9
-	MOVQ y+16(FP), DX
-	ADDQ 0(DX), BX
-	ADCQ 8(DX), SI
-	ADCQ 16(DX), DI
-	ADCQ 24(DX), R8
-	ADCQ 32(DX), R9
-
-	// reduce element(BX,SI,DI,R8,R9) using temp registers (R10,R11,R12,R13,R14)
-	REDUCE(BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	MOVQ res+0(FP), CX
-	MOVQ BX, 0(CX)
-	MOVQ SI, 8(CX)
-	MOVQ DI, 16(CX)
-	MOVQ R8, 24(CX)
-	MOVQ R9, 32(CX)
-	MOVQ 40(AX), BX
-	MOVQ 48(AX), SI
-	MOVQ 56(AX), DI
-	MOVQ 64(AX), R8
-	MOVQ 72(AX), R9
-	ADDQ 40(DX), BX
-	ADCQ 48(DX), SI
-	ADCQ 56(DX), DI
-	ADCQ 64(DX), R8
-	ADCQ 72(DX), R9
-
-	// reduce element(BX,SI,DI,R8,R9) using temp registers (R15,R10,R11,R12,R13)
-	REDUCE(BX,SI,DI,R8,R9,R15,R10,R11,R12,R13)
-
-	MOVQ BX, 40(CX)
-	MOVQ SI, 48(CX)
-	MOVQ DI, 56(CX)
-	MOVQ R8, 64(CX)
-	MOVQ R9, 72(CX)
-	RET
-
-TEXT ·doubleE2(SB), NOSPLIT, $0-16
-	MOVQ res+0(FP), DX
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), CX
-	MOVQ 8(AX), BX
-	MOVQ 16(AX), SI
-	MOVQ 24(AX), DI
-	MOVQ 32(AX), R8
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13)
-
-	MOVQ CX, 0(DX)
-	MOVQ BX, 8(DX)
-	MOVQ SI, 16(DX)
-	MOVQ DI, 24(DX)
-	MOVQ R8, 32(DX)
-	MOVQ 40(AX), CX
-	MOVQ 48(AX), BX
-	MOVQ 56(AX), SI
-	MOVQ 64(AX), DI
-	MOVQ 72(AX), R8
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(CX,BX,SI,DI,R8) using temp registers (R14,R15,R9,R10,R11)
-	REDUCE(CX,BX,SI,DI,R8,R14,R15,R9,R10,R11)
-
-	MOVQ CX, 40(DX)
-	MOVQ BX, 48(DX)
-	MOVQ SI, 56(DX)
-	MOVQ DI, 64(DX)
-	MOVQ R8, 72(DX)
-	RET
-
-TEXT ·subE2(SB), NOSPLIT, $0-24
-	XORQ    R8, R8
-	MOVQ    x+8(FP), DI
-	MOVQ    0(DI), AX
-	MOVQ    8(DI), DX
-	MOVQ    16(DI), CX
-	MOVQ    24(DI), BX
-	MOVQ    32(DI), SI
-	MOVQ    y+16(FP), DI
-	SUBQ    0(DI), AX
-	SBBQ    8(DI), DX
-	SBBQ    16(DI), CX
-	SBBQ    24(DI), BX
-	SBBQ    32(DI), SI
-	MOVQ    x+8(FP), DI
-	MOVQ    $0x8d512e565dab2aab, R9
-	MOVQ    $0xd6f339e43424bf7e, R10
-	MOVQ    $0x169a61e684c73446, R11
-	MOVQ    $0xf28fc5a0b7f9d039, R12
-	MOVQ    $0x1058ca226f60892c, R13
-	CMOVQCC R8, R9
-	CMOVQCC R8, R10
-	CMOVQCC R8, R11
-	CMOVQCC R8, R12
-	CMOVQCC R8, R13
-	ADDQ    R9, AX
-	ADCQ    R10, DX
-	ADCQ    R11, CX
-	ADCQ    R12, BX
-	ADCQ    R13, SI
-	MOVQ    res+0(FP), R14
-	MOVQ    AX, 0(R14)
-	MOVQ    DX, 8(R14)
-	MOVQ    CX, 16(R14)
-	MOVQ    BX, 24(R14)
-	MOVQ    SI, 32(R14)
-	MOVQ    40(DI), AX
-	MOVQ    48(DI), DX
-	MOVQ    56(DI), CX
-	MOVQ    64(DI), BX
-	MOVQ    72(DI), SI
-	MOVQ    y+16(FP), DI
-	SUBQ    40(DI), AX
-	SBBQ    48(DI), DX
-	SBBQ    56(DI), CX
-	SBBQ    64(DI), BX
-	SBBQ    72(DI), SI
-	MOVQ    $0x8d512e565dab2aab, R15
-	MOVQ    $0xd6f339e43424bf7e, R9
-	MOVQ    $0x169a61e684c73446, R10
-	MOVQ    $0xf28fc5a0b7f9d039, R11
-	MOVQ    $0x1058ca226f60892c, R12
-	CMOVQCC R8, R15
-	CMOVQCC R8, R9
-	CMOVQCC R8, R10
-	CMOVQCC R8, R11
-	CMOVQCC R8, R12
-	ADDQ    R15, AX
-	ADCQ    R9, DX
-	ADCQ    R10, CX
-	ADCQ    R11, BX
-	ADCQ    R12, SI
-	MOVQ    res+0(FP), DI
-	MOVQ    AX, 40(DI)
-	MOVQ    DX, 48(DI)
-	MOVQ    CX, 56(DI)
-	MOVQ    BX, 64(DI)
-	MOVQ    SI, 72(DI)
-	RET
-
-TEXT ·negE2(SB), NOSPLIT, $0-16
-	MOVQ  res+0(FP), DX
-	MOVQ  x+8(FP), AX
-	MOVQ  0(AX), BX
-	MOVQ  8(AX), SI
-	MOVQ  16(AX), DI
-	MOVQ  24(AX), R8
-	MOVQ  32(AX), R9
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	ORQ   R9, AX
-	TESTQ AX, AX
-	JNE   l1
-	MOVQ  AX, 0(DX)
-	MOVQ  AX, 8(DX)
-	MOVQ  AX, 16(DX)
-	MOVQ  AX, 24(DX)
-	MOVQ  AX, 32(DX)
-	JMP   l3
-
-l1:
-	MOVQ $0x8d512e565dab2aab, CX
-	SUBQ BX, CX
-	MOVQ CX, 0(DX)
-	MOVQ $0xd6f339e43424bf7e, CX
-	SBBQ SI, CX
-	MOVQ CX, 8(DX)
-	MOVQ $0x169a61e684c73446, CX
-	SBBQ DI, CX
-	MOVQ CX, 16(DX)
-	MOVQ $0xf28fc5a0b7f9d039, CX
-	SBBQ R8, CX
-	MOVQ CX, 24(DX)
-	MOVQ $0x1058ca226f60892c, CX
-	SBBQ R9, CX
-	MOVQ CX, 32(DX)
-
-l3:
-	MOVQ  x+8(FP), AX
-	MOVQ  40(AX), BX
-	MOVQ  48(AX), SI
-	MOVQ  56(AX), DI
-	MOVQ  64(AX), R8
-	MOVQ  72(AX), R9
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	ORQ   R9, AX
-	TESTQ AX, AX
-	JNE   l2
-	MOVQ  AX, 40(DX)
-	MOVQ  AX, 48(DX)
-	MOVQ  AX, 56(DX)
-	MOVQ  AX, 64(DX)
-	MOVQ  AX, 72(DX)
-	RET
-
-l2:
-	MOVQ $0x8d512e565dab2aab, CX
-	SUBQ BX, CX
-	MOVQ CX, 40(DX)
-	MOVQ $0xd6f339e43424bf7e, CX
-	SBBQ SI, CX
-	MOVQ CX, 48(DX)
-	MOVQ $0x169a61e684c73446, CX
-	SBBQ DI, CX
-	MOVQ CX, 56(DX)
-	MOVQ $0xf28fc5a0b7f9d039, CX
-	SBBQ R8, CX
-	MOVQ CX, 64(DX)
-	MOVQ $0x1058ca226f60892c, CX
-	SBBQ R9, CX
-	MOVQ CX, 72(DX)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bn254/fp/element_amd64.s b/ecc/bn254/fp/element_amd64.s
--- a/ecc/bn254/fp/element_amd64.s
+++ b/ecc/bn254/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bn254/fp/element_arm64.s b/ecc/bn254/fp/element_arm64.s
--- a/ecc/bn254/fp/element_arm64.s
+++ b/ecc/bn254/fp/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bn254/fr/element_amd64.s b/ecc/bn254/fr/element_amd64.s
--- a/ecc/bn254/fr/element_amd64.s
+++ b/ecc/bn254/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bn254/fr/element_arm64.s b/ecc/bn254/fr/element_arm64.s
--- a/ecc/bn254/fr/element_arm64.s
+++ b/ecc/bn254/fr/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bn254/internal/fptower/e2_amd64.s b/ecc/bn254/internal/fptower/e2_amd64.s
--- a/ecc/bn254/internal/fptower/e2_amd64.s
+++ b/ecc/bn254/internal/fptower/e2_amd64.s
@@ -1,652 +1,1 @@
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define REDUCE(ra0, ra1, ra2, ra3, rb0, rb1, rb2, rb3) \
-	MOVQ    ra0, rb0;              \
-	SUBQ    ·qElement(SB), ra0;    \
-	MOVQ    ra1, rb1;              \
-	SBBQ    ·qElement+8(SB), ra1;  \
-	MOVQ    ra2, rb2;              \
-	SBBQ    ·qElement+16(SB), ra2; \
-	MOVQ    ra3, rb3;              \
-	SBBQ    ·qElement+24(SB), ra3; \
-	CMOVQCS rb0, ra0;              \
-	CMOVQCS rb1, ra1;              \
-	CMOVQCS rb2, ra2;              \
-	CMOVQCS rb3, ra3;              \
-
-// this code is generated and identical to fp.Mul(...)
-// A -> BP
-// t[0] -> R10
-// t[1] -> R11
-// t[2] -> R12
-// t[3] -> R13
-#define MACC(in0, in1, in2) \
-	ADCXQ in0, in1     \
-	MULXQ in2, AX, in0 \
-	ADOXQ AX, in1      \
-
-#define DIV_SHIFT() \
-	PUSHQ BP                         \
-	MOVQ  $const_qInvNeg, DX         \
-	IMULQ R10, DX                    \
-	XORQ  AX, AX                     \
-	MULXQ ·qElement+0(SB), AX, BP    \
-	ADCXQ R10, AX                    \
-	MOVQ  BP, R10                    \
-	POPQ  BP                         \
-	MACC(R11, R10, ·qElement+8(SB))  \
-	MACC(R12, R11, ·qElement+16(SB)) \
-	MACC(R13, R12, ·qElement+24(SB)) \
-	MOVQ  $0, AX                     \
-	ADCXQ AX, R13                    \
-	ADOXQ BP, R13                    \
-
-#define MUL_WORD_0() \
-	XORQ  AX, AX        \
-	MULXQ R14, R10, R11 \
-	MULXQ R15, AX, R12  \
-	ADOXQ AX, R11       \
-	MULXQ CX, AX, R13   \
-	ADOXQ AX, R12       \
-	MULXQ BX, AX, BP    \
-	ADOXQ AX, R13       \
-	MOVQ  $0, AX        \
-	ADOXQ AX, BP        \
-	DIV_SHIFT()         \
-
-#define MUL_WORD_N() \
-	XORQ  AX, AX       \
-	MULXQ R14, AX, BP  \
-	ADOXQ AX, R10      \
-	MACC(BP, R11, R15) \
-	MACC(BP, R12, CX)  \
-	MACC(BP, R13, BX)  \
-	MOVQ  $0, AX       \
-	ADCXQ AX, BP       \
-	ADOXQ AX, BP       \
-	DIV_SHIFT()        \
-
-#define MUL() \
-	MOVQ SI, DX;  \
-	MUL_WORD_0(); \
-	MOVQ DI, DX;  \
-	MUL_WORD_N(); \
-	MOVQ R8, DX;  \
-	MUL_WORD_N(); \
-	MOVQ R9, DX;  \
-	MUL_WORD_N(); \
-
-TEXT ·addE2(SB), NOSPLIT, $0-24
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), BX
-	MOVQ 8(AX), SI
-	MOVQ 16(AX), DI
-	MOVQ 24(AX), R8
-	MOVQ y+16(FP), DX
-	ADDQ 0(DX), BX
-	ADCQ 8(DX), SI
-	ADCQ 16(DX), DI
-	ADCQ 24(DX), R8
-
-	// reduce element(BX,SI,DI,R8) using temp registers (R9,R10,R11,R12)
-	REDUCE(BX,SI,DI,R8,R9,R10,R11,R12)
-
-	MOVQ res+0(FP), CX
-	MOVQ BX, 0(CX)
-	MOVQ SI, 8(CX)
-	MOVQ DI, 16(CX)
-	MOVQ R8, 24(CX)
-	MOVQ 32(AX), BX
-	MOVQ 40(AX), SI
-	MOVQ 48(AX), DI
-	MOVQ 56(AX), R8
-	ADDQ 32(DX), BX
-	ADCQ 40(DX), SI
-	ADCQ 48(DX), DI
-	ADCQ 56(DX), R8
-
-	// reduce element(BX,SI,DI,R8) using temp registers (R13,R14,R15,R9)
-	REDUCE(BX,SI,DI,R8,R13,R14,R15,R9)
-
-	MOVQ BX, 32(CX)
-	MOVQ SI, 40(CX)
-	MOVQ DI, 48(CX)
-	MOVQ R8, 56(CX)
-	RET
-
-TEXT ·doubleE2(SB), NOSPLIT, $0-16
-	MOVQ res+0(FP), DX
-	MOVQ x+8(FP), AX
-	MOVQ 0(AX), CX
-	MOVQ 8(AX), BX
-	MOVQ 16(AX), SI
-	MOVQ 24(AX), DI
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(CX,BX,SI,DI) using temp registers (R8,R9,R10,R11)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11)
-
-	MOVQ CX, 0(DX)
-	MOVQ BX, 8(DX)
-	MOVQ SI, 16(DX)
-	MOVQ DI, 24(DX)
-	MOVQ 32(AX), CX
-	MOVQ 40(AX), BX
-	MOVQ 48(AX), SI
-	MOVQ 56(AX), DI
-	ADDQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(CX,BX,SI,DI) using temp registers (R12,R13,R14,R15)
-	REDUCE(CX,BX,SI,DI,R12,R13,R14,R15)
-
-	MOVQ CX, 32(DX)
-	MOVQ BX, 40(DX)
-	MOVQ SI, 48(DX)
-	MOVQ DI, 56(DX)
-	RET
-
-TEXT ·subE2(SB), NOSPLIT, $0-24
-	XORQ    DI, DI
-	MOVQ    x+8(FP), SI
-	MOVQ    0(SI), AX
-	MOVQ    8(SI), DX
-	MOVQ    16(SI), CX
-	MOVQ    24(SI), BX
-	MOVQ    y+16(FP), SI
-	SUBQ    0(SI), AX
-	SBBQ    8(SI), DX
-	SBBQ    16(SI), CX
-	SBBQ    24(SI), BX
-	MOVQ    x+8(FP), SI
-	MOVQ    $0x3c208c16d87cfd47, R8
-	MOVQ    $0x97816a916871ca8d, R9
-	MOVQ    $0xb85045b68181585d, R10
-	MOVQ    $0x30644e72e131a029, R11
-	CMOVQCC DI, R8
-	CMOVQCC DI, R9
-	CMOVQCC DI, R10
-	CMOVQCC DI, R11
-	ADDQ    R8, AX
-	ADCQ    R9, DX
-	ADCQ    R10, CX
-	ADCQ    R11, BX
-	MOVQ    res+0(FP), R12
-	MOVQ    AX, 0(R12)
-	MOVQ    DX, 8(R12)
-	MOVQ    CX, 16(R12)
-	MOVQ    BX, 24(R12)
-	MOVQ    32(SI), AX
-	MOVQ    40(SI), DX
-	MOVQ    48(SI), CX
-	MOVQ    56(SI), BX
-	MOVQ    y+16(FP), SI
-	SUBQ    32(SI), AX
-	SBBQ    40(SI), DX
-	SBBQ    48(SI), CX
-	SBBQ    56(SI), BX
-	MOVQ    $0x3c208c16d87cfd47, R13
-	MOVQ    $0x97816a916871ca8d, R14
-	MOVQ    $0xb85045b68181585d, R15
-	MOVQ    $0x30644e72e131a029, R8
-	CMOVQCC DI, R13
-	CMOVQCC DI, R14
-	CMOVQCC DI, R15
-	CMOVQCC DI, R8
-	ADDQ    R13, AX
-	ADCQ    R14, DX
-	ADCQ    R15, CX
-	ADCQ    R8, BX
-	MOVQ    res+0(FP), SI
-	MOVQ    AX, 32(SI)
-	MOVQ    DX, 40(SI)
-	MOVQ    CX, 48(SI)
-	MOVQ    BX, 56(SI)
-	RET
-
-TEXT ·negE2(SB), NOSPLIT, $0-16
-	MOVQ  res+0(FP), DX
-	MOVQ  x+8(FP), AX
-	MOVQ  0(AX), BX
-	MOVQ  8(AX), SI
-	MOVQ  16(AX), DI
-	MOVQ  24(AX), R8
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	TESTQ AX, AX
-	JNE   l1
-	MOVQ  AX, 0(DX)
-	MOVQ  AX, 8(DX)
-	MOVQ  AX, 16(DX)
-	MOVQ  AX, 24(DX)
-	JMP   l3
-
-l1:
-	MOVQ $0x3c208c16d87cfd47, CX
-	SUBQ BX, CX
-	MOVQ CX, 0(DX)
-	MOVQ $0x97816a916871ca8d, CX
-	SBBQ SI, CX
-	MOVQ CX, 8(DX)
-	MOVQ $0xb85045b68181585d, CX
-	SBBQ DI, CX
-	MOVQ CX, 16(DX)
-	MOVQ $0x30644e72e131a029, CX
-	SBBQ R8, CX
-	MOVQ CX, 24(DX)
-
-l3:
-	MOVQ  x+8(FP), AX
-	MOVQ  32(AX), BX
-	MOVQ  40(AX), SI
-	MOVQ  48(AX), DI
-	MOVQ  56(AX), R8
-	MOVQ  BX, AX
-	ORQ   SI, AX
-	ORQ   DI, AX
-	ORQ   R8, AX
-	TESTQ AX, AX
-	JNE   l2
-	MOVQ  AX, 32(DX)
-	MOVQ  AX, 40(DX)
-	MOVQ  AX, 48(DX)
-	MOVQ  AX, 56(DX)
-	RET
-
-l2:
-	MOVQ $0x3c208c16d87cfd47, CX
-	SUBQ BX, CX
-	MOVQ CX, 32(DX)
-	MOVQ $0x97816a916871ca8d, CX
-	SBBQ SI, CX
-	MOVQ CX, 40(DX)
-	MOVQ $0xb85045b68181585d, CX
-	SBBQ DI, CX
-	MOVQ CX, 48(DX)
-	MOVQ $0x30644e72e131a029, CX
-	SBBQ R8, CX
-	MOVQ CX, 56(DX)
-	RET
-
-TEXT ·mulNonResE2(SB), NOSPLIT, $0-16
-	MOVQ x+8(FP), R10
-	MOVQ 0(R10), AX
-	MOVQ 8(R10), DX
-	MOVQ 16(R10), CX
-	MOVQ 24(R10), BX
-	ADDQ AX, AX
-	ADCQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-
-	// reduce element(AX,DX,CX,BX) using temp registers (R11,R12,R13,R14)
-	REDUCE(AX,DX,CX,BX,R11,R12,R13,R14)
-
-	ADDQ AX, AX
-	ADCQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-
-	// reduce element(AX,DX,CX,BX) using temp registers (R15,R11,R12,R13)
-	REDUCE(AX,DX,CX,BX,R15,R11,R12,R13)
-
-	ADDQ AX, AX
-	ADCQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-
-	// reduce element(AX,DX,CX,BX) using temp registers (R14,R15,R11,R12)
-	REDUCE(AX,DX,CX,BX,R14,R15,R11,R12)
-
-	ADDQ 0(R10), AX
-	ADCQ 8(R10), DX
-	ADCQ 16(R10), CX
-	ADCQ 24(R10), BX
-
-	// reduce element(AX,DX,CX,BX) using temp registers (R13,R14,R15,R11)
-	REDUCE(AX,DX,CX,BX,R13,R14,R15,R11)
-
-	MOVQ    32(R10), SI
-	MOVQ    40(R10), DI
-	MOVQ    48(R10), R8
-	MOVQ    56(R10), R9
-	XORQ    R12, R12
-	SUBQ    SI, AX
-	SBBQ    DI, DX
-	SBBQ    R8, CX
-	SBBQ    R9, BX
-	MOVQ    $0x3c208c16d87cfd47, R13
-	MOVQ    $0x97816a916871ca8d, R14
-	MOVQ    $0xb85045b68181585d, R15
-	MOVQ    $0x30644e72e131a029, R11
-	CMOVQCC R12, R13
-	CMOVQCC R12, R14
-	CMOVQCC R12, R15
-	CMOVQCC R12, R11
-	ADDQ    R13, AX
-	ADCQ    R14, DX
-	ADCQ    R15, CX
-	ADCQ    R11, BX
-	ADDQ    SI, SI
-	ADCQ    DI, DI
-	ADCQ    R8, R8
-	ADCQ    R9, R9
-
-	// reduce element(SI,DI,R8,R9) using temp registers (R13,R14,R15,R11)
-	REDUCE(SI,DI,R8,R9,R13,R14,R15,R11)
-
-	ADDQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-
-	// reduce element(SI,DI,R8,R9) using temp registers (R12,R13,R14,R15)
-	REDUCE(SI,DI,R8,R9,R12,R13,R14,R15)
-
-	ADDQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-
-	// reduce element(SI,DI,R8,R9) using temp registers (R11,R12,R13,R14)
-	REDUCE(SI,DI,R8,R9,R11,R12,R13,R14)
-
-	ADDQ 32(R10), SI
-	ADCQ 40(R10), DI
-	ADCQ 48(R10), R8
-	ADCQ 56(R10), R9
-
-	// reduce element(SI,DI,R8,R9) using temp registers (R15,R11,R12,R13)
-	REDUCE(SI,DI,R8,R9,R15,R11,R12,R13)
-
-	ADDQ 0(R10), SI
-	ADCQ 8(R10), DI
-	ADCQ 16(R10), R8
-	ADCQ 24(R10), R9
-
-	// reduce element(SI,DI,R8,R9) using temp registers (R14,R15,R11,R12)
-	REDUCE(SI,DI,R8,R9,R14,R15,R11,R12)
-
-	MOVQ res+0(FP), R10
-	MOVQ AX, 0(R10)
-	MOVQ DX, 8(R10)
-	MOVQ CX, 16(R10)
-	MOVQ BX, 24(R10)
-	MOVQ SI, 32(R10)
-	MOVQ DI, 40(R10)
-	MOVQ R8, 48(R10)
-	MOVQ R9, 56(R10)
-	RET
-
-TEXT ·mulAdxE2(SB), $64-24
-	NO_LOCAL_POINTERS
-
-	// var a, b, c fp.Element
-	// a.Add(&x.A0, &x.A1)
-	// b.Add(&y.A0, &y.A1)
-	// a.Mul(&a, &b)
-	// b.Mul(&x.A0, &y.A0)
-	// c.Mul(&x.A1, &y.A1)
-	// z.A1.Sub(&a, &b).Sub(&z.A1, &c)
-	// z.A0.Sub(&b, &c)
-
-	CMPB ·supportAdx(SB), $1
-	JNE  l4
-	MOVQ x+8(FP), AX
-	MOVQ y+16(FP), DX
-	MOVQ 32(AX), R14
-	MOVQ 40(AX), R15
-	MOVQ 48(AX), CX
-	MOVQ 56(AX), BX
-	MOVQ 32(DX), SI
-	MOVQ 40(DX), DI
-	MOVQ 48(DX), R8
-	MOVQ 56(DX), R9
-
-	// mul (R14,R15,CX,BX) with (SI,DI,R8,R9) into (R10,R11,R12,R13)
-	MUL()
-
-	// reduce element(R10,R11,R12,R13) using temp registers (SI,DI,R8,R9)
-	REDUCE(R10,R11,R12,R13,SI,DI,R8,R9)
-
-	MOVQ R10, s4-40(SP)
-	MOVQ R11, s5-48(SP)
-	MOVQ R12, s6-56(SP)
-	MOVQ R13, s7-64(SP)
-	MOVQ x+8(FP), AX
-	MOVQ y+16(FP), DX
-	ADDQ 0(AX), R14
-	ADCQ 8(AX), R15
-	ADCQ 16(AX), CX
-	ADCQ 24(AX), BX
-	MOVQ 0(DX), SI
-	MOVQ 8(DX), DI
-	MOVQ 16(DX), R8
-	MOVQ 24(DX), R9
-	ADDQ 32(DX), SI
-	ADCQ 40(DX), DI
-	ADCQ 48(DX), R8
-	ADCQ 56(DX), R9
-
-	// mul (R14,R15,CX,BX) with (SI,DI,R8,R9) into (R10,R11,R12,R13)
-	MUL()
-
-	// reduce element(R10,R11,R12,R13) using temp registers (SI,DI,R8,R9)
-	REDUCE(R10,R11,R12,R13,SI,DI,R8,R9)
-
-	MOVQ R10, s0-8(SP)
-	MOVQ R11, s1-16(SP)
-	MOVQ R12, s2-24(SP)
-	MOVQ R13, s3-32(SP)
-	MOVQ x+8(FP), AX
-	MOVQ y+16(FP), DX
-	MOVQ 0(AX), R14
-	MOVQ 8(AX), R15
-	MOVQ 16(AX), CX
-	MOVQ 24(AX), BX
-	MOVQ 0(DX), SI
-	MOVQ 8(DX), DI
-	MOVQ 16(DX), R8
-	MOVQ 24(DX), R9
-
-	// mul (R14,R15,CX,BX) with (SI,DI,R8,R9) into (R10,R11,R12,R13)
-	MUL()
-
-	// reduce element(R10,R11,R12,R13) using temp registers (SI,DI,R8,R9)
-	REDUCE(R10,R11,R12,R13,SI,DI,R8,R9)
-
-	XORQ    DX, DX
-	MOVQ    s0-8(SP), R14
-	MOVQ    s1-16(SP), R15
-	MOVQ    s2-24(SP), CX
-	MOVQ    s3-32(SP), BX
-	SUBQ    R10, R14
-	SBBQ    R11, R15
-	SBBQ    R12, CX
-	SBBQ    R13, BX
-	MOVQ    $0x3c208c16d87cfd47, SI
-	MOVQ    $0x97816a916871ca8d, DI
-	MOVQ    $0xb85045b68181585d, R8
-	MOVQ    $0x30644e72e131a029, R9
-	CMOVQCC DX, SI
-	CMOVQCC DX, DI
-	CMOVQCC DX, R8
-	CMOVQCC DX, R9
-	ADDQ    SI, R14
-	ADCQ    DI, R15
-	ADCQ    R8, CX
-	ADCQ    R9, BX
-	SUBQ    s4-40(SP), R14
-	SBBQ    s5-48(SP), R15
-	SBBQ    s6-56(SP), CX
-	SBBQ    s7-64(SP), BX
-	MOVQ    $0x3c208c16d87cfd47, SI
-	MOVQ    $0x97816a916871ca8d, DI
-	MOVQ    $0xb85045b68181585d, R8
-	MOVQ    $0x30644e72e131a029, R9
-	CMOVQCC DX, SI
-	CMOVQCC DX, DI
-	CMOVQCC DX, R8
-	CMOVQCC DX, R9
-	ADDQ    SI, R14
-	ADCQ    DI, R15
-	ADCQ    R8, CX
-	ADCQ    R9, BX
-	MOVQ    res+0(FP), AX
-	MOVQ    R14, 32(AX)
-	MOVQ    R15, 40(AX)
-	MOVQ    CX, 48(AX)
-	MOVQ    BX, 56(AX)
-	MOVQ    s4-40(SP), SI
-	MOVQ    s5-48(SP), DI
-	MOVQ    s6-56(SP), R8
-	MOVQ    s7-64(SP), R9
-	SUBQ    SI, R10
-	SBBQ    DI, R11
-	SBBQ    R8, R12
-	SBBQ    R9, R13
-	MOVQ    $0x3c208c16d87cfd47, R14
-	MOVQ    $0x97816a916871ca8d, R15
-	MOVQ    $0xb85045b68181585d, CX
-	MOVQ    $0x30644e72e131a029, BX
-	CMOVQCC DX, R14
-	CMOVQCC DX, R15
-	CMOVQCC DX, CX
-	CMOVQCC DX, BX
-	ADDQ    R14, R10
-	ADCQ    R15, R11
-	ADCQ    CX, R12
-	ADCQ    BX, R13
-	MOVQ    R10, 0(AX)
-	MOVQ    R11, 8(AX)
-	MOVQ    R12, 16(AX)
-	MOVQ    R13, 24(AX)
-	RET
-
-l4:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	MOVQ y+16(FP), AX
-	MOVQ AX, 16(SP)
-	CALL ·mulGenericE2(SB)
-	RET
-
-TEXT ·squareAdxE2(SB), $16-16
-	NO_LOCAL_POINTERS
-
-	// z.A0 = (x.A0 + x.A1) * (x.A0 - x.A1)
-	// z.A1 = 2 * x.A0 * x.A1
-
-	CMPB ·supportAdx(SB), $1
-	JNE  l5
-
-	// 2 * x.A0 * x.A1
-	MOVQ x+8(FP), AX
-
-	// x.A0[0] -> SI
-	// x.A0[1] -> DI
-	// x.A0[2] -> R8
-	// x.A0[3] -> R9
-	MOVQ 0(AX), SI
-	MOVQ 8(AX), DI
-	MOVQ 16(AX), R8
-	MOVQ 24(AX), R9
-
-	// 2 * x.A1[0] -> R14
-	// 2 * x.A1[1] -> R15
-	// 2 * x.A1[2] -> CX
-	// 2 * x.A1[3] -> BX
-	MOVQ 32(AX), R14
-	MOVQ 40(AX), R15
-	MOVQ 48(AX), CX
-	MOVQ 56(AX), BX
-	ADDQ R14, R14
-	ADCQ R15, R15
-	ADCQ CX, CX
-	ADCQ BX, BX
-
-	// mul (R14,R15,CX,BX) with (SI,DI,R8,R9) into (R10,R11,R12,R13)
-	MUL()
-
-	// reduce element(R10,R11,R12,R13) using temp registers (R14,R15,CX,BX)
-	REDUCE(R10,R11,R12,R13,R14,R15,CX,BX)
-
-	MOVQ x+8(FP), AX
-
-	// x.A1[0] -> R14
-	// x.A1[1] -> R15
-	// x.A1[2] -> CX
-	// x.A1[3] -> BX
-	MOVQ 32(AX), R14
-	MOVQ 40(AX), R15
-	MOVQ 48(AX), CX
-	MOVQ 56(AX), BX
-	MOVQ res+0(FP), DX
-	MOVQ R10, 32(DX)
-	MOVQ R11, 40(DX)
-	MOVQ R12, 48(DX)
-	MOVQ R13, 56(DX)
-	MOVQ R14, R10
-	MOVQ R15, R11
-	MOVQ CX, R12
-	MOVQ BX, R13
-
-	// Add(&x.A0, &x.A1)
-	ADDQ SI, R14
-	ADCQ DI, R15
-	ADCQ R8, CX
-	ADCQ R9, BX
-	XORQ BP, BP
-
-	// Sub(&x.A0, &x.A1)
-	SUBQ    R10, SI
-	SBBQ    R11, DI
-	SBBQ    R12, R8
-	SBBQ    R13, R9
-	MOVQ    $0x3c208c16d87cfd47, R10
-	MOVQ    $0x97816a916871ca8d, R11
-	MOVQ    $0xb85045b68181585d, R12
-	MOVQ    $0x30644e72e131a029, R13
-	CMOVQCC BP, R10
-	CMOVQCC BP, R11
-	CMOVQCC BP, R12
-	CMOVQCC BP, R13
-	ADDQ    R10, SI
-	ADCQ    R11, DI
-	ADCQ    R12, R8
-	ADCQ    R13, R9
-
-	// mul (R14,R15,CX,BX) with (SI,DI,R8,R9) into (R10,R11,R12,R13)
-	MUL()
-
-	// reduce element(R10,R11,R12,R13) using temp registers (R14,R15,CX,BX)
-	REDUCE(R10,R11,R12,R13,R14,R15,CX,BX)
-
-	MOVQ res+0(FP), AX
-	MOVQ R10, 0(AX)
-	MOVQ R11, 8(AX)
-	MOVQ R12, 16(AX)
-	MOVQ R13, 24(AX)
-	RET
-
-l5:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	CALL ·squareGenericE2(SB)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bw6-633/fp/element_amd64.s b/ecc/bw6-633/fp/element_amd64.s
--- a/ecc/bw6-633/fp/element_amd64.s
+++ b/ecc/bw6-633/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 15320537321640126458
-#include "../../../field/asm/element_10w/element_10w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bw6-633/fp/element_arm64.s b/ecc/bw6-633/fp/element_arm64.s
--- a/ecc/bw6-633/fp/element_arm64.s
+++ b/ecc/bw6-633/fp/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 4283725514119985738
-#include "../../../field/asm/element_10w/element_10w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bw6-633/fr/element_amd64.s b/ecc/bw6-633/fr/element_amd64.s
--- a/ecc/bw6-633/fr/element_amd64.s
+++ b/ecc/bw6-633/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 4600622797032586825
-#include "../../../field/asm/element_5w/element_5w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bw6-761/fp/element_amd64.s b/ecc/bw6-761/fp/element_amd64.s
--- a/ecc/bw6-761/fp/element_amd64.s
+++ b/ecc/bw6-761/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 9592094295531092101
-#include "../../../field/asm/element_12w/element_12w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bw6-761/fp/element_arm64.s b/ecc/bw6-761/fp/element_arm64.s
--- a/ecc/bw6-761/fp/element_arm64.s
+++ b/ecc/bw6-761/fp/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 17465962485072383759
-#include "../../../field/asm/element_12w/element_12w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bw6-761/fr/element_amd64.s b/ecc/bw6-761/fr/element_amd64.s
--- a/ecc/bw6-761/fr/element_amd64.s
+++ b/ecc/bw6-761/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 18408829383245254329
-#include "../../../field/asm/element_6w/element_6w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/bw6-761/fr/element_arm64.s b/ecc/bw6-761/fr/element_arm64.s
--- a/ecc/bw6-761/fr/element_arm64.s
+++ b/ecc/bw6-761/fr/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 15397482240260640864
-#include "../../../field/asm/element_6w/element_6w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/grumpkin/fp/element_amd64.s b/ecc/grumpkin/fp/element_amd64.s
--- a/ecc/grumpkin/fp/element_amd64.s
+++ b/ecc/grumpkin/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/grumpkin/fp/element_arm64.s b/ecc/grumpkin/fp/element_arm64.s
--- a/ecc/grumpkin/fp/element_arm64.s
+++ b/ecc/grumpkin/fp/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/grumpkin/fr/element_amd64.s b/ecc/grumpkin/fr/element_amd64.s
--- a/ecc/grumpkin/fr/element_amd64.s
+++ b/ecc/grumpkin/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/grumpkin/fr/element_arm64.s b/ecc/grumpkin/fr/element_arm64.s
--- a/ecc/grumpkin/fr/element_arm64.s
+++ b/ecc/grumpkin/fr/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/stark-curve/fp/element_amd64.s b/ecc/stark-curve/fp/element_amd64.s
--- a/ecc/stark-curve/fp/element_amd64.s
+++ b/ecc/stark-curve/fp/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/stark-curve/fp/element_arm64.s b/ecc/stark-curve/fp/element_arm64.s
--- a/ecc/stark-curve/fp/element_arm64.s
+++ b/ecc/stark-curve/fp/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/stark-curve/fr/element_amd64.s b/ecc/stark-curve/fr/element_amd64.s
--- a/ecc/stark-curve/fr/element_amd64.s
+++ b/ecc/stark-curve/fr/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14652627197992229521
-#include "../../../field/asm/element_4w/element_4w_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/ecc/stark-curve/fr/element_arm64.s b/ecc/stark-curve/fr/element_arm64.s
--- a/ecc/stark-curve/fr/element_arm64.s
+++ b/ecc/stark-curve/fr/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 1501560133179981797
-#include "../../../field/asm/element_4w/element_4w_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_10w/element_10w_amd64.s b/field/asm/element_10w/element_10w_amd64.s
--- a/field/asm/element_10w/element_10w_amd64.s
+++ b/field/asm/element_10w/element_10w_amd64.s
@@ -1,1187 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define REDUCE(ra0, ra1, ra2, ra3, ra4, ra5, ra6, ra7, ra8, ra9, rb0, rb1, rb2, rb3, rb4, rb5, rb6, rb7, rb8, rb9) \
-	MOVQ    ra0, rb0;              \
-	SUBQ    ·qElement(SB), ra0;    \
-	MOVQ    ra1, rb1;              \
-	SBBQ    ·qElement+8(SB), ra1;  \
-	MOVQ    ra2, rb2;              \
-	SBBQ    ·qElement+16(SB), ra2; \
-	MOVQ    ra3, rb3;              \
-	SBBQ    ·qElement+24(SB), ra3; \
-	MOVQ    ra4, rb4;              \
-	SBBQ    ·qElement+32(SB), ra4; \
-	MOVQ    ra5, rb5;              \
-	SBBQ    ·qElement+40(SB), ra5; \
-	MOVQ    ra6, rb6;              \
-	SBBQ    ·qElement+48(SB), ra6; \
-	MOVQ    ra7, rb7;              \
-	SBBQ    ·qElement+56(SB), ra7; \
-	MOVQ    ra8, rb8;              \
-	SBBQ    ·qElement+64(SB), ra8; \
-	MOVQ    ra9, rb9;              \
-	SBBQ    ·qElement+72(SB), ra9; \
-	CMOVQCS rb0, ra0;              \
-	CMOVQCS rb1, ra1;              \
-	CMOVQCS rb2, ra2;              \
-	CMOVQCS rb3, ra3;              \
-	CMOVQCS rb4, ra4;              \
-	CMOVQCS rb5, ra5;              \
-	CMOVQCS rb6, ra6;              \
-	CMOVQCS rb7, ra7;              \
-	CMOVQCS rb8, ra8;              \
-	CMOVQCS rb9, ra9;              \
-
-TEXT ·reduce(SB), $56-8
-	MOVQ res+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	RET
-
-// MulBy3(x *Element)
-TEXT ·MulBy3(SB), $56-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-	ADCQ 48(AX), R9
-	ADCQ 56(AX), R10
-	ADCQ 64(AX), R11
-	ADCQ 72(AX), R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	RET
-
-// MulBy5(x *Element)
-TEXT ·MulBy5(SB), $56-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-	ADCQ 48(AX), R9
-	ADCQ 56(AX), R10
-	ADCQ 64(AX), R11
-	ADCQ 72(AX), R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	RET
-
-// MulBy13(x *Element)
-TEXT ·MulBy13(SB), $136-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP),s12-104(SP),s13-112(SP),s14-120(SP),s15-128(SP),s16-136(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP),s12-104(SP),s13-112(SP),s14-120(SP),s15-128(SP),s16-136(SP))
-
-	MOVQ DX, s7-64(SP)
-	MOVQ CX, s8-72(SP)
-	MOVQ BX, s9-80(SP)
-	MOVQ SI, s10-88(SP)
-	MOVQ DI, s11-96(SP)
-	MOVQ R8, s12-104(SP)
-	MOVQ R9, s13-112(SP)
-	MOVQ R10, s14-120(SP)
-	MOVQ R11, s15-128(SP)
-	MOVQ R12, s16-136(SP)
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	ADDQ s7-64(SP), DX
-	ADCQ s8-72(SP), CX
-	ADCQ s9-80(SP), BX
-	ADCQ s10-88(SP), SI
-	ADCQ s11-96(SP), DI
-	ADCQ s12-104(SP), R8
-	ADCQ s13-112(SP), R9
-	ADCQ s14-120(SP), R10
-	ADCQ s15-128(SP), R11
-	ADCQ s16-136(SP), R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-	ADCQ 48(AX), R9
-	ADCQ 56(AX), R10
-	ADCQ 64(AX), R11
-	ADCQ 72(AX), R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	RET
-
-// Butterfly(a, b *Element) sets a = a + b; b = a - b
-TEXT ·Butterfly(SB), $56-16
-	MOVQ b+8(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	MOVQ a+0(FP), AX
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-	ADCQ 48(AX), R9
-	ADCQ 56(AX), R10
-	ADCQ 64(AX), R11
-	ADCQ 72(AX), R12
-	MOVQ DX, R13
-	MOVQ CX, R14
-	MOVQ BX, R15
-	MOVQ SI, s0-8(SP)
-	MOVQ DI, s1-16(SP)
-	MOVQ R8, s2-24(SP)
-	MOVQ R9, s3-32(SP)
-	MOVQ R10, s4-40(SP)
-	MOVQ R11, s5-48(SP)
-	MOVQ R12, s6-56(SP)
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	MOVQ b+8(FP), AX
-	SUBQ 0(AX), DX
-	SBBQ 8(AX), CX
-	SBBQ 16(AX), BX
-	SBBQ 24(AX), SI
-	SBBQ 32(AX), DI
-	SBBQ 40(AX), R8
-	SBBQ 48(AX), R9
-	SBBQ 56(AX), R10
-	SBBQ 64(AX), R11
-	SBBQ 72(AX), R12
-	JCC  noReduce_1
-	MOVQ $const_q0, AX
-	ADDQ AX, DX
-	MOVQ $const_q1, AX
-	ADCQ AX, CX
-	MOVQ $const_q2, AX
-	ADCQ AX, BX
-	MOVQ $const_q3, AX
-	ADCQ AX, SI
-	MOVQ $const_q4, AX
-	ADCQ AX, DI
-	MOVQ $const_q5, AX
-	ADCQ AX, R8
-	MOVQ $const_q6, AX
-	ADCQ AX, R9
-	MOVQ $const_q7, AX
-	ADCQ AX, R10
-	MOVQ $const_q8, AX
-	ADCQ AX, R11
-	MOVQ $const_q9, AX
-	ADCQ AX, R12
-
-noReduce_1:
-	MOVQ b+8(FP), AX
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	MOVQ R13, DX
-	MOVQ R14, CX
-	MOVQ R15, BX
-	MOVQ s0-8(SP), SI
-	MOVQ s1-16(SP), DI
-	MOVQ s2-24(SP), R8
-	MOVQ s3-32(SP), R9
-	MOVQ s4-40(SP), R10
-	MOVQ s5-48(SP), R11
-	MOVQ s6-56(SP), R12
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP))
-
-	MOVQ a+0(FP), AX
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	RET
-
-// mul(res, x, y *Element)
-TEXT ·mul(SB), $64-24
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
-
-	NO_LOCAL_POINTERS
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_2
-	MOVQ x+8(FP), R12
-	MOVQ y+16(FP), R13
-
-	// A -> BP
-	// t[0] -> R14
-	// t[1] -> R15
-	// t[2] -> CX
-	// t[3] -> BX
-	// t[4] -> SI
-	// t[5] -> DI
-	// t[6] -> R8
-	// t[7] -> R9
-	// t[8] -> R10
-	// t[9] -> R11
-#define MACC(in0, in1, in2) \
-	ADCXQ in0, in1     \
-	MULXQ in2, AX, in0 \
-	ADOXQ AX, in1      \
-
-#define DIV_SHIFT() \
-	PUSHQ BP                         \
-	MOVQ  $const_qInvNeg, DX         \
-	IMULQ R14, DX                    \
-	XORQ  AX, AX                     \
-	MULXQ ·qElement+0(SB), AX, BP    \
-	ADCXQ R14, AX                    \
-	MOVQ  BP, R14                    \
-	POPQ  BP                         \
-	MACC(R15, R14, ·qElement+8(SB))  \
-	MACC(CX, R15, ·qElement+16(SB))  \
-	MACC(BX, CX, ·qElement+24(SB))   \
-	MACC(SI, BX, ·qElement+32(SB))   \
-	MACC(DI, SI, ·qElement+40(SB))   \
-	MACC(R8, DI, ·qElement+48(SB))   \
-	MACC(R9, R8, ·qElement+56(SB))   \
-	MACC(R10, R9, ·qElement+64(SB))  \
-	MACC(R11, R10, ·qElement+72(SB)) \
-	MOVQ  $0, AX                     \
-	ADCXQ AX, R11                    \
-	ADOXQ BP, R11                    \
-
-#define MUL_WORD_0() \
-	XORQ  AX, AX           \
-	MULXQ 0(R12), R14, R15 \
-	MULXQ 8(R12), AX, CX   \
-	ADOXQ AX, R15          \
-	MULXQ 16(R12), AX, BX  \
-	ADOXQ AX, CX           \
-	MULXQ 24(R12), AX, SI  \
-	ADOXQ AX, BX           \
-	MULXQ 32(R12), AX, DI  \
-	ADOXQ AX, SI           \
-	MULXQ 40(R12), AX, R8  \
-	ADOXQ AX, DI           \
-	MULXQ 48(R12), AX, R9  \
-	ADOXQ AX, R8           \
-	MULXQ 56(R12), AX, R10 \
-	ADOXQ AX, R9           \
-	MULXQ 64(R12), AX, R11 \
-	ADOXQ AX, R10          \
-	MULXQ 72(R12), AX, BP  \
-	ADOXQ AX, R11          \
-	MOVQ  $0, AX           \
-	ADOXQ AX, BP           \
-	DIV_SHIFT()            \
-
-#define MUL_WORD_N() \
-	XORQ  AX, AX           \
-	MULXQ 0(R12), AX, BP   \
-	ADOXQ AX, R14          \
-	MACC(BP, R15, 8(R12))  \
-	MACC(BP, CX, 16(R12))  \
-	MACC(BP, BX, 24(R12))  \
-	MACC(BP, SI, 32(R12))  \
-	MACC(BP, DI, 40(R12))  \
-	MACC(BP, R8, 48(R12))  \
-	MACC(BP, R9, 56(R12))  \
-	MACC(BP, R10, 64(R12)) \
-	MACC(BP, R11, 72(R12)) \
-	MOVQ  $0, AX           \
-	ADCXQ AX, BP           \
-	ADOXQ AX, BP           \
-	DIV_SHIFT()            \
-
-	// mul body
-	MOVQ 0(R13), DX
-	MUL_WORD_0()
-	MOVQ 8(R13), DX
-	MUL_WORD_N()
-	MOVQ 16(R13), DX
-	MUL_WORD_N()
-	MOVQ 24(R13), DX
-	MUL_WORD_N()
-	MOVQ 32(R13), DX
-	MUL_WORD_N()
-	MOVQ 40(R13), DX
-	MUL_WORD_N()
-	MOVQ 48(R13), DX
-	MUL_WORD_N()
-	MOVQ 56(R13), DX
-	MUL_WORD_N()
-	MOVQ 64(R13), DX
-	MUL_WORD_N()
-	MOVQ 72(R13), DX
-	MUL_WORD_N()
-
-	// reduce element(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11) using temp registers (R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP))
-	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP))
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R15, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	MOVQ SI, 32(AX)
-	MOVQ DI, 40(AX)
-	MOVQ R8, 48(AX)
-	MOVQ R9, 56(AX)
-	MOVQ R10, 64(AX)
-	MOVQ R11, 72(AX)
-	RET
-
-noAdx_2:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	MOVQ y+16(FP), AX
-	MOVQ AX, 16(SP)
-	CALL ·_mulGeneric(SB)
-	RET
-
-TEXT ·fromMont(SB), $64-8
-	NO_LOCAL_POINTERS
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// when y = 1 we have:
-	// for i=0 to N-1
-	// 		t[i] = x[i]
-	// for i=0 to N-1
-	// 		m := t[0]*q'[0] mod W
-	// 		C,_ := t[0] + m*q[0]
-	// 		for j=1 to N-1
-	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
-	// 		t[N-1] = C
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_3
-	MOVQ res+0(FP), DX
-	MOVQ 0(DX), R14
-	MOVQ 8(DX), R15
-	MOVQ 16(DX), CX
-	MOVQ 24(DX), BX
-	MOVQ 32(DX), SI
-	MOVQ 40(DX), DI
-	MOVQ 48(DX), R8
-	MOVQ 56(DX), R9
-	MOVQ 64(DX), R10
-	MOVQ 72(DX), R11
-	XORQ DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-	MOVQ  $0, AX
-	ADCXQ AX, R11
-	ADOXQ AX, R11
-
-	// reduce element(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11) using temp registers (R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP))
-	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP))
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R15, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	MOVQ SI, 32(AX)
-	MOVQ DI, 40(AX)
-	MOVQ R8, 48(AX)
-	MOVQ R9, 56(AX)
-	MOVQ R10, 64(AX)
-	MOVQ R11, 72(AX)
-	RET
-
-noAdx_3:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	CALL ·_fromMontGeneric(SB)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_10w/element_10w_arm64.s b/field/asm/element_10w/element_10w_arm64.s
--- a/field/asm/element_10w/element_10w_arm64.s
+++ b/field/asm/element_10w/element_10w_arm64.s
@@ -1,266 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-// mul(res, x, y *Element)
-// Algorithm 2 of Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS
-// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-TEXT ·mul(SB), NOFRAME|NOSPLIT, $0-24
-#define DIVSHIFT() \
-	MOVD  $const_qInvNeg, R0   \
-	MUL   R12, R0, R1          \
-	MOVD  ·qElement+0(SB), R0  \
-	MUL   R0, R1, R0           \
-	ADDS  R0, R12, R12         \
-	MOVD  ·qElement+8(SB), R0  \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R13, R13         \
-	MOVD  ·qElement+16(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R14, R14         \
-	MOVD  ·qElement+24(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R15, R15         \
-	MOVD  ·qElement+32(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R16, R16         \
-	MOVD  ·qElement+40(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R17, R17         \
-	MOVD  ·qElement+48(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R19, R19         \
-	MOVD  ·qElement+56(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R20, R20         \
-	MOVD  ·qElement+64(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R21, R21         \
-	MOVD  ·qElement+72(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R22, R22         \
-	ADC   R23, ZR, R23         \
-	MOVD  ·qElement+0(SB), R0  \
-	UMULH R0, R1, R0           \
-	ADDS  R0, R13, R12         \
-	MOVD  ·qElement+8(SB), R0  \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R14, R13         \
-	MOVD  ·qElement+16(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R15, R14         \
-	MOVD  ·qElement+24(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R16, R15         \
-	MOVD  ·qElement+32(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R17, R16         \
-	MOVD  ·qElement+40(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R19, R17         \
-	MOVD  ·qElement+48(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R20, R19         \
-	MOVD  ·qElement+56(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R21, R20         \
-	MOVD  ·qElement+64(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R22, R21         \
-	MOVD  ·qElement+72(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R23, R22         \
-
-#define MUL_WORD_N() \
-	MUL   R2, R1, R0   \
-	ADDS  R0, R12, R12 \
-	MUL   R3, R1, R0   \
-	ADCS  R0, R13, R13 \
-	MUL   R4, R1, R0   \
-	ADCS  R0, R14, R14 \
-	MUL   R5, R1, R0   \
-	ADCS  R0, R15, R15 \
-	MUL   R6, R1, R0   \
-	ADCS  R0, R16, R16 \
-	MUL   R7, R1, R0   \
-	ADCS  R0, R17, R17 \
-	MUL   R8, R1, R0   \
-	ADCS  R0, R19, R19 \
-	MUL   R9, R1, R0   \
-	ADCS  R0, R20, R20 \
-	MUL   R10, R1, R0  \
-	ADCS  R0, R21, R21 \
-	MUL   R11, R1, R0  \
-	ADCS  R0, R22, R22 \
-	ADC   ZR, ZR, R23  \
-	UMULH R2, R1, R0   \
-	ADDS  R0, R13, R13 \
-	UMULH R3, R1, R0   \
-	ADCS  R0, R14, R14 \
-	UMULH R4, R1, R0   \
-	ADCS  R0, R15, R15 \
-	UMULH R5, R1, R0   \
-	ADCS  R0, R16, R16 \
-	UMULH R6, R1, R0   \
-	ADCS  R0, R17, R17 \
-	UMULH R7, R1, R0   \
-	ADCS  R0, R19, R19 \
-	UMULH R8, R1, R0   \
-	ADCS  R0, R20, R20 \
-	UMULH R9, R1, R0   \
-	ADCS  R0, R21, R21 \
-	UMULH R10, R1, R0  \
-	ADCS  R0, R22, R22 \
-	UMULH R11, R1, R0  \
-	ADC   R0, R23, R23 \
-	DIVSHIFT()         \
-
-#define MUL_WORD_0() \
-	MUL   R2, R1, R12  \
-	MUL   R3, R1, R13  \
-	MUL   R4, R1, R14  \
-	MUL   R5, R1, R15  \
-	MUL   R6, R1, R16  \
-	MUL   R7, R1, R17  \
-	MUL   R8, R1, R19  \
-	MUL   R9, R1, R20  \
-	MUL   R10, R1, R21 \
-	MUL   R11, R1, R22 \
-	UMULH R2, R1, R0   \
-	ADDS  R0, R13, R13 \
-	UMULH R3, R1, R0   \
-	ADCS  R0, R14, R14 \
-	UMULH R4, R1, R0   \
-	ADCS  R0, R15, R15 \
-	UMULH R5, R1, R0   \
-	ADCS  R0, R16, R16 \
-	UMULH R6, R1, R0   \
-	ADCS  R0, R17, R17 \
-	UMULH R7, R1, R0   \
-	ADCS  R0, R19, R19 \
-	UMULH R8, R1, R0   \
-	ADCS  R0, R20, R20 \
-	UMULH R9, R1, R0   \
-	ADCS  R0, R21, R21 \
-	UMULH R10, R1, R0  \
-	ADCS  R0, R22, R22 \
-	UMULH R11, R1, R0  \
-	ADC   R0, ZR, R23  \
-	DIVSHIFT()         \
-
-	MOVD y+16(FP), R1
-	MOVD x+8(FP), R0
-	LDP  0(R0), (R2, R3)
-	LDP  16(R0), (R4, R5)
-	LDP  32(R0), (R6, R7)
-	LDP  48(R0), (R8, R9)
-	LDP  64(R0), (R10, R11)
-	MOVD y+16(FP), R1
-	MOVD 0(R1), R1
-	MUL_WORD_0()
-	MOVD y+16(FP), R1
-	MOVD 8(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 16(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 24(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 32(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 40(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 48(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 56(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 64(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 72(R1), R1
-	MUL_WORD_N()
-	LDP  ·qElement+0(SB), (R2, R3)
-	LDP  ·qElement+16(SB), (R4, R5)
-	LDP  ·qElement+32(SB), (R6, R7)
-	LDP  ·qElement+48(SB), (R8, R9)
-	LDP  ·qElement+64(SB), (R10, R11)
-
-	// reduce if necessary
-	SUBS R2, R12, R2
-	SBCS R3, R13, R3
-	SBCS R4, R14, R4
-	SBCS R5, R15, R5
-	SBCS R6, R16, R6
-	SBCS R7, R17, R7
-	SBCS R8, R19, R8
-	SBCS R9, R20, R9
-	SBCS R10, R21, R10
-	SBCS R11, R22, R11
-	MOVD res+0(FP), R0
-	CSEL CS, R2, R12, R12
-	CSEL CS, R3, R13, R13
-	STP  (R12, R13), 0(R0)
-	CSEL CS, R4, R14, R14
-	CSEL CS, R5, R15, R15
-	STP  (R14, R15), 16(R0)
-	CSEL CS, R6, R16, R16
-	CSEL CS, R7, R17, R17
-	STP  (R16, R17), 32(R0)
-	CSEL CS, R8, R19, R19
-	CSEL CS, R9, R20, R20
-	STP  (R19, R20), 48(R0)
-	CSEL CS, R10, R21, R21
-	CSEL CS, R11, R22, R22
-	STP  (R21, R22), 64(R0)
-	RET
-
-// reduce(res *Element)
-TEXT ·reduce(SB), NOFRAME|NOSPLIT, $0-8
-	LDP  ·qElement+0(SB), (R10, R11)
-	LDP  ·qElement+16(SB), (R12, R13)
-	LDP  ·qElement+32(SB), (R14, R15)
-	LDP  ·qElement+48(SB), (R16, R17)
-	LDP  ·qElement+64(SB), (R19, R20)
-	MOVD res+0(FP), R21
-	LDP  0(R21), (R0, R1)
-	LDP  16(R21), (R2, R3)
-	LDP  32(R21), (R4, R5)
-	LDP  48(R21), (R6, R7)
-	LDP  64(R21), (R8, R9)
-
-	// q = t - q
-	SUBS R10, R0, R10
-	SBCS R11, R1, R11
-	SBCS R12, R2, R12
-	SBCS R13, R3, R13
-	SBCS R14, R4, R14
-	SBCS R15, R5, R15
-	SBCS R16, R6, R16
-	SBCS R17, R7, R17
-	SBCS R19, R8, R19
-	SBCS R20, R9, R20
-
-	// if no borrow, return q, else return t
-	CSEL CS, R10, R0, R0
-	CSEL CS, R11, R1, R1
-	STP  (R0, R1), 0(R21)
-	CSEL CS, R12, R2, R2
-	CSEL CS, R13, R3, R3
-	STP  (R2, R3), 16(R21)
-	CSEL CS, R14, R4, R4
-	CSEL CS, R15, R5, R5
-	STP  (R4, R5), 32(R21)
-	CSEL CS, R16, R6, R6
-	CSEL CS, R17, R7, R7
-	STP  (R6, R7), 48(R21)
-	CSEL CS, R19, R8, R8
-	CSEL CS, R20, R9, R9
-	STP  (R8, R9), 64(R21)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_12w/element_12w_amd64.s b/field/asm/element_12w/element_12w_amd64.s
--- a/field/asm/element_12w/element_12w_amd64.s
+++ b/field/asm/element_12w/element_12w_amd64.s
@@ -1,1557 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define REDUCE(ra0, ra1, ra2, ra3, ra4, ra5, ra6, ra7, ra8, ra9, ra10, ra11, rb0, rb1, rb2, rb3, rb4, rb5, rb6, rb7, rb8, rb9, rb10, rb11) \
-	MOVQ    ra0, rb0;               \
-	SUBQ    ·qElement(SB), ra0;     \
-	MOVQ    ra1, rb1;               \
-	SBBQ    ·qElement+8(SB), ra1;   \
-	MOVQ    ra2, rb2;               \
-	SBBQ    ·qElement+16(SB), ra2;  \
-	MOVQ    ra3, rb3;               \
-	SBBQ    ·qElement+24(SB), ra3;  \
-	MOVQ    ra4, rb4;               \
-	SBBQ    ·qElement+32(SB), ra4;  \
-	MOVQ    ra5, rb5;               \
-	SBBQ    ·qElement+40(SB), ra5;  \
-	MOVQ    ra6, rb6;               \
-	SBBQ    ·qElement+48(SB), ra6;  \
-	MOVQ    ra7, rb7;               \
-	SBBQ    ·qElement+56(SB), ra7;  \
-	MOVQ    ra8, rb8;               \
-	SBBQ    ·qElement+64(SB), ra8;  \
-	MOVQ    ra9, rb9;               \
-	SBBQ    ·qElement+72(SB), ra9;  \
-	MOVQ    ra10, rb10;             \
-	SBBQ    ·qElement+80(SB), ra10; \
-	MOVQ    ra11, rb11;             \
-	SBBQ    ·qElement+88(SB), ra11; \
-	CMOVQCS rb0, ra0;               \
-	CMOVQCS rb1, ra1;               \
-	CMOVQCS rb2, ra2;               \
-	CMOVQCS rb3, ra3;               \
-	CMOVQCS rb4, ra4;               \
-	CMOVQCS rb5, ra5;               \
-	CMOVQCS rb6, ra6;               \
-	CMOVQCS rb7, ra7;               \
-	CMOVQCS rb8, ra8;               \
-	CMOVQCS rb9, ra9;               \
-	CMOVQCS rb10, ra10;             \
-	CMOVQCS rb11, ra11;             \
-
-TEXT ·reduce(SB), $88-8
-	MOVQ res+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	MOVQ 80(AX), R13
-	MOVQ 88(AX), R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	MOVQ R13, 80(AX)
-	MOVQ R14, 88(AX)
-	RET
-
-// MulBy3(x *Element)
-TEXT ·MulBy3(SB), $88-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	MOVQ 80(AX), R13
-	MOVQ 88(AX), R14
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-	ADCQ R13, R13
-	ADCQ R14, R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-	ADCQ 48(AX), R9
-	ADCQ 56(AX), R10
-	ADCQ 64(AX), R11
-	ADCQ 72(AX), R12
-	ADCQ 80(AX), R13
-	ADCQ 88(AX), R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	MOVQ R13, 80(AX)
-	MOVQ R14, 88(AX)
-	RET
-
-// MulBy5(x *Element)
-TEXT ·MulBy5(SB), $88-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	MOVQ 80(AX), R13
-	MOVQ 88(AX), R14
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-	ADCQ R13, R13
-	ADCQ R14, R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-	ADCQ R13, R13
-	ADCQ R14, R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-	ADCQ 48(AX), R9
-	ADCQ 56(AX), R10
-	ADCQ 64(AX), R11
-	ADCQ 72(AX), R12
-	ADCQ 80(AX), R13
-	ADCQ 88(AX), R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	MOVQ R13, 80(AX)
-	MOVQ R14, 88(AX)
-	RET
-
-// MulBy13(x *Element)
-TEXT ·MulBy13(SB), $184-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	MOVQ 80(AX), R13
-	MOVQ 88(AX), R14
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-	ADCQ R13, R13
-	ADCQ R14, R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-	ADCQ R13, R13
-	ADCQ R14, R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (s11-96(SP),s12-104(SP),s13-112(SP),s14-120(SP),s15-128(SP),s16-136(SP),s17-144(SP),s18-152(SP),s19-160(SP),s20-168(SP),s21-176(SP),s22-184(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,s11-96(SP),s12-104(SP),s13-112(SP),s14-120(SP),s15-128(SP),s16-136(SP),s17-144(SP),s18-152(SP),s19-160(SP),s20-168(SP),s21-176(SP),s22-184(SP))
-
-	MOVQ DX, s11-96(SP)
-	MOVQ CX, s12-104(SP)
-	MOVQ BX, s13-112(SP)
-	MOVQ SI, s14-120(SP)
-	MOVQ DI, s15-128(SP)
-	MOVQ R8, s16-136(SP)
-	MOVQ R9, s17-144(SP)
-	MOVQ R10, s18-152(SP)
-	MOVQ R11, s19-160(SP)
-	MOVQ R12, s20-168(SP)
-	MOVQ R13, s21-176(SP)
-	MOVQ R14, s22-184(SP)
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-	ADCQ R9, R9
-	ADCQ R10, R10
-	ADCQ R11, R11
-	ADCQ R12, R12
-	ADCQ R13, R13
-	ADCQ R14, R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	ADDQ s11-96(SP), DX
-	ADCQ s12-104(SP), CX
-	ADCQ s13-112(SP), BX
-	ADCQ s14-120(SP), SI
-	ADCQ s15-128(SP), DI
-	ADCQ s16-136(SP), R8
-	ADCQ s17-144(SP), R9
-	ADCQ s18-152(SP), R10
-	ADCQ s19-160(SP), R11
-	ADCQ s20-168(SP), R12
-	ADCQ s21-176(SP), R13
-	ADCQ s22-184(SP), R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-	ADCQ 48(AX), R9
-	ADCQ 56(AX), R10
-	ADCQ 64(AX), R11
-	ADCQ 72(AX), R12
-	ADCQ 80(AX), R13
-	ADCQ 88(AX), R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	MOVQ R13, 80(AX)
-	MOVQ R14, 88(AX)
-	RET
-
-// Butterfly(a, b *Element) sets a = a + b; b = a - b
-TEXT ·Butterfly(SB), $88-16
-	MOVQ b+8(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	MOVQ 80(AX), R13
-	MOVQ 88(AX), R14
-	MOVQ a+0(FP), AX
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-	ADCQ 48(AX), R9
-	ADCQ 56(AX), R10
-	ADCQ 64(AX), R11
-	ADCQ 72(AX), R12
-	ADCQ 80(AX), R13
-	ADCQ 88(AX), R14
-	MOVQ DX, R15
-	MOVQ CX, s0-8(SP)
-	MOVQ BX, s1-16(SP)
-	MOVQ SI, s2-24(SP)
-	MOVQ DI, s3-32(SP)
-	MOVQ R8, s4-40(SP)
-	MOVQ R9, s5-48(SP)
-	MOVQ R10, s6-56(SP)
-	MOVQ R11, s7-64(SP)
-	MOVQ R12, s8-72(SP)
-	MOVQ R13, s9-80(SP)
-	MOVQ R14, s10-88(SP)
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	MOVQ 48(AX), R9
-	MOVQ 56(AX), R10
-	MOVQ 64(AX), R11
-	MOVQ 72(AX), R12
-	MOVQ 80(AX), R13
-	MOVQ 88(AX), R14
-	MOVQ b+8(FP), AX
-	SUBQ 0(AX), DX
-	SBBQ 8(AX), CX
-	SBBQ 16(AX), BX
-	SBBQ 24(AX), SI
-	SBBQ 32(AX), DI
-	SBBQ 40(AX), R8
-	SBBQ 48(AX), R9
-	SBBQ 56(AX), R10
-	SBBQ 64(AX), R11
-	SBBQ 72(AX), R12
-	SBBQ 80(AX), R13
-	SBBQ 88(AX), R14
-	JCC  noReduce_1
-	MOVQ $const_q0, AX
-	ADDQ AX, DX
-	MOVQ $const_q1, AX
-	ADCQ AX, CX
-	MOVQ $const_q2, AX
-	ADCQ AX, BX
-	MOVQ $const_q3, AX
-	ADCQ AX, SI
-	MOVQ $const_q4, AX
-	ADCQ AX, DI
-	MOVQ $const_q5, AX
-	ADCQ AX, R8
-	MOVQ $const_q6, AX
-	ADCQ AX, R9
-	MOVQ $const_q7, AX
-	ADCQ AX, R10
-	MOVQ $const_q8, AX
-	ADCQ AX, R11
-	MOVQ $const_q9, AX
-	ADCQ AX, R12
-	MOVQ $const_q10, AX
-	ADCQ AX, R13
-	MOVQ $const_q11, AX
-	ADCQ AX, R14
-
-noReduce_1:
-	MOVQ b+8(FP), AX
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	MOVQ R13, 80(AX)
-	MOVQ R14, 88(AX)
-	MOVQ R15, DX
-	MOVQ s0-8(SP), CX
-	MOVQ s1-16(SP), BX
-	MOVQ s2-24(SP), SI
-	MOVQ s3-32(SP), DI
-	MOVQ s4-40(SP), R8
-	MOVQ s5-48(SP), R9
-	MOVQ s6-56(SP), R10
-	MOVQ s7-64(SP), R11
-	MOVQ s8-72(SP), R12
-	MOVQ s9-80(SP), R13
-	MOVQ s10-88(SP), R14
-
-	// reduce element(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP))
-
-	MOVQ a+0(FP), AX
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	MOVQ R9, 48(AX)
-	MOVQ R10, 56(AX)
-	MOVQ R11, 64(AX)
-	MOVQ R12, 72(AX)
-	MOVQ R13, 80(AX)
-	MOVQ R14, 88(AX)
-	RET
-
-// mul(res, x, y *Element)
-TEXT ·mul(SB), $96-24
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
-
-	NO_LOCAL_POINTERS
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_2
-	MOVQ x+8(FP), AX
-
-	// x[0] -> s0-8(SP)
-	// x[1] -> s1-16(SP)
-	// x[2] -> s2-24(SP)
-	// x[3] -> s3-32(SP)
-	// x[4] -> s4-40(SP)
-	// x[5] -> s5-48(SP)
-	// x[6] -> s6-56(SP)
-	// x[7] -> s7-64(SP)
-	// x[8] -> s8-72(SP)
-	// x[9] -> s9-80(SP)
-	// x[10] -> s10-88(SP)
-	// x[11] -> s11-96(SP)
-	MOVQ 0(AX), R14
-	MOVQ 8(AX), R15
-	MOVQ 16(AX), CX
-	MOVQ 24(AX), BX
-	MOVQ 32(AX), SI
-	MOVQ 40(AX), DI
-	MOVQ 48(AX), R8
-	MOVQ 56(AX), R9
-	MOVQ 64(AX), R10
-	MOVQ 72(AX), R11
-	MOVQ 80(AX), R12
-	MOVQ 88(AX), R13
-	MOVQ R14, s0-8(SP)
-	MOVQ R15, s1-16(SP)
-	MOVQ CX, s2-24(SP)
-	MOVQ BX, s3-32(SP)
-	MOVQ SI, s4-40(SP)
-	MOVQ DI, s5-48(SP)
-	MOVQ R8, s6-56(SP)
-	MOVQ R9, s7-64(SP)
-	MOVQ R10, s8-72(SP)
-	MOVQ R11, s9-80(SP)
-	MOVQ R12, s10-88(SP)
-	MOVQ R13, s11-96(SP)
-
-	// A -> BP
-	// t[0] -> R14
-	// t[1] -> R15
-	// t[2] -> CX
-	// t[3] -> BX
-	// t[4] -> SI
-	// t[5] -> DI
-	// t[6] -> R8
-	// t[7] -> R9
-	// t[8] -> R10
-	// t[9] -> R11
-	// t[10] -> R12
-	// t[11] -> R13
-#define MACC(in0, in1, in2) \
-	ADCXQ in0, in1     \
-	MULXQ in2, AX, in0 \
-	ADOXQ AX, in1      \
-
-#define DIV_SHIFT() \
-	PUSHQ BP                         \
-	MOVQ  $const_qInvNeg, DX         \
-	IMULQ R14, DX                    \
-	XORQ  AX, AX                     \
-	MULXQ ·qElement+0(SB), AX, BP    \
-	ADCXQ R14, AX                    \
-	MOVQ  BP, R14                    \
-	POPQ  BP                         \
-	MACC(R15, R14, ·qElement+8(SB))  \
-	MACC(CX, R15, ·qElement+16(SB))  \
-	MACC(BX, CX, ·qElement+24(SB))   \
-	MACC(SI, BX, ·qElement+32(SB))   \
-	MACC(DI, SI, ·qElement+40(SB))   \
-	MACC(R8, DI, ·qElement+48(SB))   \
-	MACC(R9, R8, ·qElement+56(SB))   \
-	MACC(R10, R9, ·qElement+64(SB))  \
-	MACC(R11, R10, ·qElement+72(SB)) \
-	MACC(R12, R11, ·qElement+80(SB)) \
-	MACC(R13, R12, ·qElement+88(SB)) \
-	MOVQ  $0, AX                     \
-	ADCXQ AX, R13                    \
-	ADOXQ BP, R13                    \
-
-#define MUL_WORD_0() \
-	XORQ  AX, AX              \
-	MULXQ s0-8(SP), R14, R15  \
-	MULXQ s1-16(SP), AX, CX   \
-	ADOXQ AX, R15             \
-	MULXQ s2-24(SP), AX, BX   \
-	ADOXQ AX, CX              \
-	MULXQ s3-32(SP), AX, SI   \
-	ADOXQ AX, BX              \
-	MULXQ s4-40(SP), AX, DI   \
-	ADOXQ AX, SI              \
-	MULXQ s5-48(SP), AX, R8   \
-	ADOXQ AX, DI              \
-	MULXQ s6-56(SP), AX, R9   \
-	ADOXQ AX, R8              \
-	MULXQ s7-64(SP), AX, R10  \
-	ADOXQ AX, R9              \
-	MULXQ s8-72(SP), AX, R11  \
-	ADOXQ AX, R10             \
-	MULXQ s9-80(SP), AX, R12  \
-	ADOXQ AX, R11             \
-	MULXQ s10-88(SP), AX, R13 \
-	ADOXQ AX, R12             \
-	MULXQ s11-96(SP), AX, BP  \
-	ADOXQ AX, R13             \
-	MOVQ  $0, AX              \
-	ADOXQ AX, BP              \
-	DIV_SHIFT()               \
-
-#define MUL_WORD_N() \
-	XORQ  AX, AX              \
-	MULXQ s0-8(SP), AX, BP    \
-	ADOXQ AX, R14             \
-	MACC(BP, R15, s1-16(SP))  \
-	MACC(BP, CX, s2-24(SP))   \
-	MACC(BP, BX, s3-32(SP))   \
-	MACC(BP, SI, s4-40(SP))   \
-	MACC(BP, DI, s5-48(SP))   \
-	MACC(BP, R8, s6-56(SP))   \
-	MACC(BP, R9, s7-64(SP))   \
-	MACC(BP, R10, s8-72(SP))  \
-	MACC(BP, R11, s9-80(SP))  \
-	MACC(BP, R12, s10-88(SP)) \
-	MACC(BP, R13, s11-96(SP)) \
-	MOVQ  $0, AX              \
-	ADCXQ AX, BP              \
-	ADOXQ AX, BP              \
-	DIV_SHIFT()               \
-
-	// mul body
-	MOVQ y+16(FP), AX
-	MOVQ 0(AX), DX
-	MUL_WORD_0()
-	MOVQ y+16(FP), AX
-	MOVQ 8(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 16(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 24(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 32(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 40(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 48(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 56(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 64(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 72(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 80(AX), DX
-	MUL_WORD_N()
-	MOVQ y+16(FP), AX
-	MOVQ 88(AX), DX
-	MUL_WORD_N()
-
-	// reduce element(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13) using temp registers (s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP))
-	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP))
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R15, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	MOVQ SI, 32(AX)
-	MOVQ DI, 40(AX)
-	MOVQ R8, 48(AX)
-	MOVQ R9, 56(AX)
-	MOVQ R10, 64(AX)
-	MOVQ R11, 72(AX)
-	MOVQ R12, 80(AX)
-	MOVQ R13, 88(AX)
-	RET
-
-noAdx_2:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	MOVQ y+16(FP), AX
-	MOVQ AX, 16(SP)
-	CALL ·_mulGeneric(SB)
-	RET
-
-TEXT ·fromMont(SB), $96-8
-	NO_LOCAL_POINTERS
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// when y = 1 we have:
-	// for i=0 to N-1
-	// 		t[i] = x[i]
-	// for i=0 to N-1
-	// 		m := t[0]*q'[0] mod W
-	// 		C,_ := t[0] + m*q[0]
-	// 		for j=1 to N-1
-	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
-	// 		t[N-1] = C
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_3
-	MOVQ res+0(FP), DX
-	MOVQ 0(DX), R14
-	MOVQ 8(DX), R15
-	MOVQ 16(DX), CX
-	MOVQ 24(DX), BX
-	MOVQ 32(DX), SI
-	MOVQ 40(DX), DI
-	MOVQ 48(DX), R8
-	MOVQ 56(DX), R9
-	MOVQ 64(DX), R10
-	MOVQ 72(DX), R11
-	MOVQ 80(DX), R12
-	MOVQ 88(DX), R13
-	XORQ DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-
-	// (C,t[5]) := t[6] + m*q[6] + C
-	ADCXQ R8, DI
-	MULXQ ·qElement+48(SB), AX, R8
-	ADOXQ AX, DI
-
-	// (C,t[6]) := t[7] + m*q[7] + C
-	ADCXQ R9, R8
-	MULXQ ·qElement+56(SB), AX, R9
-	ADOXQ AX, R8
-
-	// (C,t[7]) := t[8] + m*q[8] + C
-	ADCXQ R10, R9
-	MULXQ ·qElement+64(SB), AX, R10
-	ADOXQ AX, R9
-
-	// (C,t[8]) := t[9] + m*q[9] + C
-	ADCXQ R11, R10
-	MULXQ ·qElement+72(SB), AX, R11
-	ADOXQ AX, R10
-
-	// (C,t[9]) := t[10] + m*q[10] + C
-	ADCXQ R12, R11
-	MULXQ ·qElement+80(SB), AX, R12
-	ADOXQ AX, R11
-
-	// (C,t[10]) := t[11] + m*q[11] + C
-	ADCXQ R13, R12
-	MULXQ ·qElement+88(SB), AX, R13
-	ADOXQ AX, R12
-	MOVQ  $0, AX
-	ADCXQ AX, R13
-	ADOXQ AX, R13
-
-	// reduce element(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13) using temp registers (s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP))
-	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP),s5-48(SP),s6-56(SP),s7-64(SP),s8-72(SP),s9-80(SP),s10-88(SP),s11-96(SP))
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R15, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	MOVQ SI, 32(AX)
-	MOVQ DI, 40(AX)
-	MOVQ R8, 48(AX)
-	MOVQ R9, 56(AX)
-	MOVQ R10, 64(AX)
-	MOVQ R11, 72(AX)
-	MOVQ R12, 80(AX)
-	MOVQ R13, 88(AX)
-	RET
-
-noAdx_3:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	CALL ·_fromMontGeneric(SB)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_12w/element_12w_arm64.s b/field/asm/element_12w/element_12w_arm64.s
--- a/field/asm/element_12w/element_12w_arm64.s
+++ b/field/asm/element_12w/element_12w_arm64.s
@@ -1,312 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-// mul(res, x, y *Element)
-// Algorithm 2 of Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS
-// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-TEXT ·mul(SB), NOFRAME|NOSPLIT, $0-24
-#define DIVSHIFT() \
-	MOVD  $const_qInvNeg, R0   \
-	MUL   R14, R0, R1          \
-	MOVD  ·qElement+0(SB), R0  \
-	MUL   R0, R1, R0           \
-	ADDS  R0, R14, R14         \
-	MOVD  ·qElement+8(SB), R0  \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R15, R15         \
-	MOVD  ·qElement+16(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R16, R16         \
-	MOVD  ·qElement+24(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R17, R17         \
-	MOVD  ·qElement+32(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R19, R19         \
-	MOVD  ·qElement+40(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R20, R20         \
-	MOVD  ·qElement+48(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R21, R21         \
-	MOVD  ·qElement+56(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R22, R22         \
-	MOVD  ·qElement+64(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R23, R23         \
-	MOVD  ·qElement+72(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R24, R24         \
-	MOVD  ·qElement+80(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R25, R25         \
-	MOVD  ·qElement+88(SB), R0 \
-	MUL   R0, R1, R0           \
-	ADCS  R0, R26, R26         \
-	ADC   R29, ZR, R29         \
-	MOVD  ·qElement+0(SB), R0  \
-	UMULH R0, R1, R0           \
-	ADDS  R0, R15, R14         \
-	MOVD  ·qElement+8(SB), R0  \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R16, R15         \
-	MOVD  ·qElement+16(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R17, R16         \
-	MOVD  ·qElement+24(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R19, R17         \
-	MOVD  ·qElement+32(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R20, R19         \
-	MOVD  ·qElement+40(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R21, R20         \
-	MOVD  ·qElement+48(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R22, R21         \
-	MOVD  ·qElement+56(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R23, R22         \
-	MOVD  ·qElement+64(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R24, R23         \
-	MOVD  ·qElement+72(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R25, R24         \
-	MOVD  ·qElement+80(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R26, R25         \
-	MOVD  ·qElement+88(SB), R0 \
-	UMULH R0, R1, R0           \
-	ADCS  R0, R29, R26         \
-
-#define MUL_WORD_N() \
-	MUL   R2, R1, R0   \
-	ADDS  R0, R14, R14 \
-	MUL   R3, R1, R0   \
-	ADCS  R0, R15, R15 \
-	MUL   R4, R1, R0   \
-	ADCS  R0, R16, R16 \
-	MUL   R5, R1, R0   \
-	ADCS  R0, R17, R17 \
-	MUL   R6, R1, R0   \
-	ADCS  R0, R19, R19 \
-	MUL   R7, R1, R0   \
-	ADCS  R0, R20, R20 \
-	MUL   R8, R1, R0   \
-	ADCS  R0, R21, R21 \
-	MUL   R9, R1, R0   \
-	ADCS  R0, R22, R22 \
-	MUL   R10, R1, R0  \
-	ADCS  R0, R23, R23 \
-	MUL   R11, R1, R0  \
-	ADCS  R0, R24, R24 \
-	MUL   R12, R1, R0  \
-	ADCS  R0, R25, R25 \
-	MUL   R13, R1, R0  \
-	ADCS  R0, R26, R26 \
-	ADC   ZR, ZR, R29  \
-	UMULH R2, R1, R0   \
-	ADDS  R0, R15, R15 \
-	UMULH R3, R1, R0   \
-	ADCS  R0, R16, R16 \
-	UMULH R4, R1, R0   \
-	ADCS  R0, R17, R17 \
-	UMULH R5, R1, R0   \
-	ADCS  R0, R19, R19 \
-	UMULH R6, R1, R0   \
-	ADCS  R0, R20, R20 \
-	UMULH R7, R1, R0   \
-	ADCS  R0, R21, R21 \
-	UMULH R8, R1, R0   \
-	ADCS  R0, R22, R22 \
-	UMULH R9, R1, R0   \
-	ADCS  R0, R23, R23 \
-	UMULH R10, R1, R0  \
-	ADCS  R0, R24, R24 \
-	UMULH R11, R1, R0  \
-	ADCS  R0, R25, R25 \
-	UMULH R12, R1, R0  \
-	ADCS  R0, R26, R26 \
-	UMULH R13, R1, R0  \
-	ADC   R0, R29, R29 \
-	DIVSHIFT()         \
-
-#define MUL_WORD_0() \
-	MUL   R2, R1, R14  \
-	MUL   R3, R1, R15  \
-	MUL   R4, R1, R16  \
-	MUL   R5, R1, R17  \
-	MUL   R6, R1, R19  \
-	MUL   R7, R1, R20  \
-	MUL   R8, R1, R21  \
-	MUL   R9, R1, R22  \
-	MUL   R10, R1, R23 \
-	MUL   R11, R1, R24 \
-	MUL   R12, R1, R25 \
-	MUL   R13, R1, R26 \
-	UMULH R2, R1, R0   \
-	ADDS  R0, R15, R15 \
-	UMULH R3, R1, R0   \
-	ADCS  R0, R16, R16 \
-	UMULH R4, R1, R0   \
-	ADCS  R0, R17, R17 \
-	UMULH R5, R1, R0   \
-	ADCS  R0, R19, R19 \
-	UMULH R6, R1, R0   \
-	ADCS  R0, R20, R20 \
-	UMULH R7, R1, R0   \
-	ADCS  R0, R21, R21 \
-	UMULH R8, R1, R0   \
-	ADCS  R0, R22, R22 \
-	UMULH R9, R1, R0   \
-	ADCS  R0, R23, R23 \
-	UMULH R10, R1, R0  \
-	ADCS  R0, R24, R24 \
-	UMULH R11, R1, R0  \
-	ADCS  R0, R25, R25 \
-	UMULH R12, R1, R0  \
-	ADCS  R0, R26, R26 \
-	UMULH R13, R1, R0  \
-	ADC   R0, ZR, R29  \
-	DIVSHIFT()         \
-
-	MOVD y+16(FP), R1
-	MOVD x+8(FP), R0
-	LDP  0(R0), (R2, R3)
-	LDP  16(R0), (R4, R5)
-	LDP  32(R0), (R6, R7)
-	LDP  48(R0), (R8, R9)
-	LDP  64(R0), (R10, R11)
-	LDP  80(R0), (R12, R13)
-	MOVD y+16(FP), R1
-	MOVD 0(R1), R1
-	MUL_WORD_0()
-	MOVD y+16(FP), R1
-	MOVD 8(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 16(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 24(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 32(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 40(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 48(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 56(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 64(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 72(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 80(R1), R1
-	MUL_WORD_N()
-	MOVD y+16(FP), R1
-	MOVD 88(R1), R1
-	MUL_WORD_N()
-	LDP  ·qElement+0(SB), (R2, R3)
-	LDP  ·qElement+16(SB), (R4, R5)
-	LDP  ·qElement+32(SB), (R6, R7)
-	LDP  ·qElement+48(SB), (R8, R9)
-	LDP  ·qElement+64(SB), (R10, R11)
-	LDP  ·qElement+80(SB), (R12, R13)
-
-	// reduce if necessary
-	SUBS R2, R14, R2
-	SBCS R3, R15, R3
-	SBCS R4, R16, R4
-	SBCS R5, R17, R5
-	SBCS R6, R19, R6
-	SBCS R7, R20, R7
-	SBCS R8, R21, R8
-	SBCS R9, R22, R9
-	SBCS R10, R23, R10
-	SBCS R11, R24, R11
-	SBCS R12, R25, R12
-	SBCS R13, R26, R13
-	MOVD res+0(FP), R0
-	CSEL CS, R2, R14, R14
-	CSEL CS, R3, R15, R15
-	STP  (R14, R15), 0(R0)
-	CSEL CS, R4, R16, R16
-	CSEL CS, R5, R17, R17
-	STP  (R16, R17), 16(R0)
-	CSEL CS, R6, R19, R19
-	CSEL CS, R7, R20, R20
-	STP  (R19, R20), 32(R0)
-	CSEL CS, R8, R21, R21
-	CSEL CS, R9, R22, R22
-	STP  (R21, R22), 48(R0)
-	CSEL CS, R10, R23, R23
-	CSEL CS, R11, R24, R24
-	STP  (R23, R24), 64(R0)
-	CSEL CS, R12, R25, R25
-	CSEL CS, R13, R26, R26
-	STP  (R25, R26), 80(R0)
-	RET
-
-// reduce(res *Element)
-TEXT ·reduce(SB), NOFRAME|NOSPLIT, $0-8
-	LDP  ·qElement+0(SB), (R12, R13)
-	LDP  ·qElement+16(SB), (R14, R15)
-	LDP  ·qElement+32(SB), (R16, R17)
-	LDP  ·qElement+48(SB), (R19, R20)
-	LDP  ·qElement+64(SB), (R21, R22)
-	LDP  ·qElement+80(SB), (R23, R24)
-	MOVD res+0(FP), R25
-	LDP  0(R25), (R0, R1)
-	LDP  16(R25), (R2, R3)
-	LDP  32(R25), (R4, R5)
-	LDP  48(R25), (R6, R7)
-	LDP  64(R25), (R8, R9)
-	LDP  80(R25), (R10, R11)
-
-	// q = t - q
-	SUBS R12, R0, R12
-	SBCS R13, R1, R13
-	SBCS R14, R2, R14
-	SBCS R15, R3, R15
-	SBCS R16, R4, R16
-	SBCS R17, R5, R17
-	SBCS R19, R6, R19
-	SBCS R20, R7, R20
-	SBCS R21, R8, R21
-	SBCS R22, R9, R22
-	SBCS R23, R10, R23
-	SBCS R24, R11, R24
-
-	// if no borrow, return q, else return t
-	CSEL CS, R12, R0, R0
-	CSEL CS, R13, R1, R1
-	STP  (R0, R1), 0(R25)
-	CSEL CS, R14, R2, R2
-	CSEL CS, R15, R3, R3
-	STP  (R2, R3), 16(R25)
-	CSEL CS, R16, R4, R4
-	CSEL CS, R17, R5, R5
-	STP  (R4, R5), 32(R25)
-	CSEL CS, R19, R6, R6
-	CSEL CS, R20, R7, R7
-	STP  (R6, R7), 48(R25)
-	CSEL CS, R21, R8, R8
-	CSEL CS, R22, R9, R9
-	STP  (R8, R9), 64(R25)
-	CSEL CS, R23, R10, R10
-	CSEL CS, R24, R11, R11
-	STP  (R10, R11), 80(R25)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_31b/element_31b_amd64.s b/field/asm/element_31b/element_31b_amd64.s
--- a/field/asm/element_31b/element_31b_amd64.s
+++ b/field/asm/element_31b/element_31b_amd64.s
@@ -1,270 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-// addVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] + b[0...n]
-// n is the number of blocks of 16 elements to process
-TEXT ·addVec(SB), NOSPLIT, $0-32
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z3
-	MOVQ         res+0(FP), CX
-	MOVQ         a+8(FP), R15
-	MOVQ         b+16(FP), DX
-	MOVQ         n+24(FP), BX
-
-loop_1:
-	TESTQ     BX, BX
-	JEQ       done_2
-	DECQ      BX
-	VMOVDQU32 0(R15), Z0
-	VMOVDQU32 0(DX), Z1
-	VPADDD    Z0, Z1, Z0 // a = a + b
-	VPSUBD    Z3, Z0, Z2 // t = a - q
-	VPMINUD   Z0, Z2, Z1 // b = min(t, a)
-	VMOVDQU32 Z1, 0(CX)  // res = b
-
-	// increment pointers to visit next element
-	ADDQ $64, R15
-	ADDQ $64, DX
-	ADDQ $64, CX
-	JMP  loop_1
-
-done_2:
-	RET
-
-// subVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] - b[0...n]
-// n is the number of blocks of 16 elements to process
-TEXT ·subVec(SB), NOSPLIT, $0-32
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z3
-	MOVQ         res+0(FP), CX
-	MOVQ         a+8(FP), R15
-	MOVQ         b+16(FP), DX
-	MOVQ         n+24(FP), BX
-
-loop_3:
-	TESTQ     BX, BX
-	JEQ       done_4
-	DECQ      BX
-	VMOVDQU32 0(R15), Z0
-	VMOVDQU32 0(DX), Z1
-	VPSUBD    Z1, Z0, Z0 // a = a - b
-	VPADDD    Z3, Z0, Z2 // t = a + q
-	VPMINUD   Z0, Z2, Z1 // b = min(t, a)
-	VMOVDQU32 Z1, 0(CX)  // res = b
-
-	// increment pointers to visit next element
-	ADDQ $64, R15
-	ADDQ $64, DX
-	ADDQ $64, CX
-	JMP  loop_3
-
-done_4:
-	RET
-
-// sumVec(res *uint64, a *[]uint32, n uint64) res = sum(a[0...n])
-// n is the number of blocks of 16 elements to process
-TEXT ·sumVec(SB), NOSPLIT, $0-24
-
-	// We load 8 31bits values at a time and accumulate them into an accumulator of
-	// 8 quadwords (64bits). The caller then needs to reduce the result mod q.
-	// We can safely accumulate ~2**33 31bits values into a single accumulator.
-	// That gives us a maximum of 2**33 * 8 = 2**36 31bits values to sum safely.
-
-	MOVQ      t+0(FP), R15
-	MOVQ      a+8(FP), R14
-	MOVQ      n+16(FP), CX
-	VXORPS    Z2, Z2, Z2   // acc1 = 0
-	VMOVDQA64 Z2, Z3       // acc2 = 0
-
-loop_5:
-	TESTQ     CX, CX
-	JEQ       done_6
-	DECQ      CX
-	VPMOVZXDQ 0(R14), Z0  // load 8 31bits values in a1
-	VPMOVZXDQ 32(R14), Z1 // load 8 31bits values in a2
-	VPADDQ    Z0, Z2, Z2  // acc1 += a1
-	VPADDQ    Z1, Z3, Z3  // acc2 += a2
-
-	// increment pointers to visit next element
-	ADDQ $64, R14
-	JMP  loop_5
-
-done_6:
-	VPADDQ    Z2, Z3, Z2 // acc1 += acc2
-	VMOVDQU64 Z2, 0(R15) // res = acc1
-	RET
-
-TEXT ·sumVec16_AVX512(SB), NOSPLIT, $0-16
-	MOVQ          t+0(FP), R15
-	MOVQ          a+8(FP), R14
-	VPMOVZXDQ     0(R14), Z0
-	VPMOVZXDQ     32(R14), Z1
-	VPADDQ        Z0, Z1, Z0
-	VEXTRACTI64X4 $1, Z0, Y1
-	VPADDQ        Y0, Y1, Y0
-	VEXTRACTI64X2 $1, Y0, X1
-	VPADDQ        X0, X1, X0
-	PEXTRQ        $0, X0, AX
-	PEXTRQ        $1, X0, DX
-	ADDQ          DX, AX
-	MOVQ          AX, 0(R15)
-	RET
-
-TEXT ·sumVec24_AVX512(SB), NOSPLIT, $0-16
-	MOVQ          t+0(FP), R15
-	MOVQ          a+8(FP), R14
-	VPMOVZXDQ     0(R14), Z0
-	VPMOVZXDQ     32(R14), Z1
-	VPMOVZXDQ     64(R14), Z2
-	VPADDQ        Z2, Z1, Z1
-	VPADDQ        Z0, Z1, Z0
-	VEXTRACTI64X4 $1, Z0, Y1
-	VPADDQ        Y0, Y1, Y0
-	VEXTRACTI64X2 $1, Y0, X1
-	VPADDQ        X0, X1, X0
-	PEXTRQ        $0, X0, AX
-	PEXTRQ        $1, X0, DX
-	ADDQ          DX, AX
-	MOVQ          AX, 0(R15)
-	RET
-
-// mulVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] * b[0...n]
-// n is the number of blocks of 16 elements to process
-TEXT ·mulVec(SB), NOSPLIT, $0-32
-	// code inspired by Plonky3: https://github.com/Plonky3/Plonky3/blob/36e619f3c6526ee86e2e5639a24b3224e1c1700f/monty-31/src/x86_64_avx512/packing.rs#L319
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z0
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z1
-	MOVQ         res+0(FP), CX
-	MOVQ         a+8(FP), R15
-	MOVQ         b+16(FP), DX
-	MOVQ         n+24(FP), BX
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-
-loop_7:
-	TESTQ     BX, BX
-	JEQ       done_8
-	DECQ      BX
-	VMOVDQU32 0(R15), Z2
-	VMOVDQU32 0(DX), Z3
-	VPSRLQ    $32, Z2, Z5
-	VPSRLQ    $32, Z3, Z6
-	VPMULUDQ  Z2, Z3, Z7
-	VPMULUDQ  Z5, Z6, Z4
-	VPMULUDQ  Z7, Z1, Z8
-	VPMULUDQ  Z4, Z1, Z9
-	VPMULUDQ  Z8, Z0, Z8
-	VPMULUDQ  Z9, Z0, Z9
-	VPADDQ    Z7, Z8, Z7
-	VPADDQ    Z4, Z9, Z4
-	VMOVSHDUP Z7, K3, Z4
-	VPSUBD    Z0, Z4, Z9
-	VPMINUD   Z4, Z9, Z4
-	VMOVDQU32 Z4, 0(CX)   // res = P
-
-	// increment pointers to visit next element
-	ADDQ $64, R15
-	ADDQ $64, DX
-	ADDQ $64, CX
-	JMP  loop_7
-
-done_8:
-	RET
-
-// scalarMulVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] * b
-// n is the number of blocks of 16 elements to process
-TEXT ·scalarMulVec(SB), NOSPLIT, $0-32
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z7
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z8
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         res+0(FP), CX
-	MOVQ         a+8(FP), R15
-	MOVQ         b+16(FP), DX
-	MOVQ         n+24(FP), BX
-	VPBROADCASTD 0(DX), Z1
-
-loop_9:
-	TESTQ     BX, BX
-	JEQ       done_10
-	DECQ      BX
-	VMOVDQU32 0(R15), Z0
-	VPSRLQ    $32, Z0, Z3
-	VPMULUDQ  Z0, Z1, Z4
-	VPMULUDQ  Z3, Z1, Z2
-	VPMULUDQ  Z4, Z8, Z5
-	VPMULUDQ  Z2, Z8, Z6
-	VPMULUDQ  Z5, Z7, Z5
-	VPMULUDQ  Z6, Z7, Z6
-	VPADDQ    Z4, Z5, Z4
-	VPADDQ    Z2, Z6, Z2
-	VMOVSHDUP Z4, K3, Z2
-	VPSUBD    Z7, Z2, Z6
-	VPMINUD   Z2, Z6, Z2
-	VMOVDQU32 Z2, 0(CX)   // res = P
-
-	// increment pointers to visit next element
-	ADDQ $64, R15
-	ADDQ $64, CX
-	JMP  loop_9
-
-done_10:
-	RET
-
-// innerProdVec(t *uint64, a,b *[]uint32, n uint64) res = sum(a[0...n] * b[0...n])
-// n is the number of blocks of 16 elements to process
-TEXT ·innerProdVec(SB), NOSPLIT, $0-32
-
-	// Similar to mulVec; we do most of the montgomery multiplication but don't do
-	// the final reduction. We accumulate the result like in sumVec and let the caller
-	// reduce mod q.
-
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z0
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z1
-	MOVQ         t+0(FP), CX
-	MOVQ         a+8(FP), R14
-	MOVQ         b+16(FP), R15
-	MOVQ         n+24(FP), BX
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	VXORPS       Z10, Z10, Z10           // acc0 = 0
-	VMOVDQA64    Z10, Z11                // acc1 = 0
-
-loop_11:
-	TESTQ     BX, BX
-	JEQ       done_12
-	DECQ      BX
-	VMOVDQU32 0(R14), Z2
-	VMOVDQU32 0(R15), Z3
-	VPSRLQ    $32, Z2, Z5
-	VPSRLQ    $32, Z3, Z6
-	VPMULUDQ  Z2, Z3, Z7
-	VPMULUDQ  Z5, Z6, Z4
-	VPMULUDQ  Z7, Z1, Z8
-	VPMULUDQ  Z4, Z1, Z9
-	VPMULUDQ  Z8, Z0, Z8
-	VPMULUDQ  Z9, Z0, Z9
-	VPADDQ    Z7, Z8, Z7
-	VPSRLQ    $32, Z7, Z7
-	VPADDQ    Z7, Z10, Z10
-	VPADDQ    Z4, Z9, Z4
-	VPSRLQ    $32, Z4, Z4
-	VPADDQ    Z4, Z11, Z11
-
-	// increment pointers to visit next element
-	ADDQ $64, R14
-	ADDQ $64, R15
-	JMP  loop_11
-
-done_12:
-	VPADDQ    Z11, Z10, Z11 // acc1 += acc0
-	VMOVDQU64 Z11, 0(CX)    // res = acc1
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_31b/element_31b_arm64.s b/field/asm/element_31b/element_31b_arm64.s
--- a/field/asm/element_31b/element_31b_arm64.s
+++ b/field/asm/element_31b/element_31b_arm64.s
@@ -1,90 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-// addVec(res, a, b *Element, n uint64)
-// n is the number of blocks of 4 uint32 to process
-TEXT ·addVec(SB), NOFRAME|NOSPLIT, $0-32
-	LDP   res+0(FP), (R0, R1)
-	LDP   b+16(FP), (R2, R3)
-	VMOVS $const_q, V3
-	VDUP  V3.S[0], V3.S4      // broadcast q into V3
-
-loop1:
-	CBZ    R3, done2
-	VLD1.P 16(R1), [V0.S4]
-	VLD1.P 16(R2), [V1.S4]
-	VADD   V0.S4, V1.S4, V1.S4 // b = a + b
-	VSUB   V3.S4, V1.S4, V2.S4 // t = b - q
-	VUMIN  V2.S4, V1.S4, V1.S4 // b = min(t, b)
-	VST1.P [V1.S4], 16(R0)     // res = b
-	SUB    $1, R3, R3
-	JMP    loop1
-
-done2:
-	RET
-
-// subVec(res, a, b *Element, n uint64)
-// n is the number of blocks of 4 uint32 to process
-TEXT ·subVec(SB), NOFRAME|NOSPLIT, $0-32
-	LDP   res+0(FP), (R0, R1)
-	LDP   b+16(FP), (R2, R3)
-	VMOVS $const_q, V3
-	VDUP  V3.S[0], V3.S4      // broadcast q into V3
-
-loop3:
-	CBZ    R3, done4
-	VLD1.P 16(R1), [V0.S4]
-	VLD1.P 16(R2), [V1.S4]
-	VSUB   V1.S4, V0.S4, V1.S4 // b = a - b
-	VADD   V1.S4, V3.S4, V2.S4 // t = b + q
-	VUMIN  V2.S4, V1.S4, V1.S4 // b = min(t, b)
-	VST1.P [V1.S4], 16(R0)     // res = b
-	SUB    $1, R3, R3
-	JMP    loop3
-
-done4:
-	RET
-
-// sumVec(t *uint64, a *[]uint32, n uint64) res = sum(a[0...n])
-// n is the number of blocks of 16 uint32 to process
-TEXT ·sumVec(SB), NOFRAME|NOSPLIT, $0-24
-	// zeroing accumulators
-	VMOVQ $0, $0, V4
-	VMOVQ $0, $0, V5
-	VMOVQ $0, $0, V6
-	VMOVQ $0, $0, V7
-	LDP   t+0(FP), (R1, R0)
-	MOVD  n+16(FP), R2
-
-loop5:
-	CBZ R2, done6
-
-	// blockSize is 16 uint32; we load 4 vectors of 4 uint32 at a time
-	// (4*4)*4 = 64 bytes ~= 1 cache line
-	// since our values are 31 bits, we can add 2 by 2 these vectors
-	// we are left with 2 vectors of 4x32 bits values
-	// that we accumulate in 4*2*64bits accumulators
-	// the caller will reduce mod q the accumulators.
-
-	VLD2.P  32(R0), [V0.S4, V1.S4]
-	VADD    V0.S4, V1.S4, V0.S4    // a1 += a2
-	VLD2.P  32(R0), [V2.S4, V3.S4]
-	VADD    V2.S4, V3.S4, V2.S4    // a3 += a4
-	VUSHLL  $0, V0.S2, V1.D2       // convert low words to 64 bits
-	VADD    V1.D2, V5.D2, V5.D2    // acc2 += a2
-	VUSHLL2 $0, V0.S4, V0.D2       // convert high words to 64 bits
-	VADD    V0.D2, V4.D2, V4.D2    // acc1 += a1
-	VUSHLL  $0, V2.S2, V3.D2       // convert low words to 64 bits
-	VADD    V3.D2, V7.D2, V7.D2    // acc4 += a4
-	VUSHLL2 $0, V2.S4, V2.D2       // convert high words to 64 bits
-	VADD    V2.D2, V6.D2, V6.D2    // acc3 += a3
-	SUB     $1, R2, R2
-	JMP     loop5
-
-done6:
-	VADD   V4.D2, V6.D2, V4.D2   // acc1 += acc3
-	VADD   V5.D2, V7.D2, V5.D2   // acc2 += acc4
-	VST2.P [V4.D2, V5.D2], 0(R1) // store acc1 and acc2
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_4w/element_4w_amd64.s b/field/asm/element_4w/element_4w_amd64.s
--- a/field/asm/element_4w/element_4w_amd64.s
+++ b/field/asm/element_4w/element_4w_amd64.s
@@ -1,2438 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define REDUCE(ra0, ra1, ra2, ra3, rb0, rb1, rb2, rb3) \
-	MOVQ    ra0, rb0;              \
-	SUBQ    ·qElement(SB), ra0;    \
-	MOVQ    ra1, rb1;              \
-	SBBQ    ·qElement+8(SB), ra1;  \
-	MOVQ    ra2, rb2;              \
-	SBBQ    ·qElement+16(SB), ra2; \
-	MOVQ    ra3, rb3;              \
-	SBBQ    ·qElement+24(SB), ra3; \
-	CMOVQCS rb0, ra0;              \
-	CMOVQCS rb1, ra1;              \
-	CMOVQCS rb2, ra2;              \
-	CMOVQCS rb3, ra3;              \
-
-TEXT ·reduce(SB), NOSPLIT, $0-8
-	MOVQ res+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	RET
-
-// MulBy3(x *Element)
-TEXT ·MulBy3(SB), NOSPLIT, $0-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	RET
-
-// MulBy5(x *Element)
-TEXT ·MulBy5(SB), NOSPLIT, $0-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (R15,DI,R8,R9)
-	REDUCE(DX,CX,BX,SI,R15,DI,R8,R9)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	RET
-
-// MulBy13(x *Element)
-TEXT ·MulBy13(SB), NOSPLIT, $0-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,R11,R12,R13,R14)
-
-	MOVQ DX, R11
-	MOVQ CX, R12
-	MOVQ BX, R13
-	MOVQ SI, R14
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
-
-	ADDQ R11, DX
-	ADCQ R12, CX
-	ADCQ R13, BX
-	ADCQ R14, SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-
-	// reduce element(DX,CX,BX,SI) using temp registers (DI,R8,R9,R10)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	RET
-
-// Butterfly(a, b *Element) sets a = a + b; b = a - b
-TEXT ·Butterfly(SB), NOSPLIT, $0-16
-	MOVQ    a+0(FP), AX
-	MOVQ    0(AX), CX
-	MOVQ    8(AX), BX
-	MOVQ    16(AX), SI
-	MOVQ    24(AX), DI
-	MOVQ    CX, R8
-	MOVQ    BX, R9
-	MOVQ    SI, R10
-	MOVQ    DI, R11
-	XORQ    AX, AX
-	MOVQ    b+8(FP), DX
-	ADDQ    0(DX), CX
-	ADCQ    8(DX), BX
-	ADCQ    16(DX), SI
-	ADCQ    24(DX), DI
-	SUBQ    0(DX), R8
-	SBBQ    8(DX), R9
-	SBBQ    16(DX), R10
-	SBBQ    24(DX), R11
-	MOVQ    $const_q0, R12
-	MOVQ    $const_q1, R13
-	MOVQ    $const_q2, R14
-	MOVQ    $const_q3, R15
-	CMOVQCC AX, R12
-	CMOVQCC AX, R13
-	CMOVQCC AX, R14
-	CMOVQCC AX, R15
-	ADDQ    R12, R8
-	ADCQ    R13, R9
-	ADCQ    R14, R10
-	ADCQ    R15, R11
-	MOVQ    R8, 0(DX)
-	MOVQ    R9, 8(DX)
-	MOVQ    R10, 16(DX)
-	MOVQ    R11, 24(DX)
-
-	// reduce element(CX,BX,SI,DI) using temp registers (R8,R9,R10,R11)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11)
-
-	MOVQ a+0(FP), AX
-	MOVQ CX, 0(AX)
-	MOVQ BX, 8(AX)
-	MOVQ SI, 16(AX)
-	MOVQ DI, 24(AX)
-	RET
-
-// mul(res, x, y *Element)
-TEXT ·mul(SB), $24-24
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
-
-	NO_LOCAL_POINTERS
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_1
-	MOVQ x+8(FP), SI
-
-	// x[0] -> DI
-	// x[1] -> R8
-	// x[2] -> R9
-	// x[3] -> R10
-	MOVQ 0(SI), DI
-	MOVQ 8(SI), R8
-	MOVQ 16(SI), R9
-	MOVQ 24(SI), R10
-	MOVQ y+16(FP), R11
-
-	// A -> BP
-	// t[0] -> R14
-	// t[1] -> R13
-	// t[2] -> CX
-	// t[3] -> BX
-#define MACC(in0, in1, in2) \
-	ADCXQ in0, in1     \
-	MULXQ in2, AX, in0 \
-	ADOXQ AX, in1      \
-
-#define DIV_SHIFT() \
-	MOVQ  $const_qInvNeg, DX        \
-	IMULQ R14, DX                   \
-	XORQ  AX, AX                    \
-	MULXQ ·qElement+0(SB), AX, R12  \
-	ADCXQ R14, AX                   \
-	MOVQ  R12, R14                  \
-	MACC(R13, R14, ·qElement+8(SB)) \
-	MACC(CX, R13, ·qElement+16(SB)) \
-	MACC(BX, CX, ·qElement+24(SB))  \
-	MOVQ  $0, AX                    \
-	ADCXQ AX, BX                    \
-	ADOXQ BP, BX                    \
-
-#define MUL_WORD_0() \
-	XORQ  AX, AX       \
-	MULXQ DI, R14, R13 \
-	MULXQ R8, AX, CX   \
-	ADOXQ AX, R13      \
-	MULXQ R9, AX, BX   \
-	ADOXQ AX, CX       \
-	MULXQ R10, AX, BP  \
-	ADOXQ AX, BX       \
-	MOVQ  $0, AX       \
-	ADOXQ AX, BP       \
-	DIV_SHIFT()        \
-
-#define MUL_WORD_N() \
-	XORQ  AX, AX      \
-	MULXQ DI, AX, BP  \
-	ADOXQ AX, R14     \
-	MACC(BP, R13, R8) \
-	MACC(BP, CX, R9)  \
-	MACC(BP, BX, R10) \
-	MOVQ  $0, AX      \
-	ADCXQ AX, BP      \
-	ADOXQ AX, BP      \
-	DIV_SHIFT()       \
-
-	// mul body
-	MOVQ 0(R11), DX
-	MUL_WORD_0()
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (SI,R12,R11,DI)
-	REDUCE(R14,R13,CX,BX,SI,R12,R11,DI)
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R13, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	RET
-
-noAdx_1:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	MOVQ y+16(FP), AX
-	MOVQ AX, 16(SP)
-	CALL ·_mulGeneric(SB)
-	RET
-
-TEXT ·fromMont(SB), $8-8
-	NO_LOCAL_POINTERS
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// when y = 1 we have:
-	// for i=0 to N-1
-	// 		t[i] = x[i]
-	// for i=0 to N-1
-	// 		m := t[0]*q'[0] mod W
-	// 		C,_ := t[0] + m*q[0]
-	// 		for j=1 to N-1
-	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
-	// 		t[N-1] = C
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_2
-	MOVQ res+0(FP), DX
-	MOVQ 0(DX), R14
-	MOVQ 8(DX), R13
-	MOVQ 16(DX), CX
-	MOVQ 24(DX), BX
-	XORQ DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-	MOVQ  $0, AX
-	ADCXQ AX, BX
-	ADOXQ AX, BX
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-	MOVQ  $0, AX
-	ADCXQ AX, BX
-	ADOXQ AX, BX
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-	MOVQ  $0, AX
-	ADCXQ AX, BX
-	ADOXQ AX, BX
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-	MOVQ  $0, AX
-	ADCXQ AX, BX
-	ADOXQ AX, BX
-
-	// reduce element(R14,R13,CX,BX) using temp registers (SI,DI,R8,R9)
-	REDUCE(R14,R13,CX,BX,SI,DI,R8,R9)
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R13, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	RET
-
-noAdx_2:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	CALL ·_fromMontGeneric(SB)
-	RET
-
-// Vector operations are partially derived from Dag Arne Osvik's work in github.com/a16z/vectorized-fields
-
-// addVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] + b[0...n]
-TEXT ·addVec(SB), NOSPLIT, $0-32
-	MOVQ res+0(FP), CX
-	MOVQ a+8(FP), AX
-	MOVQ b+16(FP), DX
-	MOVQ n+24(FP), BX
-
-loop_3:
-	TESTQ BX, BX
-	JEQ   done_4 // n == 0, we are done
-
-	// a[0] -> SI
-	// a[1] -> DI
-	// a[2] -> R8
-	// a[3] -> R9
-	MOVQ       0(AX), SI
-	MOVQ       8(AX), DI
-	MOVQ       16(AX), R8
-	MOVQ       24(AX), R9
-	ADDQ       0(DX), SI
-	ADCQ       8(DX), DI
-	ADCQ       16(DX), R8
-	ADCQ       24(DX), R9
-	PREFETCHT0 2048(AX)
-	PREFETCHT0 2048(DX)
-
-	// reduce element(SI,DI,R8,R9) using temp registers (R10,R11,R12,R13)
-	REDUCE(SI,DI,R8,R9,R10,R11,R12,R13)
-
-	MOVQ SI, 0(CX)
-	MOVQ DI, 8(CX)
-	MOVQ R8, 16(CX)
-	MOVQ R9, 24(CX)
-
-	// increment pointers to visit next element
-	ADDQ $32, AX
-	ADDQ $32, DX
-	ADDQ $32, CX
-	DECQ BX      // decrement n
-	JMP  loop_3
-
-done_4:
-	RET
-
-// subVec(res, a, b *Element, n uint64) res[0...n] = a[0...n] - b[0...n]
-TEXT ·subVec(SB), NOSPLIT, $0-32
-	MOVQ res+0(FP), CX
-	MOVQ a+8(FP), AX
-	MOVQ b+16(FP), DX
-	MOVQ n+24(FP), BX
-	XORQ SI, SI
-
-loop_5:
-	TESTQ BX, BX
-	JEQ   done_6 // n == 0, we are done
-
-	// a[0] -> DI
-	// a[1] -> R8
-	// a[2] -> R9
-	// a[3] -> R10
-	MOVQ       0(AX), DI
-	MOVQ       8(AX), R8
-	MOVQ       16(AX), R9
-	MOVQ       24(AX), R10
-	SUBQ       0(DX), DI
-	SBBQ       8(DX), R8
-	SBBQ       16(DX), R9
-	SBBQ       24(DX), R10
-	PREFETCHT0 2048(AX)
-	PREFETCHT0 2048(DX)
-
-	// reduce (a-b) mod q
-	// q[0] -> R11
-	// q[1] -> R12
-	// q[2] -> R13
-	// q[3] -> R14
-	MOVQ    $const_q0, R11
-	MOVQ    $const_q1, R12
-	MOVQ    $const_q2, R13
-	MOVQ    $const_q3, R14
-	CMOVQCC SI, R11
-	CMOVQCC SI, R12
-	CMOVQCC SI, R13
-	CMOVQCC SI, R14
-
-	// add registers (q or 0) to a, and set to result
-	ADDQ R11, DI
-	ADCQ R12, R8
-	ADCQ R13, R9
-	ADCQ R14, R10
-	MOVQ DI, 0(CX)
-	MOVQ R8, 8(CX)
-	MOVQ R9, 16(CX)
-	MOVQ R10, 24(CX)
-
-	// increment pointers to visit next element
-	ADDQ $32, AX
-	ADDQ $32, DX
-	ADDQ $32, CX
-	DECQ BX      // decrement n
-	JMP  loop_5
-
-done_6:
-	RET
-
-// sumVec(res, a *Element, n uint64) res = sum(a[0...n])
-TEXT ·sumVec(SB), NOSPLIT, $0-24
-
-	// Derived from https://github.com/a16z/vectorized-fields
-	// The idea is to use Z registers to accumulate the sum of elements, 8 by 8
-	// first, we handle the case where n % 8 != 0
-	// then, we loop over the elements 8 by 8 and accumulate the sum in the Z registers
-	// finally, we reduce the sum and store it in res
-	//
-	// when we move an element of a into a Z register, we use VPMOVZXDQ
-	// let's note w0...w3 the 4 64bits words of ai: w0 = ai[0], w1 = ai[1], w2 = ai[2], w3 = ai[3]
-	// VPMOVZXDQ(ai, Z0) will result in
-	// Z0= [hi(w3), lo(w3), hi(w2), lo(w2), hi(w1), lo(w1), hi(w0), lo(w0)]
-	// with hi(wi) the high 32 bits of wi and lo(wi) the low 32 bits of wi
-	// we can safely add 2^32+1 times Z registers constructed this way without overflow
-	// since each of this lo/hi bits are moved into a "64bits" slot
-	// N = 2^64-1 / 2^32-1 = 2^32+1
-	//
-	// we then propagate the carry using ADOXQ and ADCXQ
-	// r0 = w0l + lo(woh)
-	// r1 = carry + hi(woh) + w1l + lo(w1h)
-	// r2 = carry + hi(w1h) + w2l + lo(w2h)
-	// r3 = carry + hi(w2h) + w3l + lo(w3h)
-	// r4 = carry + hi(w3h)
-	// we then reduce the sum using a single-word Barrett reduction
-	// we pick mu = 2^288 / q; which correspond to 4.5 words max.
-	// meaning we must guarantee that r4 fits in 32bits.
-	// To do so, we reduce N to 2^32-1 (since r4 receives 2 carries max)
-
-	MOVQ a+8(FP), R14
-	MOVQ n+16(FP), R15
-
-	// initialize accumulators Z0, Z1, Z2, Z3, Z4, Z5, Z6, Z7
-	VXORPS    Z0, Z0, Z0
-	VMOVDQA64 Z0, Z1
-	VMOVDQA64 Z0, Z2
-	VMOVDQA64 Z0, Z3
-	VMOVDQA64 Z0, Z4
-	VMOVDQA64 Z0, Z5
-	VMOVDQA64 Z0, Z6
-	VMOVDQA64 Z0, Z7
-
-	// n % 8 -> CX
-	// n / 8 -> R15
-	MOVQ R15, CX
-	ANDQ $7, CX
-	SHRQ $3, R15
-
-loop_single_9:
-	TESTQ     CX, CX
-	JEQ       loop8by8_7    // n % 8 == 0, we are going to loop over 8 by 8
-	VPMOVZXDQ 0(R14), Z8
-	VPADDQ    Z8, Z0, Z0
-	ADDQ      $32, R14
-	DECQ      CX            // decrement nMod8
-	JMP       loop_single_9
-
-loop8by8_7:
-	TESTQ      R15, R15
-	JEQ        accumulate_10  // n == 0, we are going to accumulate
-	VPMOVZXDQ  0*32(R14), Z8
-	VPMOVZXDQ  1*32(R14), Z9
-	VPMOVZXDQ  2*32(R14), Z10
-	VPMOVZXDQ  3*32(R14), Z11
-	VPMOVZXDQ  4*32(R14), Z12
-	VPMOVZXDQ  5*32(R14), Z13
-	VPMOVZXDQ  6*32(R14), Z14
-	VPMOVZXDQ  7*32(R14), Z15
-	PREFETCHT0 4096(R14)
-	VPADDQ     Z8, Z0, Z0
-	VPADDQ     Z9, Z1, Z1
-	VPADDQ     Z10, Z2, Z2
-	VPADDQ     Z11, Z3, Z3
-	VPADDQ     Z12, Z4, Z4
-	VPADDQ     Z13, Z5, Z5
-	VPADDQ     Z14, Z6, Z6
-	VPADDQ     Z15, Z7, Z7
-
-	// increment pointers to visit next 8 elements
-	ADDQ $256, R14
-	DECQ R15        // decrement n
-	JMP  loop8by8_7
-
-accumulate_10:
-	// accumulate the 8 Z registers into Z0
-	VPADDQ Z7, Z6, Z6
-	VPADDQ Z6, Z5, Z5
-	VPADDQ Z5, Z4, Z4
-	VPADDQ Z4, Z3, Z3
-	VPADDQ Z3, Z2, Z2
-	VPADDQ Z2, Z1, Z1
-	VPADDQ Z1, Z0, Z0
-
-	// carry propagation
-	// lo(w0) -> BX
-	// hi(w0) -> SI
-	// lo(w1) -> DI
-	// hi(w1) -> R8
-	// lo(w2) -> R9
-	// hi(w2) -> R10
-	// lo(w3) -> R11
-	// hi(w3) -> R12
-	VMOVQ   X0, BX
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, SI
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, DI
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, R8
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, R9
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, R10
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, R11
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, R12
-
-	// lo(hi(wo)) -> R13
-	// lo(hi(w1)) -> CX
-	// lo(hi(w2)) -> R15
-	// lo(hi(w3)) -> R14
-#define SPLIT_LO_HI(in0, in1) \
-	MOVQ in1, in0         \
-	ANDQ $0xffffffff, in0 \
-	SHLQ $32, in0         \
-	SHRQ $32, in1         \
-
-	SPLIT_LO_HI(R13, SI)
-	SPLIT_LO_HI(CX, R8)
-	SPLIT_LO_HI(R15, R10)
-	SPLIT_LO_HI(R14, R12)
-
-	// r0 = w0l + lo(woh)
-	// r1 = carry + hi(woh) + w1l + lo(w1h)
-	// r2 = carry + hi(w1h) + w2l + lo(w2h)
-	// r3 = carry + hi(w2h) + w3l + lo(w3h)
-	// r4 = carry + hi(w3h)
-
-	XORQ  AX, AX   // clear the flags
-	ADOXQ R13, BX
-	ADOXQ CX, DI
-	ADCXQ SI, DI
-	ADOXQ R15, R9
-	ADCXQ R8, R9
-	ADOXQ R14, R11
-	ADCXQ R10, R11
-	ADOXQ AX, R12
-	ADCXQ AX, R12
-
-	// r[0] -> BX
-	// r[1] -> DI
-	// r[2] -> R9
-	// r[3] -> R11
-	// r[4] -> R12
-	// reduce using single-word Barrett
-	// see see Handbook of Applied Cryptography, Algorithm 14.42.
-	// mu=2^288 / q -> SI
-	MOVQ  $const_mu, SI
-	MOVQ  R11, AX
-	SHRQ  $32, R12, AX
-	MULQ  SI                       // high bits of res stored in DX
-	MULXQ ·qElement+0(SB), AX, SI
-	SUBQ  AX, BX
-	SBBQ  SI, DI
-	MULXQ ·qElement+16(SB), AX, SI
-	SBBQ  AX, R9
-	SBBQ  SI, R11
-	SBBQ  $0, R12
-	MULXQ ·qElement+8(SB), AX, SI
-	SUBQ  AX, DI
-	SBBQ  SI, R9
-	MULXQ ·qElement+24(SB), AX, SI
-	SBBQ  AX, R11
-	SBBQ  SI, R12
-	MOVQ  BX, R8
-	MOVQ  DI, R10
-	MOVQ  R9, R13
-	MOVQ  R11, CX
-	SUBQ  ·qElement+0(SB), BX
-	SBBQ  ·qElement+8(SB), DI
-	SBBQ  ·qElement+16(SB), R9
-	SBBQ  ·qElement+24(SB), R11
-	SBBQ  $0, R12
-	JCS   modReduced_11
-	MOVQ  BX, R8
-	MOVQ  DI, R10
-	MOVQ  R9, R13
-	MOVQ  R11, CX
-	SUBQ  ·qElement+0(SB), BX
-	SBBQ  ·qElement+8(SB), DI
-	SBBQ  ·qElement+16(SB), R9
-	SBBQ  ·qElement+24(SB), R11
-	SBBQ  $0, R12
-	JCS   modReduced_11
-	MOVQ  BX, R8
-	MOVQ  DI, R10
-	MOVQ  R9, R13
-	MOVQ  R11, CX
-
-modReduced_11:
-	MOVQ res+0(FP), SI
-	MOVQ R8, 0(SI)
-	MOVQ R10, 8(SI)
-	MOVQ R13, 16(SI)
-	MOVQ CX, 24(SI)
-
-done_8:
-	RET
-
-// innerProdVec(res, a,b *Element, n uint64) res = sum(a[0...n] * b[0...n])
-TEXT ·innerProdVec(SB), NOSPLIT, $0-32
-	MOVQ a+8(FP), R14
-	MOVQ b+16(FP), R15
-	MOVQ n+24(FP), CX
-
-	// Create mask for low dword in each qword
-	VPCMPEQB  Y0, Y0, Y0
-	VPMOVZXDQ Y0, Z5
-	VPXORQ    Z16, Z16, Z16
-	VMOVDQA64 Z16, Z17
-	VMOVDQA64 Z16, Z18
-	VMOVDQA64 Z16, Z19
-	VMOVDQA64 Z16, Z20
-	VMOVDQA64 Z16, Z21
-	VMOVDQA64 Z16, Z22
-	VMOVDQA64 Z16, Z23
-	VMOVDQA64 Z16, Z24
-	VMOVDQA64 Z16, Z25
-	VMOVDQA64 Z16, Z26
-	VMOVDQA64 Z16, Z27
-	VMOVDQA64 Z16, Z28
-	VMOVDQA64 Z16, Z29
-	VMOVDQA64 Z16, Z30
-	VMOVDQA64 Z16, Z31
-	TESTQ     CX, CX
-	JEQ       done_13       // n == 0, we are done
-
-loop_12:
-	TESTQ     CX, CX
-	JEQ       accumulate_14 // n == 0 we can accumulate
-	VPMOVZXDQ (R15), Z4
-	ADDQ      $32, R15
-
-	// we multiply and accumulate partial products of 4 bytes * 32 bytes
-#define MAC(in0, in1, in2) \
-	VPMULUDQ.BCST in0, Z4, Z2  \
-	VPSRLQ        $32, Z2, Z3  \
-	VPANDQ        Z5, Z2, Z2   \
-	VPADDQ        Z2, in1, in1 \
-	VPADDQ        Z3, in2, in2 \
-
-	MAC(0*4(R14), Z16, Z24)
-	MAC(1*4(R14), Z17, Z25)
-	MAC(2*4(R14), Z18, Z26)
-	MAC(3*4(R14), Z19, Z27)
-	MAC(4*4(R14), Z20, Z28)
-	MAC(5*4(R14), Z21, Z29)
-	MAC(6*4(R14), Z22, Z30)
-	MAC(7*4(R14), Z23, Z31)
-	ADDQ $32, R14
-	DECQ CX       // decrement n
-	JMP  loop_12
-
-accumulate_14:
-	// we accumulate the partial products into 544bits in Z1:Z0
-	MOVQ  $0x0000000000001555, AX
-	KMOVD AX, K1
-	MOVQ  $1, AX
-	KMOVD AX, K2
-
-	// store the least significant 32 bits of ACC (starts with A0L) in Z0
-	VALIGND.Z $16, Z16, Z16, K2, Z0
-	KSHIFTLW  $1, K2, K2
-	VPSRLQ    $32, Z16, Z2
-	VALIGND.Z $2, Z16, Z16, K1, Z16
-	VPADDQ    Z2, Z16, Z16
-	VPANDQ    Z5, Z24, Z2
-	VPADDQ    Z2, Z16, Z16
-	VPANDQ    Z5, Z17, Z2
-	VPADDQ    Z2, Z16, Z16
-	VALIGND   $15, Z16, Z16, K2, Z0
-	KSHIFTLW  $1, K2, K2
-
-	// macro to add partial products and store the result in Z0
-#define ADDPP(in0, in1, in2, in3, in4) \
-	VPSRLQ    $32, Z16, Z2              \
-	VALIGND.Z $2, Z16, Z16, K1, Z16     \
-	VPADDQ    Z2, Z16, Z16              \
-	VPSRLQ    $32, in0, in0             \
-	VPADDQ    in0, Z16, Z16             \
-	VPSRLQ    $32, in1, in1             \
-	VPADDQ    in1, Z16, Z16             \
-	VPANDQ    Z5, in2, Z2               \
-	VPADDQ    Z2, Z16, Z16              \
-	VPANDQ    Z5, in3, Z2               \
-	VPADDQ    Z2, Z16, Z16              \
-	VALIGND   $16-in4, Z16, Z16, K2, Z0 \
-	KADDW     K2, K2, K2                \
-
-	ADDPP(Z24, Z17, Z25, Z18, 2)
-	ADDPP(Z25, Z18, Z26, Z19, 3)
-	ADDPP(Z26, Z19, Z27, Z20, 4)
-	ADDPP(Z27, Z20, Z28, Z21, 5)
-	ADDPP(Z28, Z21, Z29, Z22, 6)
-	ADDPP(Z29, Z22, Z30, Z23, 7)
-	VPSRLQ    $32, Z16, Z2
-	VALIGND.Z $2, Z16, Z16, K1, Z16
-	VPADDQ    Z2, Z16, Z16
-	VPSRLQ    $32, Z30, Z30
-	VPADDQ    Z30, Z16, Z16
-	VPSRLQ    $32, Z23, Z23
-	VPADDQ    Z23, Z16, Z16
-	VPANDQ    Z5, Z31, Z2
-	VPADDQ    Z2, Z16, Z16
-	VALIGND   $16-8, Z16, Z16, K2, Z0
-	KSHIFTLW  $1, K2, K2
-	VPSRLQ    $32, Z16, Z2
-	VALIGND.Z $2, Z16, Z16, K1, Z16
-	VPADDQ    Z2, Z16, Z16
-	VPSRLQ    $32, Z31, Z31
-	VPADDQ    Z31, Z16, Z16
-	VALIGND   $16-9, Z16, Z16, K2, Z0
-	KSHIFTLW  $1, K2, K2
-
-#define ADDPP2(in0) \
-	VPSRLQ    $32, Z16, Z2              \
-	VALIGND.Z $2, Z16, Z16, K1, Z16     \
-	VPADDQ    Z2, Z16, Z16              \
-	VALIGND   $16-in0, Z16, Z16, K2, Z0 \
-	KSHIFTLW  $1, K2, K2                \
-
-	ADDPP2(10)
-	ADDPP2(11)
-	ADDPP2(12)
-	ADDPP2(13)
-	ADDPP2(14)
-	ADDPP2(15)
-	VPSRLQ      $32, Z16, Z2
-	VALIGND.Z   $2, Z16, Z16, K1, Z16
-	VPADDQ      Z2, Z16, Z16
-	VMOVDQA64.Z Z16, K1, Z1
-
-	// Extract the 4 least significant qwords of Z0
-	VMOVQ   X0, SI
-	VALIGNQ $1, Z0, Z1, Z0
-	VMOVQ   X0, DI
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, R8
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, R9
-	VALIGNQ $1, Z0, Z0, Z0
-	XORQ    BX, BX
-	MOVQ    $const_qInvNeg, DX
-	MULXQ   SI, DX, R10
-	MULXQ   ·qElement+0(SB), AX, R10
-	ADDQ    AX, SI
-	ADCQ    R10, DI
-	MULXQ   ·qElement+16(SB), AX, R10
-	ADCQ    AX, R8
-	ADCQ    R10, R9
-	ADCQ    $0, BX
-	MULXQ   ·qElement+8(SB), AX, R10
-	ADDQ    AX, DI
-	ADCQ    R10, R8
-	MULXQ   ·qElement+24(SB), AX, R10
-	ADCQ    AX, R9
-	ADCQ    R10, BX
-	ADCQ    $0, SI
-	MOVQ    $const_qInvNeg, DX
-	MULXQ   DI, DX, R10
-	MULXQ   ·qElement+0(SB), AX, R10
-	ADDQ    AX, DI
-	ADCQ    R10, R8
-	MULXQ   ·qElement+16(SB), AX, R10
-	ADCQ    AX, R9
-	ADCQ    R10, BX
-	ADCQ    $0, SI
-	MULXQ   ·qElement+8(SB), AX, R10
-	ADDQ    AX, R8
-	ADCQ    R10, R9
-	MULXQ   ·qElement+24(SB), AX, R10
-	ADCQ    AX, BX
-	ADCQ    R10, SI
-	ADCQ    $0, DI
-	MOVQ    $const_qInvNeg, DX
-	MULXQ   R8, DX, R10
-	MULXQ   ·qElement+0(SB), AX, R10
-	ADDQ    AX, R8
-	ADCQ    R10, R9
-	MULXQ   ·qElement+16(SB), AX, R10
-	ADCQ    AX, BX
-	ADCQ    R10, SI
-	ADCQ    $0, DI
-	MULXQ   ·qElement+8(SB), AX, R10
-	ADDQ    AX, R9
-	ADCQ    R10, BX
-	MULXQ   ·qElement+24(SB), AX, R10
-	ADCQ    AX, SI
-	ADCQ    R10, DI
-	ADCQ    $0, R8
-	MOVQ    $const_qInvNeg, DX
-	MULXQ   R9, DX, R10
-	MULXQ   ·qElement+0(SB), AX, R10
-	ADDQ    AX, R9
-	ADCQ    R10, BX
-	MULXQ   ·qElement+16(SB), AX, R10
-	ADCQ    AX, SI
-	ADCQ    R10, DI
-	ADCQ    $0, R8
-	MULXQ   ·qElement+8(SB), AX, R10
-	ADDQ    AX, BX
-	ADCQ    R10, SI
-	MULXQ   ·qElement+24(SB), AX, R10
-	ADCQ    AX, DI
-	ADCQ    R10, R8
-	ADCQ    $0, R9
-	VMOVQ   X0, AX
-	ADDQ    AX, BX
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, AX
-	ADCQ    AX, SI
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, AX
-	ADCQ    AX, DI
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, AX
-	ADCQ    AX, R8
-	VALIGNQ $1, Z0, Z0, Z0
-	VMOVQ   X0, AX
-	ADCQ    AX, R9
-
-	// Barrett reduction; see Handbook of Applied Cryptography, Algorithm 14.42.
-	MOVQ  R8, AX
-	SHRQ  $32, R9, AX
-	MOVQ  $const_mu, DX
-	MULQ  DX
-	MULXQ ·qElement+0(SB), AX, R10
-	SUBQ  AX, BX
-	SBBQ  R10, SI
-	MULXQ ·qElement+16(SB), AX, R10
-	SBBQ  AX, DI
-	SBBQ  R10, R8
-	SBBQ  $0, R9
-	MULXQ ·qElement+8(SB), AX, R10
-	SUBQ  AX, SI
-	SBBQ  R10, DI
-	MULXQ ·qElement+24(SB), AX, R10
-	SBBQ  AX, R8
-	SBBQ  R10, R9
-
-	// we need up to 2 conditional substractions to be < q
-	MOVQ res+0(FP), R11
-	MOVQ BX, 0(R11)
-	MOVQ SI, 8(R11)
-	MOVQ DI, 16(R11)
-	MOVQ R8, 24(R11)
-	SUBQ ·qElement+0(SB), BX
-	SBBQ ·qElement+8(SB), SI
-	SBBQ ·qElement+16(SB), DI
-	SBBQ ·qElement+24(SB), R8
-	SBBQ $0, R9
-	JCS  done_13
-	MOVQ BX, 0(R11)
-	MOVQ SI, 8(R11)
-	MOVQ DI, 16(R11)
-	MOVQ R8, 24(R11)
-	SUBQ ·qElement+0(SB), BX
-	SBBQ ·qElement+8(SB), SI
-	SBBQ ·qElement+16(SB), DI
-	SBBQ ·qElement+24(SB), R8
-	SBBQ $0, R9
-	JCS  done_13
-	MOVQ BX, 0(R11)
-	MOVQ SI, 8(R11)
-	MOVQ DI, 16(R11)
-	MOVQ R8, 24(R11)
-
-done_13:
-	RET
-
-TEXT ·scalarMulVec(SB), $8-40
-#define AVX_MUL_Q_LO() \
-	VPMULUDQ.BCST ·qElement+0(SB), Z9, Z10  \
-	VPADDQ        Z10, Z0, Z0               \
-	VPMULUDQ.BCST ·qElement+4(SB), Z9, Z11  \
-	VPADDQ        Z11, Z1, Z1               \
-	VPMULUDQ.BCST ·qElement+8(SB), Z9, Z12  \
-	VPADDQ        Z12, Z2, Z2               \
-	VPMULUDQ.BCST ·qElement+12(SB), Z9, Z13 \
-	VPADDQ        Z13, Z3, Z3               \
-
-#define AVX_MUL_Q_HI() \
-	VPMULUDQ.BCST ·qElement+16(SB), Z9, Z14 \
-	VPADDQ        Z14, Z4, Z4               \
-	VPMULUDQ.BCST ·qElement+20(SB), Z9, Z15 \
-	VPADDQ        Z15, Z5, Z5               \
-	VPMULUDQ.BCST ·qElement+24(SB), Z9, Z16 \
-	VPADDQ        Z16, Z6, Z6               \
-	VPMULUDQ.BCST ·qElement+28(SB), Z9, Z17 \
-	VPADDQ        Z17, Z7, Z7               \
-
-#define SHIFT_ADD_AND(in0, in1, in2, in3) \
-	VPSRLQ $32, in0, in1 \
-	VPADDQ in1, in2, in2 \
-	VPANDQ in3, in2, in0 \
-
-#define CARRY1() \
-	SHIFT_ADD_AND(Z0, Z10, Z1, Z8) \
-	SHIFT_ADD_AND(Z1, Z11, Z2, Z8) \
-	SHIFT_ADD_AND(Z2, Z12, Z3, Z8) \
-	SHIFT_ADD_AND(Z3, Z13, Z4, Z8) \
-
-#define CARRY2() \
-	SHIFT_ADD_AND(Z4, Z14, Z5, Z8) \
-	SHIFT_ADD_AND(Z5, Z15, Z6, Z8) \
-	SHIFT_ADD_AND(Z6, Z16, Z7, Z8) \
-	VPSRLQ $32, Z7, Z7             \
-
-#define CARRY3() \
-	VPSRLQ $32, Z0, Z10 \
-	VPANDQ Z8, Z0, Z0   \
-	VPADDQ Z10, Z1, Z1  \
-	VPSRLQ $32, Z1, Z11 \
-	VPANDQ Z8, Z1, Z1   \
-	VPADDQ Z11, Z2, Z2  \
-	VPSRLQ $32, Z2, Z12 \
-	VPANDQ Z8, Z2, Z2   \
-	VPADDQ Z12, Z3, Z3  \
-	VPSRLQ $32, Z3, Z13 \
-	VPANDQ Z8, Z3, Z3   \
-	VPADDQ Z13, Z4, Z4  \
-
-#define CARRY4() \
-	VPSRLQ $32, Z4, Z14 \
-	VPANDQ Z8, Z4, Z4   \
-	VPADDQ Z14, Z5, Z5  \
-	VPSRLQ $32, Z5, Z15 \
-	VPANDQ Z8, Z5, Z5   \
-	VPADDQ Z15, Z6, Z6  \
-	VPSRLQ $32, Z6, Z16 \
-	VPANDQ Z8, Z6, Z6   \
-	VPADDQ Z16, Z7, Z7  \
-
-	// t[0] -> R14
-	// t[1] -> R13
-	// t[2] -> CX
-	// t[3] -> BX
-	// y[0] -> DI
-	// y[1] -> R8
-	// y[2] -> R9
-	// y[3] -> R10
-	MOVQ res+0(FP), SI
-	MOVQ a+8(FP), R11
-	MOVQ b+16(FP), R15
-	MOVQ n+24(FP), R12
-	MOVQ 0(R15), DI
-	MOVQ 8(R15), R8
-	MOVQ 16(R15), R9
-	MOVQ 24(R15), R10
-	MOVQ R12, s0-8(SP)
-
-	// Create mask for low dword in each qword
-	VPCMPEQB  Y8, Y8, Y8
-	VPMOVZXDQ Y8, Z8
-	MOVQ      $0x5555, DX
-	KMOVD     DX, K1
-
-loop_16:
-	TESTQ     R12, R12
-	JEQ       done_15            // n == 0, we are done
-	MOVQ      0(R11), DX
-	VMOVDQU64 256+0*64(R11), Z16
-	VMOVDQU64 256+1*64(R11), Z17
-	VMOVDQU64 256+2*64(R11), Z18
-	VMOVDQU64 256+3*64(R11), Z19
-	VMOVDQU64 0(R15), Z24
-	VMOVDQU64 0(R15), Z25
-	VMOVDQU64 0(R15), Z26
-	VMOVDQU64 0(R15), Z27
-
-	// Transpose and expand x and y
-	VSHUFI64X2 $0x88, Z17, Z16, Z20
-	VSHUFI64X2 $0xdd, Z17, Z16, Z22
-	VSHUFI64X2 $0x88, Z19, Z18, Z21
-	VSHUFI64X2 $0xdd, Z19, Z18, Z23
-	VSHUFI64X2 $0x88, Z25, Z24, Z28
-	VSHUFI64X2 $0xdd, Z25, Z24, Z30
-	VSHUFI64X2 $0x88, Z27, Z26, Z29
-	VSHUFI64X2 $0xdd, Z27, Z26, Z31
-	VPERMQ     $0xd8, Z20, Z20
-	VPERMQ     $0xd8, Z21, Z21
-	VPERMQ     $0xd8, Z22, Z22
-	VPERMQ     $0xd8, Z23, Z23
-
-	// z[0] -> y * x[0]
-	MUL_WORD_0()
-	VPERMQ     $0xd8, Z28, Z28
-	VPERMQ     $0xd8, Z29, Z29
-	VPERMQ     $0xd8, Z30, Z30
-	VPERMQ     $0xd8, Z31, Z31
-	VSHUFI64X2 $0xd8, Z20, Z20, Z20
-	VSHUFI64X2 $0xd8, Z21, Z21, Z21
-	VSHUFI64X2 $0xd8, Z22, Z22, Z22
-	VSHUFI64X2 $0xd8, Z23, Z23, Z23
-
-	// z[0] -> y * x[1]
-	MOVQ       8(R11), DX
-	MUL_WORD_N()
-	VSHUFI64X2 $0xd8, Z28, Z28, Z28
-	VSHUFI64X2 $0xd8, Z29, Z29, Z29
-	VSHUFI64X2 $0xd8, Z30, Z30, Z30
-	VSHUFI64X2 $0xd8, Z31, Z31, Z31
-	VSHUFI64X2 $0x44, Z21, Z20, Z16
-	VSHUFI64X2 $0xee, Z21, Z20, Z18
-	VSHUFI64X2 $0x44, Z23, Z22, Z20
-	VSHUFI64X2 $0xee, Z23, Z22, Z22
-
-	// z[0] -> y * x[2]
-	MOVQ       16(R11), DX
-	MUL_WORD_N()
-	VSHUFI64X2 $0x44, Z29, Z28, Z24
-	VSHUFI64X2 $0xee, Z29, Z28, Z26
-	VSHUFI64X2 $0x44, Z31, Z30, Z28
-	VSHUFI64X2 $0xee, Z31, Z30, Z30
-	PREFETCHT0 1024(R11)
-	VPSRLQ     $32, Z16, Z17
-	VPSRLQ     $32, Z18, Z19
-	VPSRLQ     $32, Z20, Z21
-	VPSRLQ     $32, Z22, Z23
-	VPSRLQ     $32, Z24, Z25
-	VPSRLQ     $32, Z26, Z27
-	VPSRLQ     $32, Z28, Z29
-	VPSRLQ     $32, Z30, Z31
-
-	// z[0] -> y * x[3]
-	MOVQ   24(R11), DX
-	MUL_WORD_N()
-	VPANDQ Z8, Z16, Z16
-	VPANDQ Z8, Z18, Z18
-	VPANDQ Z8, Z20, Z20
-	VPANDQ Z8, Z22, Z22
-	VPANDQ Z8, Z24, Z24
-	VPANDQ Z8, Z26, Z26
-	VPANDQ Z8, Z28, Z28
-	VPANDQ Z8, Z30, Z30
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[0]
-	MOVQ R14, 0(SI)
-	MOVQ R13, 8(SI)
-	MOVQ CX, 16(SI)
-	MOVQ BX, 24(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// For each 256-bit input value, each zmm register now represents a 32-bit input word zero-extended to 64 bits.
-	// Multiply y by doubleword 0 of x
-	VPMULUDQ      Z16, Z24, Z0
-	VPMULUDQ      Z16, Z25, Z1
-	VPMULUDQ      Z16, Z26, Z2
-	VPMULUDQ      Z16, Z27, Z3
-	VPMULUDQ      Z16, Z28, Z4
-	VPMULUDQ      Z16, Z29, Z5
-	VPMULUDQ      Z16, Z30, Z6
-	VPMULUDQ      Z16, Z31, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-	VPSRLQ        $32, Z0, Z10
-	VPANDQ        Z8, Z0, Z0
-	VPADDQ        Z10, Z1, Z1
-	VPSRLQ        $32, Z1, Z11
-	VPANDQ        Z8, Z1, Z1
-	VPADDQ        Z11, Z2, Z2
-	VPSRLQ        $32, Z2, Z12
-	VPANDQ        Z8, Z2, Z2
-	VPADDQ        Z12, Z3, Z3
-	VPSRLQ        $32, Z3, Z13
-	VPANDQ        Z8, Z3, Z3
-	VPADDQ        Z13, Z4, Z4
-
-	// z[1] -> y * x[0]
-	MUL_WORD_0()
-	VPSRLQ        $32, Z4, Z14
-	VPANDQ        Z8, Z4, Z4
-	VPADDQ        Z14, Z5, Z5
-	VPSRLQ        $32, Z5, Z15
-	VPANDQ        Z8, Z5, Z5
-	VPADDQ        Z15, Z6, Z6
-	VPSRLQ        $32, Z6, Z16
-	VPANDQ        Z8, Z6, Z6
-	VPADDQ        Z16, Z7, Z7
-	VPMULUDQ.BCST ·qElement+0(SB), Z9, Z10
-	VPADDQ        Z10, Z0, Z0
-	VPMULUDQ.BCST ·qElement+4(SB), Z9, Z11
-	VPADDQ        Z11, Z1, Z1
-	VPMULUDQ.BCST ·qElement+8(SB), Z9, Z12
-	VPADDQ        Z12, Z2, Z2
-	VPMULUDQ.BCST ·qElement+12(SB), Z9, Z13
-	VPADDQ        Z13, Z3, Z3
-
-	// z[1] -> y * x[1]
-	MOVQ          8(R11), DX
-	MUL_WORD_N()
-	VPMULUDQ.BCST ·qElement+16(SB), Z9, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ.BCST ·qElement+20(SB), Z9, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ.BCST ·qElement+24(SB), Z9, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ.BCST ·qElement+28(SB), Z9, Z10
-	VPADDQ        Z10, Z7, Z7
-	CARRY1()
-
-	// z[1] -> y * x[2]
-	MOVQ   16(R11), DX
-	MUL_WORD_N()
-	SHIFT_ADD_AND(Z4, Z14, Z5, Z8)
-	SHIFT_ADD_AND(Z5, Z15, Z6, Z8)
-	SHIFT_ADD_AND(Z6, Z16, Z7, Z8)
-	VPSRLQ $32, Z7, Z7
-
-	// Process doubleword 1 of x
-	VPMULUDQ Z17, Z24, Z10
-	VPADDQ   Z10, Z0, Z0
-	VPMULUDQ Z17, Z25, Z11
-	VPADDQ   Z11, Z1, Z1
-	VPMULUDQ Z17, Z26, Z12
-	VPADDQ   Z12, Z2, Z2
-	VPMULUDQ Z17, Z27, Z13
-	VPADDQ   Z13, Z3, Z3
-
-	// z[1] -> y * x[3]
-	MOVQ          24(R11), DX
-	MUL_WORD_N()
-	VPMULUDQ      Z17, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z17, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z17, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z17, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[1]
-	MOVQ R14, 32(SI)
-	MOVQ R13, 40(SI)
-	MOVQ CX, 48(SI)
-	MOVQ BX, 56(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	VPSRLQ $32, Z0, Z10
-	VPANDQ Z8, Z0, Z0
-	VPADDQ Z10, Z1, Z1
-	VPSRLQ $32, Z1, Z11
-	VPANDQ Z8, Z1, Z1
-	VPADDQ Z11, Z2, Z2
-	VPSRLQ $32, Z2, Z12
-	VPANDQ Z8, Z2, Z2
-	VPADDQ Z12, Z3, Z3
-	VPSRLQ $32, Z3, Z13
-	VPANDQ Z8, Z3, Z3
-	VPADDQ Z13, Z4, Z4
-	CARRY4()
-
-	// z[2] -> y * x[0]
-	MUL_WORD_0()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[2] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-	CARRY1()
-	CARRY2()
-
-	// z[2] -> y * x[2]
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-
-	// Process doubleword 2 of x
-	VPMULUDQ      Z18, Z24, Z10
-	VPADDQ        Z10, Z0, Z0
-	VPMULUDQ      Z18, Z25, Z11
-	VPADDQ        Z11, Z1, Z1
-	VPMULUDQ      Z18, Z26, Z12
-	VPADDQ        Z12, Z2, Z2
-	VPMULUDQ      Z18, Z27, Z13
-	VPADDQ        Z13, Z3, Z3
-	VPMULUDQ      Z18, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z18, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z18, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z18, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// z[2] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	CARRY3()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[2]
-	MOVQ R14, 64(SI)
-	MOVQ R13, 72(SI)
-	MOVQ CX, 80(SI)
-	MOVQ BX, 88(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-	CARRY4()
-	AVX_MUL_Q_LO()
-
-	// z[3] -> y * x[0]
-	MUL_WORD_0()
-	AVX_MUL_Q_HI()
-	CARRY1()
-	CARRY2()
-
-	// Process doubleword 3 of x
-	VPMULUDQ Z19, Z24, Z10
-	VPADDQ   Z10, Z0, Z0
-	VPMULUDQ Z19, Z25, Z11
-	VPADDQ   Z11, Z1, Z1
-	VPMULUDQ Z19, Z26, Z12
-	VPADDQ   Z12, Z2, Z2
-	VPMULUDQ Z19, Z27, Z13
-	VPADDQ   Z13, Z3, Z3
-
-	// z[3] -> y * x[1]
-	MOVQ     8(R11), DX
-	MUL_WORD_N()
-	VPMULUDQ Z19, Z28, Z14
-	VPADDQ   Z14, Z4, Z4
-	VPMULUDQ Z19, Z29, Z15
-	VPADDQ   Z15, Z5, Z5
-	VPMULUDQ Z19, Z30, Z16
-	VPADDQ   Z16, Z6, Z6
-	VPMULUDQ Z19, Z31, Z17
-	VPADDQ   Z17, Z7, Z7
-
-	// z[3] -> y * x[2]
-	MOVQ          16(R11), DX
-	MUL_WORD_N()
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-	CARRY3()
-	CARRY4()
-
-	// z[3] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[3]
-	MOVQ R14, 96(SI)
-	MOVQ R13, 104(SI)
-	MOVQ CX, 112(SI)
-	MOVQ BX, 120(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// Propagate carries and shift down by one dword
-	CARRY1()
-	CARRY2()
-
-	// Process doubleword 4 of x
-	VPMULUDQ Z20, Z24, Z10
-	VPADDQ   Z10, Z0, Z0
-	VPMULUDQ Z20, Z25, Z11
-	VPADDQ   Z11, Z1, Z1
-	VPMULUDQ Z20, Z26, Z12
-	VPADDQ   Z12, Z2, Z2
-	VPMULUDQ Z20, Z27, Z13
-	VPADDQ   Z13, Z3, Z3
-
-	// z[4] -> y * x[0]
-	MUL_WORD_0()
-	VPMULUDQ      Z20, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z20, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z20, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z20, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// z[4] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	CARRY3()
-	CARRY4()
-
-	// z[4] -> y * x[2]
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-
-	// zmm7 keeps all 64 bits
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[4] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-
-	// Propagate carries and shift down by one dword
-	CARRY1()
-	CARRY2()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[4]
-	MOVQ R14, 128(SI)
-	MOVQ R13, 136(SI)
-	MOVQ CX, 144(SI)
-	MOVQ BX, 152(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// Process doubleword 5 of x
-	VPMULUDQ Z21, Z24, Z10
-	VPADDQ   Z10, Z0, Z0
-	VPMULUDQ Z21, Z25, Z11
-	VPADDQ   Z11, Z1, Z1
-	VPMULUDQ Z21, Z26, Z12
-	VPADDQ   Z12, Z2, Z2
-	VPMULUDQ Z21, Z27, Z13
-	VPADDQ   Z13, Z3, Z3
-	VPMULUDQ Z21, Z28, Z14
-	VPADDQ   Z14, Z4, Z4
-	VPMULUDQ Z21, Z29, Z15
-	VPADDQ   Z15, Z5, Z5
-	VPMULUDQ Z21, Z30, Z16
-	VPADDQ   Z16, Z6, Z6
-	VPMULUDQ Z21, Z31, Z17
-	VPADDQ   Z17, Z7, Z7
-
-	// z[5] -> y * x[0]
-	MUL_WORD_0()
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	CARRY3()
-	CARRY4()
-
-	// z[5] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[5] -> y * x[2]
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-	CARRY1()
-	CARRY2()
-
-	// z[5] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-
-	// Process doubleword 6 of x
-	VPMULUDQ      Z22, Z24, Z10
-	VPADDQ        Z10, Z0, Z0
-	VPMULUDQ      Z22, Z25, Z11
-	VPADDQ        Z11, Z1, Z1
-	VPMULUDQ      Z22, Z26, Z12
-	VPADDQ        Z12, Z2, Z2
-	VPMULUDQ      Z22, Z27, Z13
-	VPADDQ        Z13, Z3, Z3
-	VPMULUDQ      Z22, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z22, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z22, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z22, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[5]
-	MOVQ R14, 160(SI)
-	MOVQ R13, 168(SI)
-	MOVQ CX, 176(SI)
-	MOVQ BX, 184(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	CARRY3()
-	CARRY4()
-
-	// z[6] -> y * x[0]
-	MUL_WORD_0()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[6] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-	CARRY1()
-	CARRY2()
-
-	// z[6] -> y * x[2]
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-
-	// Process doubleword 7 of x
-	VPMULUDQ      Z23, Z24, Z10
-	VPADDQ        Z10, Z0, Z0
-	VPMULUDQ      Z23, Z25, Z11
-	VPADDQ        Z11, Z1, Z1
-	VPMULUDQ      Z23, Z26, Z12
-	VPADDQ        Z12, Z2, Z2
-	VPMULUDQ      Z23, Z27, Z13
-	VPADDQ        Z13, Z3, Z3
-	VPMULUDQ      Z23, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z23, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z23, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z23, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// z[6] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-	CARRY3()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[6]
-	MOVQ R14, 192(SI)
-	MOVQ R13, 200(SI)
-	MOVQ CX, 208(SI)
-	MOVQ BX, 216(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-	CARRY4()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[7] -> y * x[0]
-	MUL_WORD_0()
-	CARRY1()
-	CARRY2()
-
-	// z[7] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-
-	// Conditional subtraction of the modulus
-	VPERMD.BCST.Z ·qElement+0(SB), Z8, K1, Z10
-	VPERMD.BCST.Z ·qElement+4(SB), Z8, K1, Z11
-	VPERMD.BCST.Z ·qElement+8(SB), Z8, K1, Z12
-	VPERMD.BCST.Z ·qElement+12(SB), Z8, K1, Z13
-	VPERMD.BCST.Z ·qElement+16(SB), Z8, K1, Z14
-	VPERMD.BCST.Z ·qElement+20(SB), Z8, K1, Z15
-	VPERMD.BCST.Z ·qElement+24(SB), Z8, K1, Z16
-	VPERMD.BCST.Z ·qElement+28(SB), Z8, K1, Z17
-	VPSUBQ        Z10, Z0, Z10
-	VPSRLQ        $63, Z10, Z20
-	VPANDQ        Z8, Z10, Z10
-	VPSUBQ        Z11, Z1, Z11
-	VPSUBQ        Z20, Z11, Z11
-	VPSRLQ        $63, Z11, Z21
-	VPANDQ        Z8, Z11, Z11
-	VPSUBQ        Z12, Z2, Z12
-	VPSUBQ        Z21, Z12, Z12
-	VPSRLQ        $63, Z12, Z22
-	VPANDQ        Z8, Z12, Z12
-	VPSUBQ        Z13, Z3, Z13
-	VPSUBQ        Z22, Z13, Z13
-	VPSRLQ        $63, Z13, Z23
-	VPANDQ        Z8, Z13, Z13
-	VPSUBQ        Z14, Z4, Z14
-	VPSUBQ        Z23, Z14, Z14
-	VPSRLQ        $63, Z14, Z24
-	VPANDQ        Z8, Z14, Z14
-	VPSUBQ        Z15, Z5, Z15
-	VPSUBQ        Z24, Z15, Z15
-	VPSRLQ        $63, Z15, Z25
-	VPANDQ        Z8, Z15, Z15
-	VPSUBQ        Z16, Z6, Z16
-	VPSUBQ        Z25, Z16, Z16
-	VPSRLQ        $63, Z16, Z26
-	VPANDQ        Z8, Z16, Z16
-	VPSUBQ        Z17, Z7, Z17
-	VPSUBQ        Z26, Z17, Z17
-	VPMOVQ2M      Z17, K2
-	KNOTB         K2, K2
-	VMOVDQU64     Z10, K2, Z0
-	VMOVDQU64     Z11, K2, Z1
-	VMOVDQU64     Z12, K2, Z2
-	VMOVDQU64     Z13, K2, Z3
-	VMOVDQU64     Z14, K2, Z4
-
-	// z[7] -> y * x[2]
-	MOVQ      16(R11), DX
-	MUL_WORD_N()
-	VMOVDQU64 Z15, K2, Z5
-	VMOVDQU64 Z16, K2, Z6
-	VMOVDQU64 Z17, K2, Z7
-
-	// Transpose results back
-	VALIGND   $0, ·pattern1+0(SB), Z11, Z11
-	VALIGND   $0, ·pattern2+0(SB), Z12, Z12
-	VALIGND   $0, ·pattern3+0(SB), Z13, Z13
-	VALIGND   $0, ·pattern4+0(SB), Z14, Z14
-	VPSLLQ    $32, Z1, Z1
-	VPORQ     Z1, Z0, Z0
-	VPSLLQ    $32, Z3, Z3
-	VPORQ     Z3, Z2, Z1
-	VPSLLQ    $32, Z5, Z5
-	VPORQ     Z5, Z4, Z2
-	VPSLLQ    $32, Z7, Z7
-	VPORQ     Z7, Z6, Z3
-	VMOVDQU64 Z0, Z4
-	VMOVDQU64 Z2, Z6
-
-	// z[7] -> y * x[3]
-	MOVQ     24(R11), DX
-	MUL_WORD_N()
-	VPERMT2Q Z1, Z11, Z0
-	VPERMT2Q Z4, Z12, Z1
-	VPERMT2Q Z3, Z11, Z2
-	VPERMT2Q Z6, Z12, Z3
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[7]
-	MOVQ      R14, 224(SI)
-	MOVQ      R13, 232(SI)
-	MOVQ      CX, 240(SI)
-	MOVQ      BX, 248(SI)
-	ADDQ      $288, R11
-	VMOVDQU64 Z0, Z4
-	VMOVDQU64 Z1, Z5
-	VPERMT2Q  Z2, Z13, Z0
-	VPERMT2Q  Z4, Z14, Z2
-	VPERMT2Q  Z3, Z13, Z1
-	VPERMT2Q  Z5, Z14, Z3
-
-	// Save AVX-512 results
-	VMOVDQU64 Z0, 256+0*64(SI)
-	VMOVDQU64 Z2, 256+1*64(SI)
-	VMOVDQU64 Z1, 256+2*64(SI)
-	VMOVDQU64 Z3, 256+3*64(SI)
-	ADDQ      $512, SI
-	MOVQ      s0-8(SP), R12
-	DECQ      R12              // decrement n
-	MOVQ      R12, s0-8(SP)
-	JMP       loop_16
-
-done_15:
-	RET
-
-TEXT ·mulVec(SB), $8-40
-	// t[0] -> R14
-	// t[1] -> R13
-	// t[2] -> CX
-	// t[3] -> BX
-	// y[0] -> DI
-	// y[1] -> R8
-	// y[2] -> R9
-	// y[3] -> R10
-	MOVQ res+0(FP), SI
-	MOVQ a+8(FP), R11
-	MOVQ b+16(FP), R15
-	MOVQ n+24(FP), R12
-	MOVQ R12, s0-8(SP)
-
-	// Create mask for low dword in each qword
-	VPCMPEQB  Y8, Y8, Y8
-	VPMOVZXDQ Y8, Z8
-	MOVQ      $0x5555, DX
-	KMOVD     DX, K1
-
-loop_18:
-	TESTQ     R12, R12
-	JEQ       done_17            // n == 0, we are done
-	MOVQ      0(R11), DX
-	VMOVDQU64 256+0*64(R11), Z16
-	VMOVDQU64 256+1*64(R11), Z17
-	VMOVDQU64 256+2*64(R11), Z18
-	VMOVDQU64 256+3*64(R11), Z19
-
-	// load input y[0]
-	MOVQ      0(R15), DI
-	MOVQ      8(R15), R8
-	MOVQ      16(R15), R9
-	MOVQ      24(R15), R10
-	VMOVDQU64 256+0*64(R15), Z24
-	VMOVDQU64 256+1*64(R15), Z25
-	VMOVDQU64 256+2*64(R15), Z26
-	VMOVDQU64 256+3*64(R15), Z27
-
-	// Transpose and expand x and y
-	VSHUFI64X2 $0x88, Z17, Z16, Z20
-	VSHUFI64X2 $0xdd, Z17, Z16, Z22
-	VSHUFI64X2 $0x88, Z19, Z18, Z21
-	VSHUFI64X2 $0xdd, Z19, Z18, Z23
-	VSHUFI64X2 $0x88, Z25, Z24, Z28
-	VSHUFI64X2 $0xdd, Z25, Z24, Z30
-	VSHUFI64X2 $0x88, Z27, Z26, Z29
-	VSHUFI64X2 $0xdd, Z27, Z26, Z31
-	VPERMQ     $0xd8, Z20, Z20
-	VPERMQ     $0xd8, Z21, Z21
-	VPERMQ     $0xd8, Z22, Z22
-	VPERMQ     $0xd8, Z23, Z23
-
-	// z[0] -> y * x[0]
-	MUL_WORD_0()
-	VPERMQ     $0xd8, Z28, Z28
-	VPERMQ     $0xd8, Z29, Z29
-	VPERMQ     $0xd8, Z30, Z30
-	VPERMQ     $0xd8, Z31, Z31
-	VSHUFI64X2 $0xd8, Z20, Z20, Z20
-	VSHUFI64X2 $0xd8, Z21, Z21, Z21
-	VSHUFI64X2 $0xd8, Z22, Z22, Z22
-	VSHUFI64X2 $0xd8, Z23, Z23, Z23
-
-	// z[0] -> y * x[1]
-	MOVQ       8(R11), DX
-	MUL_WORD_N()
-	VSHUFI64X2 $0xd8, Z28, Z28, Z28
-	VSHUFI64X2 $0xd8, Z29, Z29, Z29
-	VSHUFI64X2 $0xd8, Z30, Z30, Z30
-	VSHUFI64X2 $0xd8, Z31, Z31, Z31
-	VSHUFI64X2 $0x44, Z21, Z20, Z16
-	VSHUFI64X2 $0xee, Z21, Z20, Z18
-	VSHUFI64X2 $0x44, Z23, Z22, Z20
-	VSHUFI64X2 $0xee, Z23, Z22, Z22
-
-	// z[0] -> y * x[2]
-	MOVQ       16(R11), DX
-	MUL_WORD_N()
-	VSHUFI64X2 $0x44, Z29, Z28, Z24
-	VSHUFI64X2 $0xee, Z29, Z28, Z26
-	VSHUFI64X2 $0x44, Z31, Z30, Z28
-	VSHUFI64X2 $0xee, Z31, Z30, Z30
-	PREFETCHT0 1024(R11)
-	VPSRLQ     $32, Z16, Z17
-	VPSRLQ     $32, Z18, Z19
-	VPSRLQ     $32, Z20, Z21
-	VPSRLQ     $32, Z22, Z23
-	VPSRLQ     $32, Z24, Z25
-	VPSRLQ     $32, Z26, Z27
-	VPSRLQ     $32, Z28, Z29
-	VPSRLQ     $32, Z30, Z31
-
-	// z[0] -> y * x[3]
-	MOVQ   24(R11), DX
-	MUL_WORD_N()
-	VPANDQ Z8, Z16, Z16
-	VPANDQ Z8, Z18, Z18
-	VPANDQ Z8, Z20, Z20
-	VPANDQ Z8, Z22, Z22
-	VPANDQ Z8, Z24, Z24
-	VPANDQ Z8, Z26, Z26
-	VPANDQ Z8, Z28, Z28
-	VPANDQ Z8, Z30, Z30
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[0]
-	MOVQ R14, 0(SI)
-	MOVQ R13, 8(SI)
-	MOVQ CX, 16(SI)
-	MOVQ BX, 24(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// For each 256-bit input value, each zmm register now represents a 32-bit input word zero-extended to 64 bits.
-	// Multiply y by doubleword 0 of x
-	VPMULUDQ   Z16, Z24, Z0
-	VPMULUDQ   Z16, Z25, Z1
-	VPMULUDQ   Z16, Z26, Z2
-	VPMULUDQ   Z16, Z27, Z3
-	VPMULUDQ   Z16, Z28, Z4
-	PREFETCHT0 1024(R15)
-	VPMULUDQ   Z16, Z29, Z5
-	VPMULUDQ   Z16, Z30, Z6
-	VPMULUDQ   Z16, Z31, Z7
-
-	// load input y[1]
-	MOVQ          32(R15), DI
-	MOVQ          40(R15), R8
-	MOVQ          48(R15), R9
-	MOVQ          56(R15), R10
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-	VPSRLQ        $32, Z0, Z10
-	VPANDQ        Z8, Z0, Z0
-	VPADDQ        Z10, Z1, Z1
-	VPSRLQ        $32, Z1, Z11
-	VPANDQ        Z8, Z1, Z1
-	VPADDQ        Z11, Z2, Z2
-	VPSRLQ        $32, Z2, Z12
-	VPANDQ        Z8, Z2, Z2
-	VPADDQ        Z12, Z3, Z3
-	VPSRLQ        $32, Z3, Z13
-	VPANDQ        Z8, Z3, Z3
-	VPADDQ        Z13, Z4, Z4
-
-	// z[1] -> y * x[0]
-	MUL_WORD_0()
-	VPSRLQ        $32, Z4, Z14
-	VPANDQ        Z8, Z4, Z4
-	VPADDQ        Z14, Z5, Z5
-	VPSRLQ        $32, Z5, Z15
-	VPANDQ        Z8, Z5, Z5
-	VPADDQ        Z15, Z6, Z6
-	VPSRLQ        $32, Z6, Z16
-	VPANDQ        Z8, Z6, Z6
-	VPADDQ        Z16, Z7, Z7
-	VPMULUDQ.BCST ·qElement+0(SB), Z9, Z10
-	VPADDQ        Z10, Z0, Z0
-	VPMULUDQ.BCST ·qElement+4(SB), Z9, Z11
-	VPADDQ        Z11, Z1, Z1
-	VPMULUDQ.BCST ·qElement+8(SB), Z9, Z12
-	VPADDQ        Z12, Z2, Z2
-	VPMULUDQ.BCST ·qElement+12(SB), Z9, Z13
-	VPADDQ        Z13, Z3, Z3
-
-	// z[1] -> y * x[1]
-	MOVQ          8(R11), DX
-	MUL_WORD_N()
-	VPMULUDQ.BCST ·qElement+16(SB), Z9, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ.BCST ·qElement+20(SB), Z9, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ.BCST ·qElement+24(SB), Z9, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ.BCST ·qElement+28(SB), Z9, Z10
-	VPADDQ        Z10, Z7, Z7
-	CARRY1()
-
-	// z[1] -> y * x[2]
-	MOVQ   16(R11), DX
-	MUL_WORD_N()
-	SHIFT_ADD_AND(Z4, Z14, Z5, Z8)
-	SHIFT_ADD_AND(Z5, Z15, Z6, Z8)
-	SHIFT_ADD_AND(Z6, Z16, Z7, Z8)
-	VPSRLQ $32, Z7, Z7
-
-	// Process doubleword 1 of x
-	VPMULUDQ Z17, Z24, Z10
-	VPADDQ   Z10, Z0, Z0
-	VPMULUDQ Z17, Z25, Z11
-	VPADDQ   Z11, Z1, Z1
-	VPMULUDQ Z17, Z26, Z12
-	VPADDQ   Z12, Z2, Z2
-	VPMULUDQ Z17, Z27, Z13
-	VPADDQ   Z13, Z3, Z3
-
-	// z[1] -> y * x[3]
-	MOVQ          24(R11), DX
-	MUL_WORD_N()
-	VPMULUDQ      Z17, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z17, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z17, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z17, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[1]
-	MOVQ R14, 32(SI)
-	MOVQ R13, 40(SI)
-	MOVQ CX, 48(SI)
-	MOVQ BX, 56(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	VPSRLQ $32, Z0, Z10
-	VPANDQ Z8, Z0, Z0
-	VPADDQ Z10, Z1, Z1
-	VPSRLQ $32, Z1, Z11
-	VPANDQ Z8, Z1, Z1
-	VPADDQ Z11, Z2, Z2
-	VPSRLQ $32, Z2, Z12
-	VPANDQ Z8, Z2, Z2
-	VPADDQ Z12, Z3, Z3
-
-	// load input y[2]
-	MOVQ   64(R15), DI
-	MOVQ   72(R15), R8
-	MOVQ   80(R15), R9
-	MOVQ   88(R15), R10
-	VPSRLQ $32, Z3, Z13
-	VPANDQ Z8, Z3, Z3
-	VPADDQ Z13, Z4, Z4
-	CARRY4()
-
-	// z[2] -> y * x[0]
-	MUL_WORD_0()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[2] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-	CARRY1()
-	CARRY2()
-
-	// z[2] -> y * x[2]
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-
-	// Process doubleword 2 of x
-	VPMULUDQ      Z18, Z24, Z10
-	VPADDQ        Z10, Z0, Z0
-	VPMULUDQ      Z18, Z25, Z11
-	VPADDQ        Z11, Z1, Z1
-	VPMULUDQ      Z18, Z26, Z12
-	VPADDQ        Z12, Z2, Z2
-	VPMULUDQ      Z18, Z27, Z13
-	VPADDQ        Z13, Z3, Z3
-	VPMULUDQ      Z18, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z18, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z18, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z18, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// z[2] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	CARRY3()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[2]
-	MOVQ R14, 64(SI)
-	MOVQ R13, 72(SI)
-	MOVQ CX, 80(SI)
-	MOVQ BX, 88(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// load input y[3]
-	MOVQ 96(R15), DI
-	MOVQ 104(R15), R8
-	MOVQ 112(R15), R9
-	MOVQ 120(R15), R10
-	CARRY4()
-	AVX_MUL_Q_LO()
-
-	// z[3] -> y * x[0]
-	MUL_WORD_0()
-	AVX_MUL_Q_HI()
-	CARRY1()
-	CARRY2()
-
-	// Process doubleword 3 of x
-	VPMULUDQ Z19, Z24, Z10
-	VPADDQ   Z10, Z0, Z0
-	VPMULUDQ Z19, Z25, Z11
-	VPADDQ   Z11, Z1, Z1
-	VPMULUDQ Z19, Z26, Z12
-	VPADDQ   Z12, Z2, Z2
-	VPMULUDQ Z19, Z27, Z13
-	VPADDQ   Z13, Z3, Z3
-
-	// z[3] -> y * x[1]
-	MOVQ     8(R11), DX
-	MUL_WORD_N()
-	VPMULUDQ Z19, Z28, Z14
-	VPADDQ   Z14, Z4, Z4
-	VPMULUDQ Z19, Z29, Z15
-	VPADDQ   Z15, Z5, Z5
-	VPMULUDQ Z19, Z30, Z16
-	VPADDQ   Z16, Z6, Z6
-	VPMULUDQ Z19, Z31, Z17
-	VPADDQ   Z17, Z7, Z7
-
-	// z[3] -> y * x[2]
-	MOVQ          16(R11), DX
-	MUL_WORD_N()
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-	CARRY3()
-	CARRY4()
-
-	// z[3] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[3]
-	MOVQ R14, 96(SI)
-	MOVQ R13, 104(SI)
-	MOVQ CX, 112(SI)
-	MOVQ BX, 120(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// Propagate carries and shift down by one dword
-	CARRY1()
-	CARRY2()
-
-	// load input y[4]
-	MOVQ 128(R15), DI
-	MOVQ 136(R15), R8
-	MOVQ 144(R15), R9
-	MOVQ 152(R15), R10
-
-	// Process doubleword 4 of x
-	VPMULUDQ Z20, Z24, Z10
-	VPADDQ   Z10, Z0, Z0
-	VPMULUDQ Z20, Z25, Z11
-	VPADDQ   Z11, Z1, Z1
-	VPMULUDQ Z20, Z26, Z12
-	VPADDQ   Z12, Z2, Z2
-	VPMULUDQ Z20, Z27, Z13
-	VPADDQ   Z13, Z3, Z3
-
-	// z[4] -> y * x[0]
-	MUL_WORD_0()
-	VPMULUDQ      Z20, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z20, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z20, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z20, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// z[4] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	CARRY3()
-	CARRY4()
-
-	// z[4] -> y * x[2]
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-
-	// zmm7 keeps all 64 bits
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[4] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-
-	// Propagate carries and shift down by one dword
-	CARRY1()
-	CARRY2()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[4]
-	MOVQ R14, 128(SI)
-	MOVQ R13, 136(SI)
-	MOVQ CX, 144(SI)
-	MOVQ BX, 152(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// Process doubleword 5 of x
-	VPMULUDQ Z21, Z24, Z10
-	VPADDQ   Z10, Z0, Z0
-	VPMULUDQ Z21, Z25, Z11
-	VPADDQ   Z11, Z1, Z1
-	VPMULUDQ Z21, Z26, Z12
-	VPADDQ   Z12, Z2, Z2
-	VPMULUDQ Z21, Z27, Z13
-	VPADDQ   Z13, Z3, Z3
-
-	// load input y[5]
-	MOVQ     160(R15), DI
-	MOVQ     168(R15), R8
-	MOVQ     176(R15), R9
-	MOVQ     184(R15), R10
-	VPMULUDQ Z21, Z28, Z14
-	VPADDQ   Z14, Z4, Z4
-	VPMULUDQ Z21, Z29, Z15
-	VPADDQ   Z15, Z5, Z5
-	VPMULUDQ Z21, Z30, Z16
-	VPADDQ   Z16, Z6, Z6
-	VPMULUDQ Z21, Z31, Z17
-	VPADDQ   Z17, Z7, Z7
-
-	// z[5] -> y * x[0]
-	MUL_WORD_0()
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	CARRY3()
-	CARRY4()
-
-	// z[5] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[5] -> y * x[2]
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-	CARRY1()
-	CARRY2()
-
-	// z[5] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-
-	// Process doubleword 6 of x
-	VPMULUDQ      Z22, Z24, Z10
-	VPADDQ        Z10, Z0, Z0
-	VPMULUDQ      Z22, Z25, Z11
-	VPADDQ        Z11, Z1, Z1
-	VPMULUDQ      Z22, Z26, Z12
-	VPADDQ        Z12, Z2, Z2
-	VPMULUDQ      Z22, Z27, Z13
-	VPADDQ        Z13, Z3, Z3
-	VPMULUDQ      Z22, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z22, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z22, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z22, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[5]
-	MOVQ R14, 160(SI)
-	MOVQ R13, 168(SI)
-	MOVQ CX, 176(SI)
-	MOVQ BX, 184(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-
-	// Move high dwords to zmm10-16, add each to the corresponding low dword (propagate 32-bit carries)
-	CARRY3()
-
-	// load input y[6]
-	MOVQ 192(R15), DI
-	MOVQ 200(R15), R8
-	MOVQ 208(R15), R9
-	MOVQ 216(R15), R10
-	CARRY4()
-
-	// z[6] -> y * x[0]
-	MUL_WORD_0()
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[6] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-	CARRY1()
-	CARRY2()
-
-	// z[6] -> y * x[2]
-	MOVQ 16(R11), DX
-	MUL_WORD_N()
-
-	// Process doubleword 7 of x
-	VPMULUDQ      Z23, Z24, Z10
-	VPADDQ        Z10, Z0, Z0
-	VPMULUDQ      Z23, Z25, Z11
-	VPADDQ        Z11, Z1, Z1
-	VPMULUDQ      Z23, Z26, Z12
-	VPADDQ        Z12, Z2, Z2
-	VPMULUDQ      Z23, Z27, Z13
-	VPADDQ        Z13, Z3, Z3
-	VPMULUDQ      Z23, Z28, Z14
-	VPADDQ        Z14, Z4, Z4
-	VPMULUDQ      Z23, Z29, Z15
-	VPADDQ        Z15, Z5, Z5
-	VPMULUDQ      Z23, Z30, Z16
-	VPADDQ        Z16, Z6, Z6
-	VPMULUDQ      Z23, Z31, Z17
-	VPADDQ        Z17, Z7, Z7
-	VPMULUDQ.BCST qInvNeg+32(FP), Z0, Z9
-
-	// z[6] -> y * x[3]
-	MOVQ 24(R11), DX
-	MUL_WORD_N()
-	CARRY3()
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[6]
-	MOVQ R14, 192(SI)
-	MOVQ R13, 200(SI)
-	MOVQ CX, 208(SI)
-	MOVQ BX, 216(SI)
-	ADDQ $32, R11
-	MOVQ 0(R11), DX
-	CARRY4()
-
-	// load input y[7]
-	MOVQ 224(R15), DI
-	MOVQ 232(R15), R8
-	MOVQ 240(R15), R9
-	MOVQ 248(R15), R10
-	AVX_MUL_Q_LO()
-	AVX_MUL_Q_HI()
-
-	// z[7] -> y * x[0]
-	MUL_WORD_0()
-	CARRY1()
-	CARRY2()
-
-	// z[7] -> y * x[1]
-	MOVQ 8(R11), DX
-	MUL_WORD_N()
-
-	// Conditional subtraction of the modulus
-	VPERMD.BCST.Z ·qElement+0(SB), Z8, K1, Z10
-	VPERMD.BCST.Z ·qElement+4(SB), Z8, K1, Z11
-	VPERMD.BCST.Z ·qElement+8(SB), Z8, K1, Z12
-	VPERMD.BCST.Z ·qElement+12(SB), Z8, K1, Z13
-	VPERMD.BCST.Z ·qElement+16(SB), Z8, K1, Z14
-	VPERMD.BCST.Z ·qElement+20(SB), Z8, K1, Z15
-	VPERMD.BCST.Z ·qElement+24(SB), Z8, K1, Z16
-	VPERMD.BCST.Z ·qElement+28(SB), Z8, K1, Z17
-	VPSUBQ        Z10, Z0, Z10
-	VPSRLQ        $63, Z10, Z20
-	VPANDQ        Z8, Z10, Z10
-	VPSUBQ        Z11, Z1, Z11
-	VPSUBQ        Z20, Z11, Z11
-	VPSRLQ        $63, Z11, Z21
-	VPANDQ        Z8, Z11, Z11
-	VPSUBQ        Z12, Z2, Z12
-	VPSUBQ        Z21, Z12, Z12
-	VPSRLQ        $63, Z12, Z22
-	VPANDQ        Z8, Z12, Z12
-	VPSUBQ        Z13, Z3, Z13
-	VPSUBQ        Z22, Z13, Z13
-	VPSRLQ        $63, Z13, Z23
-	VPANDQ        Z8, Z13, Z13
-	VPSUBQ        Z14, Z4, Z14
-	VPSUBQ        Z23, Z14, Z14
-	VPSRLQ        $63, Z14, Z24
-	VPANDQ        Z8, Z14, Z14
-	VPSUBQ        Z15, Z5, Z15
-	VPSUBQ        Z24, Z15, Z15
-	VPSRLQ        $63, Z15, Z25
-	VPANDQ        Z8, Z15, Z15
-	VPSUBQ        Z16, Z6, Z16
-	VPSUBQ        Z25, Z16, Z16
-	VPSRLQ        $63, Z16, Z26
-	VPANDQ        Z8, Z16, Z16
-	VPSUBQ        Z17, Z7, Z17
-	VPSUBQ        Z26, Z17, Z17
-	VPMOVQ2M      Z17, K2
-	KNOTB         K2, K2
-	VMOVDQU64     Z10, K2, Z0
-	VMOVDQU64     Z11, K2, Z1
-	VMOVDQU64     Z12, K2, Z2
-	VMOVDQU64     Z13, K2, Z3
-	VMOVDQU64     Z14, K2, Z4
-
-	// z[7] -> y * x[2]
-	MOVQ      16(R11), DX
-	MUL_WORD_N()
-	VMOVDQU64 Z15, K2, Z5
-	VMOVDQU64 Z16, K2, Z6
-	VMOVDQU64 Z17, K2, Z7
-
-	// Transpose results back
-	VALIGND   $0, ·pattern1+0(SB), Z11, Z11
-	VALIGND   $0, ·pattern2+0(SB), Z12, Z12
-	VALIGND   $0, ·pattern3+0(SB), Z13, Z13
-	VALIGND   $0, ·pattern4+0(SB), Z14, Z14
-	VPSLLQ    $32, Z1, Z1
-	VPORQ     Z1, Z0, Z0
-	VPSLLQ    $32, Z3, Z3
-	VPORQ     Z3, Z2, Z1
-	VPSLLQ    $32, Z5, Z5
-	VPORQ     Z5, Z4, Z2
-	VPSLLQ    $32, Z7, Z7
-	VPORQ     Z7, Z6, Z3
-	VMOVDQU64 Z0, Z4
-	VMOVDQU64 Z2, Z6
-
-	// z[7] -> y * x[3]
-	MOVQ     24(R11), DX
-	MUL_WORD_N()
-	VPERMT2Q Z1, Z11, Z0
-	VPERMT2Q Z4, Z12, Z1
-	VPERMT2Q Z3, Z11, Z2
-	VPERMT2Q Z6, Z12, Z3
-
-	// reduce element(R14,R13,CX,BX) using temp registers (BP,R12,AX,DX)
-	REDUCE(R14,R13,CX,BX,BP,R12,AX,DX)
-
-	// store output z[7]
-	MOVQ      R14, 224(SI)
-	MOVQ      R13, 232(SI)
-	MOVQ      CX, 240(SI)
-	MOVQ      BX, 248(SI)
-	ADDQ      $288, R11
-	VMOVDQU64 Z0, Z4
-	VMOVDQU64 Z1, Z5
-	VPERMT2Q  Z2, Z13, Z0
-	VPERMT2Q  Z4, Z14, Z2
-	VPERMT2Q  Z3, Z13, Z1
-	VPERMT2Q  Z5, Z14, Z3
-
-	// Save AVX-512 results
-	VMOVDQU64 Z0, 256+0*64(SI)
-	VMOVDQU64 Z2, 256+1*64(SI)
-	VMOVDQU64 Z1, 256+2*64(SI)
-	VMOVDQU64 Z3, 256+3*64(SI)
-	ADDQ      $512, SI
-	ADDQ      $512, R15
-	MOVQ      s0-8(SP), R12
-	DECQ      R12              // decrement n
-	MOVQ      R12, s0-8(SP)
-	JMP       loop_18
-
-done_17:
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_4w/element_4w_arm64.s b/field/asm/element_4w/element_4w_arm64.s
--- a/field/asm/element_4w/element_4w_arm64.s
+++ b/field/asm/element_4w/element_4w_arm64.s
@@ -1,163 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-// butterfly(a, b *Element)
-// a, b = a+b, a-b
-TEXT ·Butterfly(SB), NOFRAME|NOSPLIT, $0-16
-	LDP  x+0(FP), (R16, R17)
-	LDP  0(R16), (R0, R1)
-	LDP  16(R16), (R2, R3)
-	LDP  0(R17), (R4, R5)
-	LDP  16(R17), (R6, R7)
-	ADDS R0, R4, R8
-	ADCS R1, R5, R9
-	ADCS R2, R6, R10
-	ADC  R3, R7, R11
-	SUBS R4, R0, R4
-	SBCS R5, R1, R5
-	SBCS R6, R2, R6
-	SBCS R7, R3, R7
-	LDP  ·qElement+0(SB), (R0, R1)
-	CSEL CS, ZR, R0, R12
-	CSEL CS, ZR, R1, R13
-	LDP  ·qElement+16(SB), (R2, R3)
-	CSEL CS, ZR, R2, R14
-	CSEL CS, ZR, R3, R15
-
-	// add q if underflow, 0 if not
-	ADDS R4, R12, R4
-	ADCS R5, R13, R5
-	STP  (R4, R5), 0(R17)
-	ADCS R6, R14, R6
-	ADC  R7, R15, R7
-	STP  (R6, R7), 16(R17)
-
-	// q = t - q
-	SUBS R0, R8, R0
-	SBCS R1, R9, R1
-	SBCS R2, R10, R2
-	SBCS R3, R11, R3
-
-	// if no borrow, return q, else return t
-	CSEL CS, R0, R8, R8
-	CSEL CS, R1, R9, R9
-	STP  (R8, R9), 0(R16)
-	CSEL CS, R2, R10, R10
-	CSEL CS, R3, R11, R11
-	STP  (R10, R11), 16(R16)
-	RET
-
-// mul(res, x, y *Element)
-// Algorithm 2 of Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS
-// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-TEXT ·mul(SB), NOFRAME|NOSPLIT, $0-24
-#define DIVSHIFT() \
-	MUL   R13, R12, R0 \
-	ADDS  R0, R6, R6   \
-	MUL   R14, R12, R0 \
-	ADCS  R0, R7, R7   \
-	MUL   R15, R12, R0 \
-	ADCS  R0, R8, R8   \
-	MUL   R16, R12, R0 \
-	ADCS  R0, R9, R9   \
-	ADC   R10, ZR, R10 \
-	UMULH R13, R12, R0 \
-	ADDS  R0, R7, R6   \
-	UMULH R14, R12, R0 \
-	ADCS  R0, R8, R7   \
-	UMULH R15, R12, R0 \
-	ADCS  R0, R9, R8   \
-	UMULH R16, R12, R0 \
-	ADCS  R0, R10, R9  \
-
-#define MUL_WORD_N() \
-	MUL   R2, R1, R0   \
-	ADDS  R0, R6, R6   \
-	MUL   R6, R11, R12 \
-	MUL   R3, R1, R0   \
-	ADCS  R0, R7, R7   \
-	MUL   R4, R1, R0   \
-	ADCS  R0, R8, R8   \
-	MUL   R5, R1, R0   \
-	ADCS  R0, R9, R9   \
-	ADC   ZR, ZR, R10  \
-	UMULH R2, R1, R0   \
-	ADDS  R0, R7, R7   \
-	UMULH R3, R1, R0   \
-	ADCS  R0, R8, R8   \
-	UMULH R4, R1, R0   \
-	ADCS  R0, R9, R9   \
-	UMULH R5, R1, R0   \
-	ADC   R0, R10, R10 \
-	DIVSHIFT()         \
-
-#define MUL_WORD_0() \
-	MUL   R2, R1, R6   \
-	MUL   R3, R1, R7   \
-	MUL   R4, R1, R8   \
-	MUL   R5, R1, R9   \
-	UMULH R2, R1, R0   \
-	ADDS  R0, R7, R7   \
-	UMULH R3, R1, R0   \
-	ADCS  R0, R8, R8   \
-	UMULH R4, R1, R0   \
-	ADCS  R0, R9, R9   \
-	UMULH R5, R1, R0   \
-	ADC   R0, ZR, R10  \
-	MUL   R6, R11, R12 \
-	DIVSHIFT()         \
-
-	MOVD y+16(FP), R17
-	MOVD x+8(FP), R0
-	LDP  0(R0), (R2, R3)
-	LDP  16(R0), (R4, R5)
-	MOVD 0(R17), R1
-	MOVD $const_qInvNeg, R11
-	LDP  ·qElement+0(SB), (R13, R14)
-	LDP  ·qElement+16(SB), (R15, R16)
-	MUL_WORD_0()
-	MOVD 8(R17), R1
-	MUL_WORD_N()
-	MOVD 16(R17), R1
-	MUL_WORD_N()
-	MOVD 24(R17), R1
-	MUL_WORD_N()
-
-	// reduce if necessary
-	SUBS R13, R6, R13
-	SBCS R14, R7, R14
-	SBCS R15, R8, R15
-	SBCS R16, R9, R16
-	MOVD res+0(FP), R0
-	CSEL CS, R13, R6, R6
-	CSEL CS, R14, R7, R7
-	STP  (R6, R7), 0(R0)
-	CSEL CS, R15, R8, R8
-	CSEL CS, R16, R9, R9
-	STP  (R8, R9), 16(R0)
-	RET
-
-// reduce(res *Element)
-TEXT ·reduce(SB), NOFRAME|NOSPLIT, $0-8
-	LDP  ·qElement+0(SB), (R4, R5)
-	LDP  ·qElement+16(SB), (R6, R7)
-	MOVD res+0(FP), R8
-	LDP  0(R8), (R0, R1)
-	LDP  16(R8), (R2, R3)
-
-	// q = t - q
-	SUBS R4, R0, R4
-	SBCS R5, R1, R5
-	SBCS R6, R2, R6
-	SBCS R7, R3, R7
-
-	// if no borrow, return q, else return t
-	CSEL CS, R4, R0, R0
-	CSEL CS, R5, R1, R1
-	STP  (R0, R1), 0(R8)
-	CSEL CS, R6, R2, R2
-	CSEL CS, R7, R3, R3
-	STP  (R2, R3), 16(R8)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_5w/element_5w_amd64.s b/field/asm/element_5w/element_5w_amd64.s
--- a/field/asm/element_5w/element_5w_amd64.s
+++ b/field/asm/element_5w/element_5w_amd64.s
@@ -1,563 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define REDUCE(ra0, ra1, ra2, ra3, ra4, rb0, rb1, rb2, rb3, rb4) \
-	MOVQ    ra0, rb0;              \
-	SUBQ    ·qElement(SB), ra0;    \
-	MOVQ    ra1, rb1;              \
-	SBBQ    ·qElement+8(SB), ra1;  \
-	MOVQ    ra2, rb2;              \
-	SBBQ    ·qElement+16(SB), ra2; \
-	MOVQ    ra3, rb3;              \
-	SBBQ    ·qElement+24(SB), ra3; \
-	MOVQ    ra4, rb4;              \
-	SBBQ    ·qElement+32(SB), ra4; \
-	CMOVQCS rb0, ra0;              \
-	CMOVQCS rb1, ra1;              \
-	CMOVQCS rb2, ra2;              \
-	CMOVQCS rb3, ra3;              \
-	CMOVQCS rb4, ra4;              \
-
-TEXT ·reduce(SB), NOSPLIT, $0-8
-	MOVQ res+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	RET
-
-// MulBy3(x *Element)
-TEXT ·MulBy3(SB), NOSPLIT, $0-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R13,R14,R15,R8,R9)
-	REDUCE(DX,CX,BX,SI,DI,R13,R14,R15,R8,R9)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	RET
-
-// MulBy5(x *Element)
-TEXT ·MulBy5(SB), NOSPLIT, $0-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R13,R14,R15,R8,R9)
-	REDUCE(DX,CX,BX,SI,DI,R13,R14,R15,R8,R9)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R10,R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,DI,R10,R11,R12,R13,R14)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	RET
-
-// MulBy13(x *Element)
-TEXT ·MulBy13(SB), $16-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R13,R14,R15,s0-8(SP),s1-16(SP))
-	REDUCE(DX,CX,BX,SI,DI,R13,R14,R15,s0-8(SP),s1-16(SP))
-
-	MOVQ DX, R13
-	MOVQ CX, R14
-	MOVQ BX, R15
-	MOVQ SI, s0-8(SP)
-	MOVQ DI, s1-16(SP)
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	ADDQ R13, DX
-	ADCQ R14, CX
-	ADCQ R15, BX
-	ADCQ s0-8(SP), SI
-	ADCQ s1-16(SP), DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-
-	// reduce element(DX,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	RET
-
-// Butterfly(a, b *Element) sets a = a + b; b = a - b
-TEXT ·Butterfly(SB), $24-16
-	MOVQ    a+0(FP), AX
-	MOVQ    0(AX), CX
-	MOVQ    8(AX), BX
-	MOVQ    16(AX), SI
-	MOVQ    24(AX), DI
-	MOVQ    32(AX), R8
-	MOVQ    CX, R9
-	MOVQ    BX, R10
-	MOVQ    SI, R11
-	MOVQ    DI, R12
-	MOVQ    R8, R13
-	XORQ    AX, AX
-	MOVQ    b+8(FP), DX
-	ADDQ    0(DX), CX
-	ADCQ    8(DX), BX
-	ADCQ    16(DX), SI
-	ADCQ    24(DX), DI
-	ADCQ    32(DX), R8
-	SUBQ    0(DX), R9
-	SBBQ    8(DX), R10
-	SBBQ    16(DX), R11
-	SBBQ    24(DX), R12
-	SBBQ    32(DX), R13
-	MOVQ    CX, R14
-	MOVQ    BX, R15
-	MOVQ    SI, s0-8(SP)
-	MOVQ    DI, s1-16(SP)
-	MOVQ    R8, s2-24(SP)
-	MOVQ    $const_q0, CX
-	MOVQ    $const_q1, BX
-	MOVQ    $const_q2, SI
-	MOVQ    $const_q3, DI
-	MOVQ    $const_q4, R8
-	CMOVQCC AX, CX
-	CMOVQCC AX, BX
-	CMOVQCC AX, SI
-	CMOVQCC AX, DI
-	CMOVQCC AX, R8
-	ADDQ    CX, R9
-	ADCQ    BX, R10
-	ADCQ    SI, R11
-	ADCQ    DI, R12
-	ADCQ    R8, R13
-	MOVQ    R14, CX
-	MOVQ    R15, BX
-	MOVQ    s0-8(SP), SI
-	MOVQ    s1-16(SP), DI
-	MOVQ    s2-24(SP), R8
-	MOVQ    R9, 0(DX)
-	MOVQ    R10, 8(DX)
-	MOVQ    R11, 16(DX)
-	MOVQ    R12, 24(DX)
-	MOVQ    R13, 32(DX)
-
-	// reduce element(CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13)
-
-	MOVQ a+0(FP), AX
-	MOVQ CX, 0(AX)
-	MOVQ BX, 8(AX)
-	MOVQ SI, 16(AX)
-	MOVQ DI, 24(AX)
-	MOVQ R8, 32(AX)
-	RET
-
-// mul(res, x, y *Element)
-TEXT ·mul(SB), $24-24
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
-
-	NO_LOCAL_POINTERS
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_1
-	MOVQ x+8(FP), DI
-
-	// x[0] -> R9
-	// x[1] -> R10
-	// x[2] -> R11
-	MOVQ 0(DI), R9
-	MOVQ 8(DI), R10
-	MOVQ 16(DI), R11
-	MOVQ y+16(FP), R12
-
-	// A -> BP
-	// t[0] -> R14
-	// t[1] -> R13
-	// t[2] -> CX
-	// t[3] -> BX
-	// t[4] -> SI
-#define MACC(in0, in1, in2) \
-	ADCXQ in0, in1     \
-	MULXQ in2, AX, in0 \
-	ADOXQ AX, in1      \
-
-#define DIV_SHIFT() \
-	MOVQ  $const_qInvNeg, DX        \
-	IMULQ R14, DX                   \
-	XORQ  AX, AX                    \
-	MULXQ ·qElement+0(SB), AX, R8   \
-	ADCXQ R14, AX                   \
-	MOVQ  R8, R14                   \
-	MACC(R13, R14, ·qElement+8(SB)) \
-	MACC(CX, R13, ·qElement+16(SB)) \
-	MACC(BX, CX, ·qElement+24(SB))  \
-	MACC(SI, BX, ·qElement+32(SB))  \
-	MOVQ  $0, AX                    \
-	ADCXQ AX, SI                    \
-	ADOXQ BP, SI                    \
-
-#define MUL_WORD_0() \
-	XORQ  AX, AX         \
-	MULXQ R9, R14, R13   \
-	MULXQ R10, AX, CX    \
-	ADOXQ AX, R13        \
-	MULXQ R11, AX, BX    \
-	ADOXQ AX, CX         \
-	MULXQ 24(DI), AX, SI \
-	ADOXQ AX, BX         \
-	MULXQ 32(DI), AX, BP \
-	ADOXQ AX, SI         \
-	MOVQ  $0, AX         \
-	ADOXQ AX, BP         \
-	DIV_SHIFT()          \
-
-#define MUL_WORD_N() \
-	XORQ  AX, AX         \
-	MULXQ R9, AX, BP     \
-	ADOXQ AX, R14        \
-	MACC(BP, R13, R10)   \
-	MACC(BP, CX, R11)    \
-	MACC(BP, BX, 24(DI)) \
-	MACC(BP, SI, 32(DI)) \
-	MOVQ  $0, AX         \
-	ADCXQ AX, BP         \
-	ADOXQ AX, BP         \
-	DIV_SHIFT()          \
-
-	// mul body
-	MOVQ 0(R12), DX
-	MUL_WORD_0()
-	MOVQ 8(R12), DX
-	MUL_WORD_N()
-	MOVQ 16(R12), DX
-	MUL_WORD_N()
-	MOVQ 24(R12), DX
-	MUL_WORD_N()
-	MOVQ 32(R12), DX
-	MUL_WORD_N()
-
-	// reduce element(R14,R13,CX,BX,SI) using temp registers (R8,DI,R12,R9,R10)
-	REDUCE(R14,R13,CX,BX,SI,R8,DI,R12,R9,R10)
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R13, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	MOVQ SI, 32(AX)
-	RET
-
-noAdx_1:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	MOVQ y+16(FP), AX
-	MOVQ AX, 16(SP)
-	CALL ·_mulGeneric(SB)
-	RET
-
-TEXT ·fromMont(SB), $8-8
-	NO_LOCAL_POINTERS
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// when y = 1 we have:
-	// for i=0 to N-1
-	// 		t[i] = x[i]
-	// for i=0 to N-1
-	// 		m := t[0]*q'[0] mod W
-	// 		C,_ := t[0] + m*q[0]
-	// 		for j=1 to N-1
-	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
-	// 		t[N-1] = C
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_2
-	MOVQ res+0(FP), DX
-	MOVQ 0(DX), R14
-	MOVQ 8(DX), R13
-	MOVQ 16(DX), CX
-	MOVQ 24(DX), BX
-	MOVQ 32(DX), SI
-	XORQ DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-	MOVQ  $0, AX
-	ADCXQ AX, SI
-	ADOXQ AX, SI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-	MOVQ  $0, AX
-	ADCXQ AX, SI
-	ADOXQ AX, SI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-	MOVQ  $0, AX
-	ADCXQ AX, SI
-	ADOXQ AX, SI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-	MOVQ  $0, AX
-	ADCXQ AX, SI
-	ADOXQ AX, SI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R13, R14
-	MULXQ ·qElement+8(SB), AX, R13
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R13
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R13
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-	MOVQ  $0, AX
-	ADCXQ AX, SI
-	ADOXQ AX, SI
-
-	// reduce element(R14,R13,CX,BX,SI) using temp registers (DI,R8,R9,R10,R11)
-	REDUCE(R14,R13,CX,BX,SI,DI,R8,R9,R10,R11)
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R13, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	MOVQ SI, 32(AX)
-	RET
-
-noAdx_2:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	CALL ·_fromMontGeneric(SB)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_6w/element_6w_amd64.s b/field/asm/element_6w/element_6w_amd64.s
--- a/field/asm/element_6w/element_6w_amd64.s
+++ b/field/asm/element_6w/element_6w_amd64.s
@@ -1,670 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define REDUCE(ra0, ra1, ra2, ra3, ra4, ra5, rb0, rb1, rb2, rb3, rb4, rb5) \
-	MOVQ    ra0, rb0;              \
-	SUBQ    ·qElement(SB), ra0;    \
-	MOVQ    ra1, rb1;              \
-	SBBQ    ·qElement+8(SB), ra1;  \
-	MOVQ    ra2, rb2;              \
-	SBBQ    ·qElement+16(SB), ra2; \
-	MOVQ    ra3, rb3;              \
-	SBBQ    ·qElement+24(SB), ra3; \
-	MOVQ    ra4, rb4;              \
-	SBBQ    ·qElement+32(SB), ra4; \
-	MOVQ    ra5, rb5;              \
-	SBBQ    ·qElement+40(SB), ra5; \
-	CMOVQCS rb0, ra0;              \
-	CMOVQCS rb1, ra1;              \
-	CMOVQCS rb2, ra2;              \
-	CMOVQCS rb3, ra3;              \
-	CMOVQCS rb4, ra4;              \
-	CMOVQCS rb5, ra5;              \
-
-TEXT ·reduce(SB), NOSPLIT, $0-8
-	MOVQ res+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	RET
-
-// MulBy3(x *Element)
-TEXT ·MulBy3(SB), NOSPLIT, $0-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R15,R9,R10,R11,R12,R13)
-	REDUCE(DX,CX,BX,SI,DI,R8,R15,R9,R10,R11,R12,R13)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	RET
-
-// MulBy5(x *Element)
-TEXT ·MulBy5(SB), NOSPLIT, $0-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R15,R9,R10,R11,R12,R13)
-	REDUCE(DX,CX,BX,SI,DI,R8,R15,R9,R10,R11,R12,R13)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R14,R15,R9,R10,R11,R12)
-	REDUCE(DX,CX,BX,SI,DI,R8,R14,R15,R9,R10,R11,R12)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	RET
-
-// MulBy13(x *Element)
-TEXT ·MulBy13(SB), $40-8
-	MOVQ x+0(FP), AX
-	MOVQ 0(AX), DX
-	MOVQ 8(AX), CX
-	MOVQ 16(AX), BX
-	MOVQ 24(AX), SI
-	MOVQ 32(AX), DI
-	MOVQ 40(AX), R8
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP))
-	REDUCE(DX,CX,BX,SI,DI,R8,R15,s0-8(SP),s1-16(SP),s2-24(SP),s3-32(SP),s4-40(SP))
-
-	MOVQ DX, R15
-	MOVQ CX, s0-8(SP)
-	MOVQ BX, s1-16(SP)
-	MOVQ SI, s2-24(SP)
-	MOVQ DI, s3-32(SP)
-	MOVQ R8, s4-40(SP)
-	ADDQ DX, DX
-	ADCQ CX, CX
-	ADCQ BX, BX
-	ADCQ SI, SI
-	ADCQ DI, DI
-	ADCQ R8, R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	ADDQ R15, DX
-	ADCQ s0-8(SP), CX
-	ADCQ s1-16(SP), BX
-	ADCQ s2-24(SP), SI
-	ADCQ s3-32(SP), DI
-	ADCQ s4-40(SP), R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	ADDQ 0(AX), DX
-	ADCQ 8(AX), CX
-	ADCQ 16(AX), BX
-	ADCQ 24(AX), SI
-	ADCQ 32(AX), DI
-	ADCQ 40(AX), R8
-
-	// reduce element(DX,CX,BX,SI,DI,R8) using temp registers (R9,R10,R11,R12,R13,R14)
-	REDUCE(DX,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14)
-
-	MOVQ DX, 0(AX)
-	MOVQ CX, 8(AX)
-	MOVQ BX, 16(AX)
-	MOVQ SI, 24(AX)
-	MOVQ DI, 32(AX)
-	MOVQ R8, 40(AX)
-	RET
-
-// Butterfly(a, b *Element) sets a = a + b; b = a - b
-TEXT ·Butterfly(SB), $48-16
-	MOVQ    a+0(FP), AX
-	MOVQ    0(AX), CX
-	MOVQ    8(AX), BX
-	MOVQ    16(AX), SI
-	MOVQ    24(AX), DI
-	MOVQ    32(AX), R8
-	MOVQ    40(AX), R9
-	MOVQ    CX, R10
-	MOVQ    BX, R11
-	MOVQ    SI, R12
-	MOVQ    DI, R13
-	MOVQ    R8, R14
-	MOVQ    R9, R15
-	XORQ    AX, AX
-	MOVQ    b+8(FP), DX
-	ADDQ    0(DX), CX
-	ADCQ    8(DX), BX
-	ADCQ    16(DX), SI
-	ADCQ    24(DX), DI
-	ADCQ    32(DX), R8
-	ADCQ    40(DX), R9
-	SUBQ    0(DX), R10
-	SBBQ    8(DX), R11
-	SBBQ    16(DX), R12
-	SBBQ    24(DX), R13
-	SBBQ    32(DX), R14
-	SBBQ    40(DX), R15
-	MOVQ    CX, s0-8(SP)
-	MOVQ    BX, s1-16(SP)
-	MOVQ    SI, s2-24(SP)
-	MOVQ    DI, s3-32(SP)
-	MOVQ    R8, s4-40(SP)
-	MOVQ    R9, s5-48(SP)
-	MOVQ    $const_q0, CX
-	MOVQ    $const_q1, BX
-	MOVQ    $const_q2, SI
-	MOVQ    $const_q3, DI
-	MOVQ    $const_q4, R8
-	MOVQ    $const_q5, R9
-	CMOVQCC AX, CX
-	CMOVQCC AX, BX
-	CMOVQCC AX, SI
-	CMOVQCC AX, DI
-	CMOVQCC AX, R8
-	CMOVQCC AX, R9
-	ADDQ    CX, R10
-	ADCQ    BX, R11
-	ADCQ    SI, R12
-	ADCQ    DI, R13
-	ADCQ    R8, R14
-	ADCQ    R9, R15
-	MOVQ    s0-8(SP), CX
-	MOVQ    s1-16(SP), BX
-	MOVQ    s2-24(SP), SI
-	MOVQ    s3-32(SP), DI
-	MOVQ    s4-40(SP), R8
-	MOVQ    s5-48(SP), R9
-	MOVQ    R10, 0(DX)
-	MOVQ    R11, 8(DX)
-	MOVQ    R12, 16(DX)
-	MOVQ    R13, 24(DX)
-	MOVQ    R14, 32(DX)
-	MOVQ    R15, 40(DX)
-
-	// reduce element(CX,BX,SI,DI,R8,R9) using temp registers (R10,R11,R12,R13,R14,R15)
-	REDUCE(CX,BX,SI,DI,R8,R9,R10,R11,R12,R13,R14,R15)
-
-	MOVQ a+0(FP), AX
-	MOVQ CX, 0(AX)
-	MOVQ BX, 8(AX)
-	MOVQ SI, 16(AX)
-	MOVQ DI, 24(AX)
-	MOVQ R8, 32(AX)
-	MOVQ R9, 40(AX)
-	RET
-
-// mul(res, x, y *Element)
-TEXT ·mul(SB), $24-24
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// See github.com/Consensys/gnark-crypto/field/generator for more comments.
-
-	NO_LOCAL_POINTERS
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_1
-	MOVQ x+8(FP), R8
-
-	// x[0] -> R10
-	// x[1] -> R11
-	// x[2] -> R12
-	MOVQ 0(R8), R10
-	MOVQ 8(R8), R11
-	MOVQ 16(R8), R12
-	MOVQ y+16(FP), R13
-
-	// A -> BP
-	// t[0] -> R14
-	// t[1] -> R15
-	// t[2] -> CX
-	// t[3] -> BX
-	// t[4] -> SI
-	// t[5] -> DI
-#define MACC(in0, in1, in2) \
-	ADCXQ in0, in1     \
-	MULXQ in2, AX, in0 \
-	ADOXQ AX, in1      \
-
-#define DIV_SHIFT() \
-	MOVQ  $const_qInvNeg, DX        \
-	IMULQ R14, DX                   \
-	XORQ  AX, AX                    \
-	MULXQ ·qElement+0(SB), AX, R9   \
-	ADCXQ R14, AX                   \
-	MOVQ  R9, R14                   \
-	MACC(R15, R14, ·qElement+8(SB)) \
-	MACC(CX, R15, ·qElement+16(SB)) \
-	MACC(BX, CX, ·qElement+24(SB))  \
-	MACC(SI, BX, ·qElement+32(SB))  \
-	MACC(DI, SI, ·qElement+40(SB))  \
-	MOVQ  $0, AX                    \
-	ADCXQ AX, DI                    \
-	ADOXQ BP, DI                    \
-
-#define MUL_WORD_0() \
-	XORQ  AX, AX         \
-	MULXQ R10, R14, R15  \
-	MULXQ R11, AX, CX    \
-	ADOXQ AX, R15        \
-	MULXQ R12, AX, BX    \
-	ADOXQ AX, CX         \
-	MULXQ 24(R8), AX, SI \
-	ADOXQ AX, BX         \
-	MULXQ 32(R8), AX, DI \
-	ADOXQ AX, SI         \
-	MULXQ 40(R8), AX, BP \
-	ADOXQ AX, DI         \
-	MOVQ  $0, AX         \
-	ADOXQ AX, BP         \
-	DIV_SHIFT()          \
-
-#define MUL_WORD_N() \
-	XORQ  AX, AX         \
-	MULXQ R10, AX, BP    \
-	ADOXQ AX, R14        \
-	MACC(BP, R15, R11)   \
-	MACC(BP, CX, R12)    \
-	MACC(BP, BX, 24(R8)) \
-	MACC(BP, SI, 32(R8)) \
-	MACC(BP, DI, 40(R8)) \
-	MOVQ  $0, AX         \
-	ADCXQ AX, BP         \
-	ADOXQ AX, BP         \
-	DIV_SHIFT()          \
-
-	// mul body
-	MOVQ 0(R13), DX
-	MUL_WORD_0()
-	MOVQ 8(R13), DX
-	MUL_WORD_N()
-	MOVQ 16(R13), DX
-	MUL_WORD_N()
-	MOVQ 24(R13), DX
-	MUL_WORD_N()
-	MOVQ 32(R13), DX
-	MUL_WORD_N()
-	MOVQ 40(R13), DX
-	MUL_WORD_N()
-
-	// reduce element(R14,R15,CX,BX,SI,DI) using temp registers (R9,R8,R13,R10,R11,R12)
-	REDUCE(R14,R15,CX,BX,SI,DI,R9,R8,R13,R10,R11,R12)
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R15, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	MOVQ SI, 32(AX)
-	MOVQ DI, 40(AX)
-	RET
-
-noAdx_1:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	MOVQ x+8(FP), AX
-	MOVQ AX, 8(SP)
-	MOVQ y+16(FP), AX
-	MOVQ AX, 16(SP)
-	CALL ·_mulGeneric(SB)
-	RET
-
-TEXT ·fromMont(SB), $8-8
-	NO_LOCAL_POINTERS
-
-	// Algorithm 2 of "Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS"
-	// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-	// when y = 1 we have:
-	// for i=0 to N-1
-	// 		t[i] = x[i]
-	// for i=0 to N-1
-	// 		m := t[0]*q'[0] mod W
-	// 		C,_ := t[0] + m*q[0]
-	// 		for j=1 to N-1
-	// 		    (C,t[j-1]) := t[j] + m*q[j] + C
-	// 		t[N-1] = C
-	CMPB ·supportAdx(SB), $1
-	JNE  noAdx_2
-	MOVQ res+0(FP), DX
-	MOVQ 0(DX), R14
-	MOVQ 8(DX), R15
-	MOVQ 16(DX), CX
-	MOVQ 24(DX), BX
-	MOVQ 32(DX), SI
-	MOVQ 40(DX), DI
-	XORQ DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-	MOVQ  $0, AX
-	ADCXQ AX, DI
-	ADOXQ AX, DI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-	MOVQ  $0, AX
-	ADCXQ AX, DI
-	ADOXQ AX, DI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-	MOVQ  $0, AX
-	ADCXQ AX, DI
-	ADOXQ AX, DI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-	MOVQ  $0, AX
-	ADCXQ AX, DI
-	ADOXQ AX, DI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-	MOVQ  $0, AX
-	ADCXQ AX, DI
-	ADOXQ AX, DI
-	XORQ  DX, DX
-
-	// m := t[0]*q'[0] mod W
-	MOVQ  $const_qInvNeg, DX
-	IMULQ R14, DX
-	XORQ  AX, AX
-
-	// C,_ := t[0] + m*q[0]
-	MULXQ ·qElement+0(SB), AX, BP
-	ADCXQ R14, AX
-	MOVQ  BP, R14
-
-	// (C,t[0]) := t[1] + m*q[1] + C
-	ADCXQ R15, R14
-	MULXQ ·qElement+8(SB), AX, R15
-	ADOXQ AX, R14
-
-	// (C,t[1]) := t[2] + m*q[2] + C
-	ADCXQ CX, R15
-	MULXQ ·qElement+16(SB), AX, CX
-	ADOXQ AX, R15
-
-	// (C,t[2]) := t[3] + m*q[3] + C
-	ADCXQ BX, CX
-	MULXQ ·qElement+24(SB), AX, BX
-	ADOXQ AX, CX
-
-	// (C,t[3]) := t[4] + m*q[4] + C
-	ADCXQ SI, BX
-	MULXQ ·qElement+32(SB), AX, SI
-	ADOXQ AX, BX
-
-	// (C,t[4]) := t[5] + m*q[5] + C
-	ADCXQ DI, SI
-	MULXQ ·qElement+40(SB), AX, DI
-	ADOXQ AX, SI
-	MOVQ  $0, AX
-	ADCXQ AX, DI
-	ADOXQ AX, DI
-
-	// reduce element(R14,R15,CX,BX,SI,DI) using temp registers (R8,R9,R10,R11,R12,R13)
-	REDUCE(R14,R15,CX,BX,SI,DI,R8,R9,R10,R11,R12,R13)
-
-	MOVQ res+0(FP), AX
-	MOVQ R14, 0(AX)
-	MOVQ R15, 8(AX)
-	MOVQ CX, 16(AX)
-	MOVQ BX, 24(AX)
-	MOVQ SI, 32(AX)
-	MOVQ DI, 40(AX)
-	RET
-
-noAdx_2:
-	MOVQ res+0(FP), AX
-	MOVQ AX, (SP)
-	CALL ·_fromMontGeneric(SB)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/asm/element_6w/element_6w_arm64.s b/field/asm/element_6w/element_6w_arm64.s
--- a/field/asm/element_6w/element_6w_arm64.s
+++ b/field/asm/element_6w/element_6w_arm64.s
@@ -1,220 +1,1 @@
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-// butterfly(a, b *Element)
-// a, b = a+b, a-b
-TEXT ·Butterfly(SB), NOFRAME|NOSPLIT, $0-16
-	LDP  x+0(FP), (R25, R26)
-	LDP  0(R25), (R0, R1)
-	LDP  16(R25), (R2, R3)
-	LDP  32(R25), (R4, R5)
-	LDP  0(R26), (R6, R7)
-	LDP  16(R26), (R8, R9)
-	LDP  32(R26), (R10, R11)
-	ADDS R0, R6, R12
-	ADCS R1, R7, R13
-	ADCS R2, R8, R14
-	ADCS R3, R9, R15
-	ADCS R4, R10, R16
-	ADC  R5, R11, R17
-	SUBS R6, R0, R6
-	SBCS R7, R1, R7
-	SBCS R8, R2, R8
-	SBCS R9, R3, R9
-	SBCS R10, R4, R10
-	SBCS R11, R5, R11
-	LDP  ·qElement+0(SB), (R0, R1)
-	CSEL CS, ZR, R0, R19
-	CSEL CS, ZR, R1, R20
-	LDP  ·qElement+16(SB), (R2, R3)
-	CSEL CS, ZR, R2, R21
-	CSEL CS, ZR, R3, R22
-	LDP  ·qElement+32(SB), (R4, R5)
-	CSEL CS, ZR, R4, R23
-	CSEL CS, ZR, R5, R24
-
-	// add q if underflow, 0 if not
-	ADDS R6, R19, R6
-	ADCS R7, R20, R7
-	STP  (R6, R7), 0(R26)
-	ADCS R8, R21, R8
-	ADCS R9, R22, R9
-	STP  (R8, R9), 16(R26)
-	ADCS R10, R23, R10
-	ADC  R11, R24, R11
-	STP  (R10, R11), 32(R26)
-
-	// q = t - q
-	SUBS R0, R12, R0
-	SBCS R1, R13, R1
-	SBCS R2, R14, R2
-	SBCS R3, R15, R3
-	SBCS R4, R16, R4
-	SBCS R5, R17, R5
-
-	// if no borrow, return q, else return t
-	CSEL CS, R0, R12, R12
-	CSEL CS, R1, R13, R13
-	STP  (R12, R13), 0(R25)
-	CSEL CS, R2, R14, R14
-	CSEL CS, R3, R15, R15
-	STP  (R14, R15), 16(R25)
-	CSEL CS, R4, R16, R16
-	CSEL CS, R5, R17, R17
-	STP  (R16, R17), 32(R25)
-	RET
-
-// mul(res, x, y *Element)
-// Algorithm 2 of Faster Montgomery Multiplication and Multi-Scalar-Multiplication for SNARKS
-// by Y. El Housni and G. Botrel https://doi.org/10.46586/tches.v2023.i3.504-521
-TEXT ·mul(SB), NOFRAME|NOSPLIT, $0-24
-#define DIVSHIFT() \
-	MUL   R17, R16, R0 \
-	ADDS  R0, R8, R8   \
-	MUL   R19, R16, R0 \
-	ADCS  R0, R9, R9   \
-	MUL   R20, R16, R0 \
-	ADCS  R0, R10, R10 \
-	MUL   R21, R16, R0 \
-	ADCS  R0, R11, R11 \
-	MUL   R22, R16, R0 \
-	ADCS  R0, R12, R12 \
-	MUL   R23, R16, R0 \
-	ADCS  R0, R13, R13 \
-	ADC   R14, ZR, R14 \
-	UMULH R17, R16, R0 \
-	ADDS  R0, R9, R8   \
-	UMULH R19, R16, R0 \
-	ADCS  R0, R10, R9  \
-	UMULH R20, R16, R0 \
-	ADCS  R0, R11, R10 \
-	UMULH R21, R16, R0 \
-	ADCS  R0, R12, R11 \
-	UMULH R22, R16, R0 \
-	ADCS  R0, R13, R12 \
-	UMULH R23, R16, R0 \
-	ADCS  R0, R14, R13 \
-
-#define MUL_WORD_N() \
-	MUL   R2, R1, R0   \
-	ADDS  R0, R8, R8   \
-	MUL   R8, R15, R16 \
-	MUL   R3, R1, R0   \
-	ADCS  R0, R9, R9   \
-	MUL   R4, R1, R0   \
-	ADCS  R0, R10, R10 \
-	MUL   R5, R1, R0   \
-	ADCS  R0, R11, R11 \
-	MUL   R6, R1, R0   \
-	ADCS  R0, R12, R12 \
-	MUL   R7, R1, R0   \
-	ADCS  R0, R13, R13 \
-	ADC   ZR, ZR, R14  \
-	UMULH R2, R1, R0   \
-	ADDS  R0, R9, R9   \
-	UMULH R3, R1, R0   \
-	ADCS  R0, R10, R10 \
-	UMULH R4, R1, R0   \
-	ADCS  R0, R11, R11 \
-	UMULH R5, R1, R0   \
-	ADCS  R0, R12, R12 \
-	UMULH R6, R1, R0   \
-	ADCS  R0, R13, R13 \
-	UMULH R7, R1, R0   \
-	ADC   R0, R14, R14 \
-	DIVSHIFT()         \
-
-#define MUL_WORD_0() \
-	MUL   R2, R1, R8   \
-	MUL   R3, R1, R9   \
-	MUL   R4, R1, R10  \
-	MUL   R5, R1, R11  \
-	MUL   R6, R1, R12  \
-	MUL   R7, R1, R13  \
-	UMULH R2, R1, R0   \
-	ADDS  R0, R9, R9   \
-	UMULH R3, R1, R0   \
-	ADCS  R0, R10, R10 \
-	UMULH R4, R1, R0   \
-	ADCS  R0, R11, R11 \
-	UMULH R5, R1, R0   \
-	ADCS  R0, R12, R12 \
-	UMULH R6, R1, R0   \
-	ADCS  R0, R13, R13 \
-	UMULH R7, R1, R0   \
-	ADC   R0, ZR, R14  \
-	MUL   R8, R15, R16 \
-	DIVSHIFT()         \
-
-	MOVD y+16(FP), R24
-	MOVD x+8(FP), R0
-	LDP  0(R0), (R2, R3)
-	LDP  16(R0), (R4, R5)
-	LDP  32(R0), (R6, R7)
-	MOVD 0(R24), R1
-	MOVD $const_qInvNeg, R15
-	LDP  ·qElement+0(SB), (R17, R19)
-	LDP  ·qElement+16(SB), (R20, R21)
-	LDP  ·qElement+32(SB), (R22, R23)
-	MUL_WORD_0()
-	MOVD 8(R24), R1
-	MUL_WORD_N()
-	MOVD 16(R24), R1
-	MUL_WORD_N()
-	MOVD 24(R24), R1
-	MUL_WORD_N()
-	MOVD 32(R24), R1
-	MUL_WORD_N()
-	MOVD 40(R24), R1
-	MUL_WORD_N()
-
-	// reduce if necessary
-	SUBS R17, R8, R17
-	SBCS R19, R9, R19
-	SBCS R20, R10, R20
-	SBCS R21, R11, R21
-	SBCS R22, R12, R22
-	SBCS R23, R13, R23
-	MOVD res+0(FP), R0
-	CSEL CS, R17, R8, R8
-	CSEL CS, R19, R9, R9
-	STP  (R8, R9), 0(R0)
-	CSEL CS, R20, R10, R10
-	CSEL CS, R21, R11, R11
-	STP  (R10, R11), 16(R0)
-	CSEL CS, R22, R12, R12
-	CSEL CS, R23, R13, R13
-	STP  (R12, R13), 32(R0)
-	RET
-
-// reduce(res *Element)
-TEXT ·reduce(SB), NOFRAME|NOSPLIT, $0-8
-	LDP  ·qElement+0(SB), (R6, R7)
-	LDP  ·qElement+16(SB), (R8, R9)
-	LDP  ·qElement+32(SB), (R10, R11)
-	MOVD res+0(FP), R12
-	LDP  0(R12), (R0, R1)
-	LDP  16(R12), (R2, R3)
-	LDP  32(R12), (R4, R5)
-
-	// q = t - q
-	SUBS R6, R0, R6
-	SBCS R7, R1, R7
-	SBCS R8, R2, R8
-	SBCS R9, R3, R9
-	SBCS R10, R4, R10
-	SBCS R11, R5, R11
-
-	// if no borrow, return q, else return t
-	CSEL CS, R6, R0, R0
-	CSEL CS, R7, R1, R1
-	STP  (R0, R1), 0(R12)
-	CSEL CS, R8, R2, R2
-	CSEL CS, R9, R3, R3
-	STP  (R2, R3), 16(R12)
-	CSEL CS, R10, R4, R4
-	CSEL CS, R11, R5, R5
-	STP  (R4, R5), 32(R12)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/babybear/element_amd64.s b/field/babybear/element_amd64.s
--- a/field/babybear/element_amd64.s
+++ b/field/babybear/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14212064141731379453
-#include "../asm/element_31b/element_31b_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/field/babybear/element_arm64.s b/field/babybear/element_arm64.s
--- a/field/babybear/element_arm64.s
+++ b/field/babybear/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 8620676634583589757
-#include "../asm/element_31b/element_31b_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/field/babybear/extensions/e4_amd64.s b/field/babybear/extensions/e4_amd64.s
--- a/field/babybear/extensions/e4_amd64.s
+++ b/field/babybear/extensions/e4_amd64.s
@@ -1,59 +1,1 @@
-//go:build !purego
-
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-// Refer to the generator for more documentation.
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-TEXT ·mulAccE4_avx512(SB), NOSPLIT, $0-32
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z0
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z1
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         alpha+0(FP), R14
-	MOVQ         scale+8(FP), R15
-	MOVQ         res+16(FP), CX
-	MOVQ         N+24(FP), BX
-	VMOVDQU32    0(R14), X2
-	VINSERTI64X2 $1, X2, Y2, Y2
-	VINSERTI64X4 $1, Y2, Z2, Z2
-	VPSRLQ       $32, Z2, Z3
-	SHRQ         $2, BX
-
-loop_1:
-	TESTQ        BX, BX
-	JEQ          done_2
-	DECQ         BX
-	VMOVDQU32    0(CX), Z4
-	VPBROADCASTD 0(R15), X5
-	VPBROADCASTD 4(R15), X6
-	VPBROADCASTD 8(R15), X7
-	VPBROADCASTD 12(R15), X8
-	VINSERTI64X2 $1, X6, Y5, Y5
-	VINSERTI64X2 $1, X8, Y7, Y7
-	VINSERTI64X4 $1, Y7, Z5, Z5
-	VPMULUDQ     Z5, Z2, Z10
-	VPMULUDQ     Z5, Z3, Z11
-	VPMULUDQ     Z10, Z1, Z12
-	VPMULUDQ     Z11, Z1, Z13
-	VPMULUDQ     Z12, Z0, Z12
-	VPADDQ       Z10, Z12, Z10
-	VPMULUDQ     Z13, Z0, Z13
-	VPADDQ       Z11, Z13, Z11
-	VMOVSHDUP    Z10, K3, Z11
-	VPSUBD       Z0, Z11, Z12
-	VPMINUD      Z11, Z12, Z9
-	VPADDD       Z4, Z9, Z4     // result = result + acc
-	VPSUBD       Z0, Z4, Z9
-	VPMINUD      Z4, Z9, Z4
-	VMOVDQU32    Z4, 0(CX)
-	ADDQ         $64, CX
-	ADDQ         $16, R15
-	JMP          loop_1
-
-done_2:
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/babybear/fft/kernel_amd64.s b/field/babybear/fft/kernel_amd64.s
--- a/field/babybear/fft/kernel_amd64.s
+++ b/field/babybear/fft/kernel_amd64.s
@@ -1,692 +1,1 @@
-//go:build !purego
-
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-// Refer to the generator for more documentation.
-// Some sub-functions are derived from Plonky3:
-// https://github.com/Plonky3/Plonky3/blob/36e619f3c6526ee86e2e5639a24b3224e1c1700f/monty-31/src/x86_64_avx512/packing.rs#L319
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define BUTTERFLYD1Q(in0, in1, in2, in3, in4) \
-	VPADDD  in0, in1, in3 \
-	VPSUBD  in1, in0, in1 \
-	VPSUBD  in2, in3, in0 \
-	VPMINUD in3, in0, in0 \
-	VPADDD  in2, in1, in4 \
-	VPMINUD in4, in1, in1 \
-
-#define BUTTERFLYD2Q(in0, in1, in2, in3, in4) \
-	VPSUBD  in1, in0, in4 \
-	VPADDD  in0, in1, in3 \
-	VPADDD  in2, in4, in1 \
-	VPSUBD  in2, in3, in0 \
-	VPMINUD in3, in0, in0 \
-
-#define BUTTERFLYD2Q2Q(in0, in1, in2, in3) \
-	VPSUBD in1, in0, in3 \
-	VPADDD in0, in1, in0 \
-	VPADDD in2, in3, in1 \
-
-#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9) \
-	VPSRLQ    $32, in0, in2 \
-	VPSRLQ    $32, in1, in3 \
-	VPMULUDQ  in0, in1, in4 \
-	VPMULUDQ  in2, in3, in5 \
-	VPMULUDQ  in4, in9, in6 \
-	VPMULUDQ  in5, in9, in7 \
-	VPMULUDQ  in6, in8, in6 \
-	VPADDQ    in4, in6, in4 \
-	VPMULUDQ  in7, in8, in7 \
-	VPADDQ    in5, in7, in5 \
-	VMOVSHDUP in4, K3, in5  \
-	VPSUBD    in8, in5, in7 \
-	VPMINUD   in5, in7, in0 \
-
-#define PERMUTE8X8(in0, in1, in2) \
-	VSHUFI64X2 $0x000000000000004e, in1, in0, in2 \
-	VPBLENDMQ  in0, in2, K1, in0                  \
-	VPBLENDMQ  in2, in1, K1, in1                  \
-
-#define PERMUTE4X4(in0, in1, in2, in3) \
-	VMOVDQA64 in2, in3          \
-	VPERMI2Q  in1, in0, in3     \
-	VPBLENDMQ in0, in3, K2, in0 \
-	VPBLENDMQ in3, in1, K2, in1 \
-
-#define PERMUTE2X2(in0, in1, in2) \
-	VSHUFPD   $0x0000000000000055, in1, in0, in2 \
-	VPBLENDMQ in0, in2, K3, in0                  \
-	VPBLENDMQ in2, in1, K3, in1                  \
-
-#define PERMUTE1X1(in0, in1, in2) \
-	VPSHRDQ   $32, in1, in0, in2 \
-	VPBLENDMD in0, in2, K3, in0  \
-	VPBLENDMD in2, in1, K3, in1  \
-
-#define LOAD_Q(in0, in1) \
-	MOVD         $const_q, AX       \
-	VPBROADCASTD AX, in0            \
-	MOVD         $const_qInvNeg, AX \
-	VPBROADCASTD AX, in1            \
-
-#define LOAD_MASKS() \
-	MOVQ  $0x0000000000000f0f, AX \
-	KMOVQ AX, K1                  \
-	MOVQ  $0x0000000000000033, AX \
-	KMOVQ AX, K2                  \
-	MOVQ  $0x0000000000005555, AX \
-	KMOVD AX, K3                  \
-
-#define BUTTERFLY_MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11, in12, in13, in14) \
-BUTTERFLYD2Q(in0, in1, in2, in3, in4)                       \
-MULD(in5, in6, in7, in8, in9, in10, in11, in12, in13, in14) \
-
-TEXT ·innerDITWithTwiddles_avx512(SB), NOSPLIT, $0-40
-	// refer to the code generator for comments and documentation.
-	LOAD_Q(Z4, Z5)
-	LOAD_MASKS()
-	MOVQ a+0(FP), R15
-	MOVQ twiddles+8(FP), DX
-	MOVQ end+24(FP), CX
-	MOVQ m+32(FP), BX
-	SHRQ $4, CX             // we are processing 16 elements at a time
-	SHLQ $2, BX             // offset = m * 4bytes
-	MOVQ R15, SI
-	ADDQ BX, SI
-
-loop_1:
-	TESTQ     CX, CX
-	JEQ       done_2
-	DECQ      CX
-	VMOVDQU32 0(R15), Z0 // load a[i]
-	VMOVDQU32 0(SI), Z1  // load a[i+m]
-	VMOVDQU32 0(DX), Z6  // load twiddles[i]
-	MULD(Z1, Z6, Z7, Z8, Z2, Z3, Z9, Z10, Z4, Z5)
-	BUTTERFLYD1Q(Z0, Z1, Z4, Z2, Z3)
-	VMOVDQU32 Z0, 0(R15) // store a[i]
-	VMOVDQU32 Z1, 0(SI)  // store a[i+m]
-	ADDQ      $64, R15
-	ADDQ      $64, SI
-	ADDQ      $64, DX
-	JMP       loop_1
-
-done_2:
-	RET
-
-TEXT ·innerDIFWithTwiddles_avx512(SB), NOSPLIT, $0-40
-	// refer to the code generator for comments and documentation.
-	MOVQ a+0(FP), R15
-	MOVQ twiddles+8(FP), DX
-	MOVQ end+24(FP), CX
-	MOVQ m+32(FP), BX
-	LOAD_Q(Z2, Z4)
-	LOAD_MASKS()
-	SHLQ $2, BX             // offset = m * 4bytes
-	MOVQ R15, SI
-	ADDQ BX, SI
-	SHRQ $4, CX             // we are processing 16 elements at a time
-
-loop_3:
-	TESTQ     CX, CX
-	JEQ       done_4
-	DECQ      CX
-	VMOVDQU32 0(R15), Z0 // load a[i]
-	VMOVDQU32 0(SI), Z1  // load a[i+m]
-	VMOVDQU32 0(DX), Z5  // load twiddles[i]
-	BUTTERFLY_MULD(Z0, Z1, Z2, Z3, Z8, Z1, Z5, Z6, Z7, Z3, Z8, Z9, Z10, Z2, Z4)
-	VMOVDQU32 Z0, 0(R15) // store a[i]
-	VMOVDQU32 Z1, 0(SI)
-	ADDQ      $64, R15
-	ADDQ      $64, SI
-	ADDQ      $64, DX
-	JMP       loop_3
-
-done_4:
-	RET
-
-TEXT ·kerDIFNP_256_avx512(SB), NOSPLIT, $0-56
-	// refer to the code generator for comments and documentation.
-	LOAD_Q(Z16, Z17)
-	LOAD_MASKS()
-
-	// load arguments
-	MOVQ         a+0(FP), R15
-	MOVQ         twiddles+24(FP), CX
-	MOVQ         stage+48(FP), AX
-	IMULQ        $24, AX
-	ADDQ         AX, CX                             // we want twiddles[stage] as starting point
-	VMOVDQU32    0(R15), Z0                         // load a[0]
-	VMOVDQU32    64(R15), Z1                        // load a[1]
-	VMOVDQU32    128(R15), Z2                       // load a[2]
-	VMOVDQU32    192(R15), Z3                       // load a[3]
-	VMOVDQU32    256(R15), Z4                       // load a[4]
-	VMOVDQU32    320(R15), Z5                       // load a[5]
-	VMOVDQU32    384(R15), Z6                       // load a[6]
-	VMOVDQU32    448(R15), Z7                       // load a[7]
-	VMOVDQU32    512(R15), Z8                       // load a[8]
-	VMOVDQU32    576(R15), Z9                       // load a[9]
-	VMOVDQU32    640(R15), Z10                      // load a[10]
-	VMOVDQU32    704(R15), Z11                      // load a[11]
-	VMOVDQU32    768(R15), Z12                      // load a[12]
-	VMOVDQU32    832(R15), Z13                      // load a[13]
-	VMOVDQU32    896(R15), Z14                      // load a[14]
-	VMOVDQU32    960(R15), Z15                      // load a[15]
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z18
-	VMOVDQU32    64(DI), Z19
-	VMOVDQU32    128(DI), Z20
-	VMOVDQU32    192(DI), Z21
-	VMOVDQU32    256(DI), Z22
-	VMOVDQU32    320(DI), Z23
-	VMOVDQU32    384(DI), Z24
-	VMOVDQU32    448(DI), Z25
-	BUTTERFLYD2Q(Z0, Z8, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z1, Z9, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z2, Z10, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z3, Z11, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z4, Z12, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z5, Z13, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z6, Z14, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z7, Z15, Z16, Z31, Z27)
-	MULD(Z8, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z9, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z10, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z12, Z22, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z23, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z14, Z24, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z25, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	ADDQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z18
-	VMOVDQU32    64(DI), Z19
-	VMOVDQU32    128(DI), Z20
-	VMOVDQU32    192(DI), Z21
-	BUTTERFLYD2Q(Z0, Z4, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z1, Z5, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z2, Z6, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z3, Z7, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z8, Z12, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z9, Z13, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z10, Z14, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z11, Z15, Z16, Z31, Z27)
-	MULD(Z4, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z5, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z6, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z12, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z14, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	ADDQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z18
-	VMOVDQU32    64(DI), Z19
-	BUTTERFLYD2Q(Z0, Z2, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z1, Z3, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z4, Z6, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z5, Z7, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z8, Z10, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z9, Z11, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z12, Z14, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z13, Z15, Z16, Z31, Z27)
-	MULD(Z2, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z3, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z6, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z10, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z14, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	ADDQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z18
-	BUTTERFLYD2Q(Z0, Z1, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z2, Z3, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z4, Z5, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z6, Z7, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z8, Z9, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z10, Z11, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z12, Z13, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z14, Z15, Z16, Z31, Z27)
-	MULD(Z1, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z3, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z5, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z9, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	ADDQ         $24, CX
-	MOVQ         ·vInterleaveIndices+0(SB), R8
-	VMOVDQU64    0(R8), Z22
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Y18
-	VINSERTI64X4 $1, Y18, Z18, Z18
-	MOVQ         24(CX), DI
-	VMOVDQU32    0(DI), X19
-	VINSERTI64X2 $1, X19, Z19, Z19
-	VINSERTI64X2 $0x0000000000000002, X19, Z19, Z19
-	VINSERTI64X2 $0x0000000000000003, X19, Z19, Z19
-	MOVQ         48(CX), DI
-	VPBROADCASTD 0(DI), Z20
-	VPBROADCASTD 4(DI), Z21
-	VPBLENDMD    Z20, Z21, K3, Z20
-	PERMUTE8X8(Z0, Z1, Z26)
-	BUTTERFLYD2Q(Z0, Z1, Z16, Z31, Z27)
-	PERMUTE8X8(Z2, Z3, Z26)
-	BUTTERFLYD2Q(Z2, Z3, Z16, Z31, Z27)
-	PERMUTE8X8(Z4, Z5, Z26)
-	BUTTERFLYD2Q(Z4, Z5, Z16, Z31, Z27)
-	PERMUTE8X8(Z6, Z7, Z26)
-	BUTTERFLYD2Q(Z6, Z7, Z16, Z31, Z27)
-	PERMUTE8X8(Z8, Z9, Z26)
-	BUTTERFLYD2Q(Z8, Z9, Z16, Z31, Z27)
-	PERMUTE8X8(Z10, Z11, Z26)
-	BUTTERFLYD2Q(Z10, Z11, Z16, Z31, Z27)
-	PERMUTE8X8(Z12, Z13, Z26)
-	BUTTERFLYD2Q(Z12, Z13, Z16, Z31, Z27)
-	PERMUTE8X8(Z14, Z15, Z26)
-	BUTTERFLYD2Q(Z14, Z15, Z16, Z31, Z27)
-	MULD(Z1, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z3, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z5, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z9, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE4X4(Z0, Z1, Z22, Z26)
-	BUTTERFLYD2Q(Z0, Z1, Z16, Z31, Z27)
-	PERMUTE4X4(Z2, Z3, Z22, Z26)
-	BUTTERFLYD2Q(Z2, Z3, Z16, Z31, Z27)
-	PERMUTE4X4(Z4, Z5, Z22, Z26)
-	BUTTERFLYD2Q(Z4, Z5, Z16, Z31, Z27)
-	PERMUTE4X4(Z6, Z7, Z22, Z26)
-	BUTTERFLYD2Q(Z6, Z7, Z16, Z31, Z27)
-	PERMUTE4X4(Z8, Z9, Z22, Z26)
-	BUTTERFLYD2Q(Z8, Z9, Z16, Z31, Z27)
-	PERMUTE4X4(Z10, Z11, Z22, Z26)
-	BUTTERFLYD2Q(Z10, Z11, Z16, Z31, Z27)
-	PERMUTE4X4(Z12, Z13, Z22, Z26)
-	BUTTERFLYD2Q(Z12, Z13, Z16, Z31, Z27)
-	PERMUTE4X4(Z14, Z15, Z22, Z26)
-	BUTTERFLYD2Q(Z14, Z15, Z16, Z31, Z27)
-	MULD(Z1, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z3, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z5, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z9, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE2X2(Z0, Z1, Z26)
-	BUTTERFLYD2Q(Z0, Z1, Z16, Z31, Z27)
-	PERMUTE2X2(Z2, Z3, Z26)
-	BUTTERFLYD2Q(Z2, Z3, Z16, Z31, Z27)
-	PERMUTE2X2(Z4, Z5, Z26)
-	BUTTERFLYD2Q(Z4, Z5, Z16, Z31, Z27)
-	PERMUTE2X2(Z6, Z7, Z26)
-	BUTTERFLYD2Q(Z6, Z7, Z16, Z31, Z27)
-	PERMUTE2X2(Z8, Z9, Z26)
-	BUTTERFLYD2Q(Z8, Z9, Z16, Z31, Z27)
-	PERMUTE2X2(Z10, Z11, Z26)
-	BUTTERFLYD2Q(Z10, Z11, Z16, Z31, Z27)
-	PERMUTE2X2(Z12, Z13, Z26)
-	BUTTERFLYD2Q(Z12, Z13, Z16, Z31, Z27)
-	PERMUTE2X2(Z14, Z15, Z26)
-	BUTTERFLYD2Q(Z14, Z15, Z16, Z31, Z27)
-	MULD(Z1, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z0, Z1, Z26)
-	MULD(Z3, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z2, Z3, Z26)
-	MULD(Z5, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z4, Z5, Z26)
-	MULD(Z7, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z6, Z7, Z26)
-	MULD(Z9, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z8, Z9, Z26)
-	MULD(Z11, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z10, Z11, Z26)
-	MULD(Z13, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z12, Z13, Z26)
-	MULD(Z15, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z14, Z15, Z26)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z26, Z27)
-	VPUNPCKLDQ   Z1, Z0, Z23
-	VPUNPCKHDQ   Z1, Z0, Z1
-	VMOVDQA32    Z23, Z0
-	PERMUTE4X4(Z0, Z1, Z22, Z23)
-	PERMUTE8X8(Z0, Z1, Z23)
-	VMOVDQU32    Z0, 0(R15)
-	VMOVDQU32    Z1, 64(R15)
-	VPUNPCKLDQ   Z3, Z2, Z23
-	VPUNPCKHDQ   Z3, Z2, Z3
-	VMOVDQA32    Z23, Z2
-	PERMUTE4X4(Z2, Z3, Z22, Z23)
-	PERMUTE8X8(Z2, Z3, Z23)
-	VMOVDQU32    Z2, 128(R15)
-	VMOVDQU32    Z3, 192(R15)
-	VPUNPCKLDQ   Z5, Z4, Z23
-	VPUNPCKHDQ   Z5, Z4, Z5
-	VMOVDQA32    Z23, Z4
-	PERMUTE4X4(Z4, Z5, Z22, Z23)
-	PERMUTE8X8(Z4, Z5, Z23)
-	VMOVDQU32    Z4, 256(R15)
-	VMOVDQU32    Z5, 320(R15)
-	VPUNPCKLDQ   Z7, Z6, Z23
-	VPUNPCKHDQ   Z7, Z6, Z7
-	VMOVDQA32    Z23, Z6
-	PERMUTE4X4(Z6, Z7, Z22, Z23)
-	PERMUTE8X8(Z6, Z7, Z23)
-	VMOVDQU32    Z6, 384(R15)
-	VMOVDQU32    Z7, 448(R15)
-	VPUNPCKLDQ   Z9, Z8, Z23
-	VPUNPCKHDQ   Z9, Z8, Z9
-	VMOVDQA32    Z23, Z8
-	PERMUTE4X4(Z8, Z9, Z22, Z23)
-	PERMUTE8X8(Z8, Z9, Z23)
-	VMOVDQU32    Z8, 512(R15)
-	VMOVDQU32    Z9, 576(R15)
-	VPUNPCKLDQ   Z11, Z10, Z23
-	VPUNPCKHDQ   Z11, Z10, Z11
-	VMOVDQA32    Z23, Z10
-	PERMUTE4X4(Z10, Z11, Z22, Z23)
-	PERMUTE8X8(Z10, Z11, Z23)
-	VMOVDQU32    Z10, 640(R15)
-	VMOVDQU32    Z11, 704(R15)
-	VPUNPCKLDQ   Z13, Z12, Z23
-	VPUNPCKHDQ   Z13, Z12, Z13
-	VMOVDQA32    Z23, Z12
-	PERMUTE4X4(Z12, Z13, Z22, Z23)
-	PERMUTE8X8(Z12, Z13, Z23)
-	VMOVDQU32    Z12, 768(R15)
-	VMOVDQU32    Z13, 832(R15)
-	VPUNPCKLDQ   Z15, Z14, Z23
-	VPUNPCKHDQ   Z15, Z14, Z15
-	VMOVDQA32    Z23, Z14
-	PERMUTE4X4(Z14, Z15, Z22, Z23)
-	PERMUTE8X8(Z14, Z15, Z23)
-	VMOVDQU32    Z14, 896(R15)
-	VMOVDQU32    Z15, 960(R15)
-	RET
-
-TEXT ·kerDITNP_256_avx512(SB), NOSPLIT, $0-56
-	// refer to the code generator for comments and documentation.
-	LOAD_Q(Z16, Z17)
-	LOAD_MASKS()
-
-	// load arguments
-	MOVQ         a+0(FP), R15
-	MOVQ         twiddles+24(FP), CX
-	MOVQ         stage+48(FP), AX
-	IMULQ        $24, AX
-	ADDQ         AX, CX                             // we want twiddles[stage] as starting point
-	VMOVDQU32    0(R15), Z0                         // load a[0]
-	VMOVDQU32    64(R15), Z1                        // load a[1]
-	VMOVDQU32    128(R15), Z2                       // load a[2]
-	VMOVDQU32    192(R15), Z3                       // load a[3]
-	VMOVDQU32    256(R15), Z4                       // load a[4]
-	VMOVDQU32    320(R15), Z5                       // load a[5]
-	VMOVDQU32    384(R15), Z6                       // load a[6]
-	VMOVDQU32    448(R15), Z7                       // load a[7]
-	VMOVDQU32    512(R15), Z8                       // load a[8]
-	VMOVDQU32    576(R15), Z9                       // load a[9]
-	VMOVDQU32    640(R15), Z10                      // load a[10]
-	VMOVDQU32    704(R15), Z11                      // load a[11]
-	VMOVDQU32    768(R15), Z12                      // load a[12]
-	VMOVDQU32    832(R15), Z13                      // load a[13]
-	VMOVDQU32    896(R15), Z14                      // load a[14]
-	VMOVDQU32    960(R15), Z15                      // load a[15]
-	MOVQ         ·vInterleaveIndices+0(SB), R8
-	VMOVDQU64    0(R8), Z28
-	PERMUTE1X1(Z0, Z1, Z22)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	PERMUTE1X1(Z0, Z1, Z22)
-	PERMUTE1X1(Z2, Z3, Z22)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	PERMUTE1X1(Z2, Z3, Z22)
-	PERMUTE1X1(Z4, Z5, Z22)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	PERMUTE1X1(Z4, Z5, Z22)
-	PERMUTE1X1(Z6, Z7, Z22)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	PERMUTE1X1(Z6, Z7, Z22)
-	PERMUTE1X1(Z8, Z9, Z22)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	PERMUTE1X1(Z8, Z9, Z22)
-	PERMUTE1X1(Z10, Z11, Z22)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	PERMUTE1X1(Z10, Z11, Z22)
-	PERMUTE1X1(Z12, Z13, Z22)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	PERMUTE1X1(Z12, Z13, Z22)
-	PERMUTE1X1(Z14, Z15, Z22)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	PERMUTE1X1(Z14, Z15, Z22)
-	MOVQ         $0x0000000000000006, AX
-	IMULQ        $24, AX
-	ADDQ         AX, CX
-	MOVQ         0(CX), DI
-	VPBROADCASTD 0(DI), Z20
-	VPBROADCASTD 4(DI), Z21
-	VPBLENDMD    Z20, Z21, K3, Z20
-	PERMUTE2X2(Z0, Z1, Z22)
-	MULD(Z1, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	PERMUTE2X2(Z0, Z1, Z22)
-	PERMUTE2X2(Z2, Z3, Z22)
-	MULD(Z3, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	PERMUTE2X2(Z2, Z3, Z22)
-	PERMUTE2X2(Z4, Z5, Z22)
-	MULD(Z5, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	PERMUTE2X2(Z4, Z5, Z22)
-	PERMUTE2X2(Z6, Z7, Z22)
-	MULD(Z7, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	PERMUTE2X2(Z6, Z7, Z22)
-	PERMUTE2X2(Z8, Z9, Z22)
-	MULD(Z9, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	PERMUTE2X2(Z8, Z9, Z22)
-	PERMUTE2X2(Z10, Z11, Z22)
-	MULD(Z11, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	PERMUTE2X2(Z10, Z11, Z22)
-	PERMUTE2X2(Z12, Z13, Z22)
-	MULD(Z13, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	PERMUTE2X2(Z12, Z13, Z22)
-	PERMUTE2X2(Z14, Z15, Z22)
-	MULD(Z15, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	PERMUTE2X2(Z14, Z15, Z22)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), X19
-	VINSERTI64X2 $1, X19, Z19, Z19
-	VINSERTI64X2 $0x0000000000000002, X19, Z19, Z19
-	VINSERTI64X2 $0x0000000000000003, X19, Z19, Z19
-	PERMUTE4X4(Z0, Z1, Z28, Z22)
-	MULD(Z1, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	PERMUTE4X4(Z0, Z1, Z28, Z22)
-	PERMUTE4X4(Z2, Z3, Z28, Z22)
-	MULD(Z3, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	PERMUTE4X4(Z2, Z3, Z28, Z22)
-	PERMUTE4X4(Z4, Z5, Z28, Z22)
-	MULD(Z5, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	PERMUTE4X4(Z4, Z5, Z28, Z22)
-	PERMUTE4X4(Z6, Z7, Z28, Z22)
-	MULD(Z7, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	PERMUTE4X4(Z6, Z7, Z28, Z22)
-	PERMUTE4X4(Z8, Z9, Z28, Z22)
-	MULD(Z9, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	PERMUTE4X4(Z8, Z9, Z28, Z22)
-	PERMUTE4X4(Z10, Z11, Z28, Z22)
-	MULD(Z11, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	PERMUTE4X4(Z10, Z11, Z28, Z22)
-	PERMUTE4X4(Z12, Z13, Z28, Z22)
-	MULD(Z13, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	PERMUTE4X4(Z12, Z13, Z28, Z22)
-	PERMUTE4X4(Z14, Z15, Z28, Z22)
-	MULD(Z15, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	PERMUTE4X4(Z14, Z15, Z28, Z22)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Y18
-	VINSERTI64X4 $1, Y18, Z18, Z18
-	PERMUTE8X8(Z0, Z1, Z22)
-	MULD(Z1, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	PERMUTE8X8(Z0, Z1, Z22)
-	PERMUTE8X8(Z2, Z3, Z22)
-	MULD(Z3, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	PERMUTE8X8(Z2, Z3, Z22)
-	PERMUTE8X8(Z4, Z5, Z22)
-	MULD(Z5, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	PERMUTE8X8(Z4, Z5, Z22)
-	PERMUTE8X8(Z6, Z7, Z22)
-	MULD(Z7, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	PERMUTE8X8(Z6, Z7, Z22)
-	PERMUTE8X8(Z8, Z9, Z22)
-	MULD(Z9, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	PERMUTE8X8(Z8, Z9, Z22)
-	PERMUTE8X8(Z10, Z11, Z22)
-	MULD(Z11, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	PERMUTE8X8(Z10, Z11, Z22)
-	PERMUTE8X8(Z12, Z13, Z22)
-	MULD(Z13, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	PERMUTE8X8(Z12, Z13, Z22)
-	PERMUTE8X8(Z14, Z15, Z22)
-	MULD(Z15, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	PERMUTE8X8(Z14, Z15, Z22)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z29
-	MULD(Z1, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	MULD(Z3, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	MULD(Z5, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	MULD(Z7, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	MULD(Z9, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	MULD(Z11, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	MULD(Z13, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	MULD(Z15, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z29
-	VMOVDQU32    64(DI), Z30
-	MULD(Z2, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z2, Z16, Z22, Z23)
-	MULD(Z3, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z1, Z3, Z16, Z22, Z23)
-	MULD(Z6, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z6, Z16, Z22, Z23)
-	MULD(Z7, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z5, Z7, Z16, Z22, Z23)
-	MULD(Z10, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z10, Z16, Z22, Z23)
-	MULD(Z11, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z9, Z11, Z16, Z22, Z23)
-	MULD(Z14, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z14, Z16, Z22, Z23)
-	MULD(Z15, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z13, Z15, Z16, Z22, Z23)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z29
-	VMOVDQU32    64(DI), Z30
-	VMOVDQU32    128(DI), Z31
-	VMOVDQU32    192(DI), Z18
-	MULD(Z4, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z4, Z16, Z22, Z23)
-	MULD(Z5, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z1, Z5, Z16, Z22, Z23)
-	MULD(Z6, Z31, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z6, Z16, Z22, Z23)
-	MULD(Z7, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z3, Z7, Z16, Z22, Z23)
-	MULD(Z12, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z12, Z16, Z22, Z23)
-	MULD(Z13, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z9, Z13, Z16, Z22, Z23)
-	MULD(Z14, Z31, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z14, Z16, Z22, Z23)
-	MULD(Z15, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z11, Z15, Z16, Z22, Z23)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z29
-	VMOVDQU32    64(DI), Z30
-	VMOVDQU32    128(DI), Z31
-	VMOVDQU32    192(DI), Z18
-	VMOVDQU32    256(DI), Z19
-	VMOVDQU32    320(DI), Z20
-	VMOVDQU32    384(DI), Z21
-	VMOVDQU32    448(DI), Z28
-	MULD(Z8, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z8, Z16, Z22, Z23)
-	MULD(Z9, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z1, Z9, Z16, Z22, Z23)
-	MULD(Z10, Z31, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z10, Z16, Z22, Z23)
-	MULD(Z11, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z3, Z11, Z16, Z22, Z23)
-	MULD(Z12, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z12, Z16, Z22, Z23)
-	MULD(Z13, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z5, Z13, Z16, Z22, Z23)
-	MULD(Z14, Z21, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z14, Z16, Z22, Z23)
-	MULD(Z15, Z28, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z7, Z15, Z16, Z22, Z23)
-	VMOVDQU32    Z0, 0(R15)
-	VMOVDQU32    Z1, 64(R15)
-	VMOVDQU32    Z2, 128(R15)
-	VMOVDQU32    Z3, 192(R15)
-	VMOVDQU32    Z4, 256(R15)
-	VMOVDQU32    Z5, 320(R15)
-	VMOVDQU32    Z6, 384(R15)
-	VMOVDQU32    Z7, 448(R15)
-	VMOVDQU32    Z8, 512(R15)
-	VMOVDQU32    Z9, 576(R15)
-	VMOVDQU32    Z10, 640(R15)
-	VMOVDQU32    Z11, 704(R15)
-	VMOVDQU32    Z12, 768(R15)
-	VMOVDQU32    Z13, 832(R15)
-	VMOVDQU32    Z14, 896(R15)
-	VMOVDQU32    Z15, 960(R15)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/babybear/poseidon2/poseidon2_amd64.s b/field/babybear/poseidon2/poseidon2_amd64.s
--- a/field/babybear/poseidon2/poseidon2_amd64.s
+++ b/field/babybear/poseidon2/poseidon2_amd64.s
@@ -1,1229 +1,1 @@
-//go:build !purego
-
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-// Refer to the generator for more documentation.
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-TEXT ·permutation24_avx512(SB), NOSPLIT, $0-48
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         $1, AX
-	KMOVQ        AX, K2
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z0
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z1
-	MOVQ         input+0(FP), R15
-	MOVQ         roundKeys+24(FP), R14
-	VMOVDQU32    0(R15), Z2
-	VMOVDQU32    64(R15), Y3
-	MOVQ         ·diag24+0(SB), CX
-	VMOVDQU32    0(CX), Z18
-	VMOVDQU32    64(CX), Y20
-	VPSRLQ       $32, Z18, Z19
-	VPSRLQ       $32, Y20, Y21
-
-#define ADD(in0, in1, in2, in3, in4) \
-	VPADDD  in0, in1, in4 \
-	VPSUBD  in2, in4, in3 \
-	VPMINUD in4, in3, in4 \
-
-#define MAT_MUL_M4(in0, in1, in2, in3, in4, in5) \
-	VPSHUFD $0x000000000000004e, in0, in1 \
-	ADD(in1, in0, in4, in5, in1)          \
-	VPSHUFD $0x00000000000000b1, in1, in2 \
-	ADD(in1, in2, in4, in5, in1)          \
-	VPSHUFD $0x0000000000000039, in0, in3 \
-	VPSLLD  $1, in3, in3                  \
-	VPSUBD  in4, in3, in5                 \
-	VPMINUD in3, in5, in3                 \
-	ADD(in0, in1, in4, in5, in0)          \
-	ADD(in0, in3, in4, in5, in0)          \
-
-#define MAT_MUL_EXTERNAL() \
-	MAT_MUL_M4(Z2, Z6, Z7, Z8, Z0, Z11) \
-	MAT_MUL_M4(Y3, Y6, Y7, Y8, Y0, Y11) \
-	VEXTRACTI64X4 $1, Z2, Y16           \
-	ADD(Y16, Y2, Y0, Y11, Y16)          \
-	ADD(Y16, Y3, Y0, Y11, Y16)          \
-	VSHUFF64X2    $1, Y16, Y16, Y17     \
-	ADD(Y16, Y17, Y0, Y11, Y16)         \
-	VINSERTI64X4  $1, Y16, Z16, Z16     \
-	ADD(Y3, Y16, Y0, Y9, Y3)            \
-	ADD(Z2, Z16, Z0, Z11, Z2)           \
-
-#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10) \
-	VPSRLQ    $32, in0, in2  \
-	VPSRLQ    $32, in1, in3  \
-	VPMULUDQ  in0, in1, in4  \
-	VPMULUDQ  in2, in3, in5  \
-	VPMULUDQ  in4, in9, in6  \
-	VPMULUDQ  in5, in9, in7  \
-	VPMULUDQ  in6, in8, in6  \
-	VPADDQ    in4, in6, in4  \
-	VPMULUDQ  in7, in8, in7  \
-	VPADDQ    in5, in7, in10 \
-	VMOVSHDUP in4, K3, in10  \
-
-#define REDUCE1Q(in0, in1, in2) \
-	VPSUBD  in0, in1, in2 \
-	VPMINUD in1, in2, in1 \
-
-#define SBOX_FULL() \
-	MULD(Z2, Z2, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z8) \
-	REDUCE1Q(Z0, Z8, Z15)                                \
-	MULD(Z8, Z8, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z9) \
-	MULD(Z2, Z8, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z2) \
-	MULD(Z2, Z9, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z2) \
-	REDUCE1Q(Z0, Z2, Z15)                                \
-	MULD(Y3, Y3, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y8) \
-	REDUCE1Q(Y0, Y8, Y15)                                \
-	MULD(Y8, Y8, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y9) \
-	MULD(Y3, Y8, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y3) \
-	MULD(Y3, Y9, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y3) \
-	REDUCE1Q(Y0, Y3, Y15)                                \
-
-#define SBOX_PARTIAL() \
-	MULD(Y5, Y5, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y8) \
-	REDUCE1Q(Y0, Y8, Y15)                                \
-	MULD(Y8, Y8, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y9) \
-	MULD(Y5, Y8, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y5) \
-	MULD(Y5, Y9, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y5) \
-	REDUCE1Q(Y0, Y5, Y15)                                \
-
-#define SUM_STATE() \
-	VEXTRACTI64X4 $1, Z2, Y16                   \
-	ADD(Y16, Y3, Y0, Y11, Y16)                  \
-	ADD(Y16, Y10, Y0, Y11, Y16)                 \
-	VSHUFF64X2    $1, Y16, Y16, Y17             \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VPSHUFD       $0x000000000000004e, Y16, Y17 \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VPSHUFD       $0x00000000000000b1, Y16, Y17 \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VINSERTI64X4  $1, Y16, Z16, Z16             \
-
-#define FULL_ROUND() \
-	VMOVDQU32 0(BX), Z4      \
-	VMOVDQU32 64(BX), Y5     \
-	ADD(Z2, Z4, Z0, Z11, Z2) \
-	ADD(Y3, Y5, Y0, Y8, Y3)  \
-	SBOX_FULL()              \
-	MAT_MUL_EXTERNAL()       \
-
-	MAT_MUL_EXTERNAL()
-	MOVQ 0(R14), BX
-	FULL_ROUND()
-	MOVQ 24(R14), BX
-	FULL_ROUND()
-	MOVQ 48(R14), BX
-	FULL_ROUND()
-	MOVQ 72(R14), BX
-	FULL_ROUND()
-
-	// loop over the partial rounds
-	MOVQ $0x0000000000000015, SI // nb partial rounds --> 21
-	MOVQ R14, DI
-	ADDQ $0x0000000000000060, DI
-
-loop_1:
-	TESTQ     SI, SI
-	JEQ       done_2
-	DECQ      SI
-	MOVQ      0(DI), BX
-	VMOVD     0(BX), X4
-	VMOVDQA32 Z2, Z10
-	ADD(X10, X4, X0, X14, X5)
-	SBOX_PARTIAL()
-	VPBLENDMD Z5, Z10, K2, Z10
-	VPSRLQ    $32, Y3, Y12
-	VPMULUDQ  Y3, Y20, Y6
-	VPMULUDQ  Y12, Y21, Y7
-	VPMULUDQ  Y6, Y1, Y14
-	VPMULUDQ  Y7, Y1, Y15
-	VPMULUDQ  Y14, Y0, Y14
-	VPADDQ    Y6, Y14, Y6
-	VPMULUDQ  Y15, Y0, Y15
-	VPADDQ    Y7, Y15, Y9
-	VMOVSHDUP Y6, K3, Y9
-	VPSUBD    Y0, Y9, Y11
-	VPMINUD   Y9, Y11, Y9
-	VPSRLQ    $32, Z2, Z12
-	VPMULUDQ  Z12, Z19, Z8
-	VPMULUDQ  Z8, Z1, Z15
-	VPMULUDQ  Z15, Z0, Z15
-	VPADDQ    Z8, Z15, Z8
-	SUM_STATE()
-	VPMULUDQ  Z10, Z18, Z6
-	VPMULUDQ  Z6, Z1, Z14
-	VPMULUDQ  Z14, Z0, Z14
-	VPADDQ    Z6, Z14, Z6
-	VMOVSHDUP Z6, K3, Z8
-	VPSUBD    Z0, Z8, Z11
-	VPMINUD   Z8, Z11, Z2
-	ADD(Z2, Z16, Z0, Z11, Z2)
-	ADD(Y9, Y16, Y0, Y5, Y3)
-	ADDQ      $24, DI
-	JMP       loop_1
-
-done_2:
-	MOVQ      600(R14), BX
-	FULL_ROUND()
-	MOVQ      624(R14), BX
-	FULL_ROUND()
-	MOVQ      648(R14), BX
-	FULL_ROUND()
-	MOVQ      672(R14), BX
-	FULL_ROUND()
-	VMOVDQU32 Z2, 0(R15)
-	VMOVDQU32 Y3, 64(R15)
-	RET
-
-TEXT ·permutation16x24_avx512(SB), NOSPLIT, $0-32
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z24
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z25
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         input+0(FP), R15
-	MOVQ         roundKeys+8(FP), R14
-	VMOVDQU32    0(R15), Z0
-	VMOVDQU32    64(R15), Z1
-	VMOVDQU32    128(R15), Z2
-	VMOVDQU32    192(R15), Z3
-	VMOVDQU32    256(R15), Z4
-	VMOVDQU32    320(R15), Z5
-	VMOVDQU32    384(R15), Z6
-	VMOVDQU32    448(R15), Z7
-	VMOVDQU32    512(R15), Z8
-	VMOVDQU32    576(R15), Z9
-	VMOVDQU32    640(R15), Z10
-	VMOVDQU32    704(R15), Z11
-	VMOVDQU32    768(R15), Z12
-	VMOVDQU32    832(R15), Z13
-	VMOVDQU32    896(R15), Z14
-	VMOVDQU32    960(R15), Z15
-	VMOVDQU32    1024(R15), Z16
-	VMOVDQU32    1088(R15), Z17
-	VMOVDQU32    1152(R15), Z18
-	VMOVDQU32    1216(R15), Z19
-	VMOVDQU32    1280(R15), Z20
-	VMOVDQU32    1344(R15), Z21
-	VMOVDQU32    1408(R15), Z22
-	VMOVDQU32    1472(R15), Z23
-	ADD(Z0, Z1, Z24, Z31, Z26)
-	ADD(Z2, Z3, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z1, Z24, Z31, Z29)
-	ADD(Z28, Z3, Z24, Z31, Z30)
-
-#define DOUBLE(in0, in1, in2, in3) \
-	VPSLLD  $1, in0, in3  \
-	VPSUBD  in1, in3, in2 \
-	VPMINUD in3, in2, in3 \
-
-	DOUBLE(Z0, Z24, Z31, Z3)
-	ADD(Z3, Z30, Z24, Z31, Z3)
-	DOUBLE(Z2, Z24, Z31, Z1)
-	ADD(Z1, Z29, Z24, Z31, Z1)
-	ADD(Z26, Z29, Z24, Z31, Z0)
-	ADD(Z27, Z30, Z24, Z31, Z2)
-	ADD(Z4, Z5, Z24, Z31, Z26)
-	ADD(Z6, Z7, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z5, Z24, Z31, Z29)
-	ADD(Z28, Z7, Z24, Z31, Z30)
-	DOUBLE(Z4, Z24, Z31, Z7)
-	ADD(Z7, Z30, Z24, Z31, Z7)
-	DOUBLE(Z6, Z24, Z31, Z5)
-	ADD(Z5, Z29, Z24, Z31, Z5)
-	ADD(Z26, Z29, Z24, Z31, Z4)
-	ADD(Z27, Z30, Z24, Z31, Z6)
-	ADD(Z8, Z9, Z24, Z31, Z26)
-	ADD(Z10, Z11, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z9, Z24, Z31, Z29)
-	ADD(Z28, Z11, Z24, Z31, Z30)
-	DOUBLE(Z8, Z24, Z31, Z11)
-	ADD(Z11, Z30, Z24, Z31, Z11)
-	DOUBLE(Z10, Z24, Z31, Z9)
-	ADD(Z9, Z29, Z24, Z31, Z9)
-	ADD(Z26, Z29, Z24, Z31, Z8)
-	ADD(Z27, Z30, Z24, Z31, Z10)
-	ADD(Z12, Z13, Z24, Z31, Z26)
-	ADD(Z14, Z15, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z13, Z24, Z31, Z29)
-	ADD(Z28, Z15, Z24, Z31, Z30)
-	DOUBLE(Z12, Z24, Z31, Z15)
-	ADD(Z15, Z30, Z24, Z31, Z15)
-	DOUBLE(Z14, Z24, Z31, Z13)
-	ADD(Z13, Z29, Z24, Z31, Z13)
-	ADD(Z26, Z29, Z24, Z31, Z12)
-	ADD(Z27, Z30, Z24, Z31, Z14)
-	ADD(Z16, Z17, Z24, Z31, Z26)
-	ADD(Z18, Z19, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z17, Z24, Z31, Z29)
-	ADD(Z28, Z19, Z24, Z31, Z30)
-	DOUBLE(Z16, Z24, Z31, Z19)
-	ADD(Z19, Z30, Z24, Z31, Z19)
-	DOUBLE(Z18, Z24, Z31, Z17)
-	ADD(Z17, Z29, Z24, Z31, Z17)
-	ADD(Z26, Z29, Z24, Z31, Z16)
-	ADD(Z27, Z30, Z24, Z31, Z18)
-	ADD(Z20, Z21, Z24, Z31, Z26)
-	ADD(Z22, Z23, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z21, Z24, Z31, Z29)
-	ADD(Z28, Z23, Z24, Z31, Z30)
-	DOUBLE(Z20, Z24, Z31, Z23)
-	ADD(Z23, Z30, Z24, Z31, Z23)
-	DOUBLE(Z22, Z24, Z31, Z21)
-	ADD(Z21, Z29, Z24, Z31, Z21)
-	ADD(Z26, Z29, Z24, Z31, Z20)
-	ADD(Z27, Z30, Z24, Z31, Z22)
-	ADD(Z0, Z4, Z24, Z29, Z31)
-	ADD(Z1, Z5, Z24, Z30, Z26)
-	ADD(Z2, Z6, Z24, Z29, Z27)
-	ADD(Z3, Z7, Z24, Z30, Z28)
-	ADD(Z8, Z31, Z24, Z29, Z31)
-	ADD(Z9, Z26, Z24, Z30, Z26)
-	ADD(Z10, Z27, Z24, Z29, Z27)
-	ADD(Z11, Z28, Z24, Z30, Z28)
-	ADD(Z12, Z31, Z24, Z29, Z31)
-	ADD(Z13, Z26, Z24, Z30, Z26)
-	ADD(Z14, Z27, Z24, Z29, Z27)
-	ADD(Z15, Z28, Z24, Z30, Z28)
-	ADD(Z16, Z31, Z24, Z29, Z31)
-	ADD(Z17, Z26, Z24, Z30, Z26)
-	ADD(Z18, Z27, Z24, Z29, Z27)
-	ADD(Z19, Z28, Z24, Z30, Z28)
-	ADD(Z20, Z31, Z24, Z29, Z31)
-	ADD(Z21, Z26, Z24, Z30, Z26)
-	ADD(Z22, Z27, Z24, Z29, Z27)
-	ADD(Z23, Z28, Z24, Z30, Z28)
-	ADD(Z0, Z31, Z24, Z29, Z0)
-	ADD(Z1, Z26, Z24, Z30, Z1)
-	ADD(Z2, Z27, Z24, Z29, Z2)
-	ADD(Z3, Z28, Z24, Z30, Z3)
-	ADD(Z4, Z31, Z24, Z29, Z4)
-	ADD(Z5, Z26, Z24, Z30, Z5)
-	ADD(Z6, Z27, Z24, Z29, Z6)
-	ADD(Z7, Z28, Z24, Z30, Z7)
-	ADD(Z8, Z31, Z24, Z29, Z8)
-	ADD(Z9, Z26, Z24, Z30, Z9)
-	ADD(Z10, Z27, Z24, Z29, Z10)
-	ADD(Z11, Z28, Z24, Z30, Z11)
-	ADD(Z12, Z31, Z24, Z29, Z12)
-	ADD(Z13, Z26, Z24, Z30, Z13)
-	ADD(Z14, Z27, Z24, Z29, Z14)
-	ADD(Z15, Z28, Z24, Z30, Z15)
-	ADD(Z16, Z31, Z24, Z29, Z16)
-	ADD(Z17, Z26, Z24, Z30, Z17)
-	ADD(Z18, Z27, Z24, Z29, Z18)
-	ADD(Z19, Z28, Z24, Z30, Z19)
-	ADD(Z20, Z31, Z24, Z29, Z20)
-	ADD(Z21, Z26, Z24, Z30, Z21)
-	ADD(Z22, Z27, Z24, Z29, Z22)
-	ADD(Z23, Z28, Z24, Z30, Z23)
-
-	// loop over the first full rounds
-	MOVQ $0x0000000000000004, BX
-
-loop_3:
-	TESTQ        BX, BX
-	JEQ          done_4
-	DECQ         BX
-	MOVQ         0(R14), CX
-	VPBROADCASTD 0(CX), Z29
-	ADD(Z0, Z29, Z24, Z30, Z0)
-
-#define MUL_W(in0, in1, in2, in3, in4, in5, in6) \
-	VPSRLQ    $32, in0, in2 \
-	VPSRLQ    $32, in1, in3 \
-	VPMULUDQ  in0, in1, in4 \
-	VPMULUDQ  in2, in3, in5 \
-	VPMULUDQ  in4, Z25, in2 \
-	VPMULUDQ  in5, Z25, in3 \
-	VPMULUDQ  in2, Z24, in2 \
-	VPADDQ    in4, in2, in4 \
-	VPMULUDQ  in3, Z24, in3 \
-	VPADDQ    in5, in3, in6 \
-	VMOVSHDUP in4, K3, in6  \
-
-	MUL_W(Z0, Z0, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z0, Z26, Z28, Z30, Z29, Z27, Z0)
-	MUL_W(Z0, Z31, Z28, Z30, Z29, Z27, Z0)
-	REDUCE1Q(Z24, Z0, Z28)
-	VPBROADCASTD 4(CX), Z30
-	ADD(Z1, Z30, Z24, Z29, Z1)
-	MUL_W(Z1, Z1, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z1, Z28, Z26, Z29, Z30, Z31, Z1)
-	MUL_W(Z1, Z27, Z26, Z29, Z30, Z31, Z1)
-	REDUCE1Q(Z24, Z1, Z26)
-	VPBROADCASTD 8(CX), Z29
-	ADD(Z2, Z29, Z24, Z30, Z2)
-	MUL_W(Z2, Z2, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z2, Z26, Z28, Z30, Z29, Z27, Z2)
-	MUL_W(Z2, Z31, Z28, Z30, Z29, Z27, Z2)
-	REDUCE1Q(Z24, Z2, Z28)
-	VPBROADCASTD 12(CX), Z30
-	ADD(Z3, Z30, Z24, Z29, Z3)
-	MUL_W(Z3, Z3, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z3, Z28, Z26, Z29, Z30, Z31, Z3)
-	MUL_W(Z3, Z27, Z26, Z29, Z30, Z31, Z3)
-	REDUCE1Q(Z24, Z3, Z26)
-	VPBROADCASTD 16(CX), Z29
-	ADD(Z4, Z29, Z24, Z30, Z4)
-	MUL_W(Z4, Z4, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z4, Z26, Z28, Z30, Z29, Z27, Z4)
-	MUL_W(Z4, Z31, Z28, Z30, Z29, Z27, Z4)
-	REDUCE1Q(Z24, Z4, Z28)
-	VPBROADCASTD 20(CX), Z30
-	ADD(Z5, Z30, Z24, Z29, Z5)
-	MUL_W(Z5, Z5, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z5, Z28, Z26, Z29, Z30, Z31, Z5)
-	MUL_W(Z5, Z27, Z26, Z29, Z30, Z31, Z5)
-	REDUCE1Q(Z24, Z5, Z26)
-	VPBROADCASTD 24(CX), Z29
-	ADD(Z6, Z29, Z24, Z30, Z6)
-	MUL_W(Z6, Z6, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z6, Z26, Z28, Z30, Z29, Z27, Z6)
-	MUL_W(Z6, Z31, Z28, Z30, Z29, Z27, Z6)
-	REDUCE1Q(Z24, Z6, Z28)
-	VPBROADCASTD 28(CX), Z30
-	ADD(Z7, Z30, Z24, Z29, Z7)
-	MUL_W(Z7, Z7, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z7, Z28, Z26, Z29, Z30, Z31, Z7)
-	MUL_W(Z7, Z27, Z26, Z29, Z30, Z31, Z7)
-	REDUCE1Q(Z24, Z7, Z26)
-	VPBROADCASTD 32(CX), Z29
-	ADD(Z8, Z29, Z24, Z30, Z8)
-	MUL_W(Z8, Z8, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z8, Z26, Z28, Z30, Z29, Z27, Z8)
-	MUL_W(Z8, Z31, Z28, Z30, Z29, Z27, Z8)
-	REDUCE1Q(Z24, Z8, Z28)
-	VPBROADCASTD 36(CX), Z30
-	ADD(Z9, Z30, Z24, Z29, Z9)
-	MUL_W(Z9, Z9, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z9, Z28, Z26, Z29, Z30, Z31, Z9)
-	MUL_W(Z9, Z27, Z26, Z29, Z30, Z31, Z9)
-	REDUCE1Q(Z24, Z9, Z26)
-	VPBROADCASTD 40(CX), Z29
-	ADD(Z10, Z29, Z24, Z30, Z10)
-	MUL_W(Z10, Z10, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z10, Z26, Z28, Z30, Z29, Z27, Z10)
-	MUL_W(Z10, Z31, Z28, Z30, Z29, Z27, Z10)
-	REDUCE1Q(Z24, Z10, Z28)
-	VPBROADCASTD 44(CX), Z30
-	ADD(Z11, Z30, Z24, Z29, Z11)
-	MUL_W(Z11, Z11, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z11, Z28, Z26, Z29, Z30, Z31, Z11)
-	MUL_W(Z11, Z27, Z26, Z29, Z30, Z31, Z11)
-	REDUCE1Q(Z24, Z11, Z26)
-	VPBROADCASTD 48(CX), Z29
-	ADD(Z12, Z29, Z24, Z30, Z12)
-	MUL_W(Z12, Z12, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z12, Z26, Z28, Z30, Z29, Z27, Z12)
-	MUL_W(Z12, Z31, Z28, Z30, Z29, Z27, Z12)
-	REDUCE1Q(Z24, Z12, Z28)
-	VPBROADCASTD 52(CX), Z30
-	ADD(Z13, Z30, Z24, Z29, Z13)
-	MUL_W(Z13, Z13, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z13, Z28, Z26, Z29, Z30, Z31, Z13)
-	MUL_W(Z13, Z27, Z26, Z29, Z30, Z31, Z13)
-	REDUCE1Q(Z24, Z13, Z26)
-	VPBROADCASTD 56(CX), Z29
-	ADD(Z14, Z29, Z24, Z30, Z14)
-	MUL_W(Z14, Z14, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z14, Z26, Z28, Z30, Z29, Z27, Z14)
-	MUL_W(Z14, Z31, Z28, Z30, Z29, Z27, Z14)
-	REDUCE1Q(Z24, Z14, Z28)
-	VPBROADCASTD 60(CX), Z30
-	ADD(Z15, Z30, Z24, Z29, Z15)
-	MUL_W(Z15, Z15, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z15, Z28, Z26, Z29, Z30, Z31, Z15)
-	MUL_W(Z15, Z27, Z26, Z29, Z30, Z31, Z15)
-	REDUCE1Q(Z24, Z15, Z26)
-	VPBROADCASTD 64(CX), Z29
-	ADD(Z16, Z29, Z24, Z30, Z16)
-	MUL_W(Z16, Z16, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z16, Z26, Z28, Z30, Z29, Z27, Z16)
-	MUL_W(Z16, Z31, Z28, Z30, Z29, Z27, Z16)
-	REDUCE1Q(Z24, Z16, Z28)
-	VPBROADCASTD 68(CX), Z30
-	ADD(Z17, Z30, Z24, Z29, Z17)
-	MUL_W(Z17, Z17, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z17, Z28, Z26, Z29, Z30, Z31, Z17)
-	MUL_W(Z17, Z27, Z26, Z29, Z30, Z31, Z17)
-	REDUCE1Q(Z24, Z17, Z26)
-	VPBROADCASTD 72(CX), Z29
-	ADD(Z18, Z29, Z24, Z30, Z18)
-	MUL_W(Z18, Z18, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z18, Z26, Z28, Z30, Z29, Z27, Z18)
-	MUL_W(Z18, Z31, Z28, Z30, Z29, Z27, Z18)
-	REDUCE1Q(Z24, Z18, Z28)
-	VPBROADCASTD 76(CX), Z30
-	ADD(Z19, Z30, Z24, Z29, Z19)
-	MUL_W(Z19, Z19, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z19, Z28, Z26, Z29, Z30, Z31, Z19)
-	MUL_W(Z19, Z27, Z26, Z29, Z30, Z31, Z19)
-	REDUCE1Q(Z24, Z19, Z26)
-	VPBROADCASTD 80(CX), Z29
-	ADD(Z20, Z29, Z24, Z30, Z20)
-	MUL_W(Z20, Z20, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z20, Z26, Z28, Z30, Z29, Z27, Z20)
-	MUL_W(Z20, Z31, Z28, Z30, Z29, Z27, Z20)
-	REDUCE1Q(Z24, Z20, Z28)
-	VPBROADCASTD 84(CX), Z30
-	ADD(Z21, Z30, Z24, Z29, Z21)
-	MUL_W(Z21, Z21, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z21, Z28, Z26, Z29, Z30, Z31, Z21)
-	MUL_W(Z21, Z27, Z26, Z29, Z30, Z31, Z21)
-	REDUCE1Q(Z24, Z21, Z26)
-	VPBROADCASTD 88(CX), Z29
-	ADD(Z22, Z29, Z24, Z30, Z22)
-	MUL_W(Z22, Z22, Z27, Z28, Z30, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z27)
-	MUL_W(Z31, Z31, Z28, Z30, Z29, Z27, Z26)
-	MUL_W(Z22, Z26, Z28, Z30, Z29, Z27, Z22)
-	MUL_W(Z22, Z31, Z28, Z30, Z29, Z27, Z22)
-	REDUCE1Q(Z24, Z22, Z28)
-	VPBROADCASTD 92(CX), Z30
-	ADD(Z23, Z30, Z24, Z29, Z23)
-	MUL_W(Z23, Z23, Z31, Z26, Z29, Z30, Z27)
-	REDUCE1Q(Z24, Z27, Z31)
-	MUL_W(Z27, Z27, Z26, Z29, Z30, Z31, Z28)
-	MUL_W(Z23, Z28, Z26, Z29, Z30, Z31, Z23)
-	MUL_W(Z23, Z27, Z26, Z29, Z30, Z31, Z23)
-	REDUCE1Q(Z24, Z23, Z26)
-	ADD(Z0, Z1, Z24, Z28, Z29)
-	ADD(Z2, Z3, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z1, Z24, Z28, Z26)
-	ADD(Z31, Z3, Z24, Z28, Z27)
-	DOUBLE(Z0, Z24, Z28, Z3)
-	ADD(Z3, Z27, Z24, Z28, Z3)
-	DOUBLE(Z2, Z24, Z28, Z1)
-	ADD(Z1, Z26, Z24, Z28, Z1)
-	ADD(Z29, Z26, Z24, Z28, Z0)
-	ADD(Z30, Z27, Z24, Z28, Z2)
-	ADD(Z4, Z5, Z24, Z28, Z29)
-	ADD(Z6, Z7, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z5, Z24, Z28, Z26)
-	ADD(Z31, Z7, Z24, Z28, Z27)
-	DOUBLE(Z4, Z24, Z28, Z7)
-	ADD(Z7, Z27, Z24, Z28, Z7)
-	DOUBLE(Z6, Z24, Z28, Z5)
-	ADD(Z5, Z26, Z24, Z28, Z5)
-	ADD(Z29, Z26, Z24, Z28, Z4)
-	ADD(Z30, Z27, Z24, Z28, Z6)
-	ADD(Z8, Z9, Z24, Z28, Z29)
-	ADD(Z10, Z11, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z9, Z24, Z28, Z26)
-	ADD(Z31, Z11, Z24, Z28, Z27)
-	DOUBLE(Z8, Z24, Z28, Z11)
-	ADD(Z11, Z27, Z24, Z28, Z11)
-	DOUBLE(Z10, Z24, Z28, Z9)
-	ADD(Z9, Z26, Z24, Z28, Z9)
-	ADD(Z29, Z26, Z24, Z28, Z8)
-	ADD(Z30, Z27, Z24, Z28, Z10)
-	ADD(Z12, Z13, Z24, Z28, Z29)
-	ADD(Z14, Z15, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z13, Z24, Z28, Z26)
-	ADD(Z31, Z15, Z24, Z28, Z27)
-	DOUBLE(Z12, Z24, Z28, Z15)
-	ADD(Z15, Z27, Z24, Z28, Z15)
-	DOUBLE(Z14, Z24, Z28, Z13)
-	ADD(Z13, Z26, Z24, Z28, Z13)
-	ADD(Z29, Z26, Z24, Z28, Z12)
-	ADD(Z30, Z27, Z24, Z28, Z14)
-	ADD(Z16, Z17, Z24, Z28, Z29)
-	ADD(Z18, Z19, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z17, Z24, Z28, Z26)
-	ADD(Z31, Z19, Z24, Z28, Z27)
-	DOUBLE(Z16, Z24, Z28, Z19)
-	ADD(Z19, Z27, Z24, Z28, Z19)
-	DOUBLE(Z18, Z24, Z28, Z17)
-	ADD(Z17, Z26, Z24, Z28, Z17)
-	ADD(Z29, Z26, Z24, Z28, Z16)
-	ADD(Z30, Z27, Z24, Z28, Z18)
-	ADD(Z20, Z21, Z24, Z28, Z29)
-	ADD(Z22, Z23, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z21, Z24, Z28, Z26)
-	ADD(Z31, Z23, Z24, Z28, Z27)
-	DOUBLE(Z20, Z24, Z28, Z23)
-	ADD(Z23, Z27, Z24, Z28, Z23)
-	DOUBLE(Z22, Z24, Z28, Z21)
-	ADD(Z21, Z26, Z24, Z28, Z21)
-	ADD(Z29, Z26, Z24, Z28, Z20)
-	ADD(Z30, Z27, Z24, Z28, Z22)
-	ADD(Z0, Z4, Z24, Z26, Z28)
-	ADD(Z1, Z5, Z24, Z27, Z29)
-	ADD(Z2, Z6, Z24, Z26, Z30)
-	ADD(Z3, Z7, Z24, Z27, Z31)
-	ADD(Z8, Z28, Z24, Z26, Z28)
-	ADD(Z9, Z29, Z24, Z27, Z29)
-	ADD(Z10, Z30, Z24, Z26, Z30)
-	ADD(Z11, Z31, Z24, Z27, Z31)
-	ADD(Z12, Z28, Z24, Z26, Z28)
-	ADD(Z13, Z29, Z24, Z27, Z29)
-	ADD(Z14, Z30, Z24, Z26, Z30)
-	ADD(Z15, Z31, Z24, Z27, Z31)
-	ADD(Z16, Z28, Z24, Z26, Z28)
-	ADD(Z17, Z29, Z24, Z27, Z29)
-	ADD(Z18, Z30, Z24, Z26, Z30)
-	ADD(Z19, Z31, Z24, Z27, Z31)
-	ADD(Z20, Z28, Z24, Z26, Z28)
-	ADD(Z21, Z29, Z24, Z27, Z29)
-	ADD(Z22, Z30, Z24, Z26, Z30)
-	ADD(Z23, Z31, Z24, Z27, Z31)
-	ADD(Z0, Z28, Z24, Z26, Z0)
-	ADD(Z1, Z29, Z24, Z27, Z1)
-	ADD(Z2, Z30, Z24, Z26, Z2)
-	ADD(Z3, Z31, Z24, Z27, Z3)
-	ADD(Z4, Z28, Z24, Z26, Z4)
-	ADD(Z5, Z29, Z24, Z27, Z5)
-	ADD(Z6, Z30, Z24, Z26, Z6)
-	ADD(Z7, Z31, Z24, Z27, Z7)
-	ADD(Z8, Z28, Z24, Z26, Z8)
-	ADD(Z9, Z29, Z24, Z27, Z9)
-	ADD(Z10, Z30, Z24, Z26, Z10)
-	ADD(Z11, Z31, Z24, Z27, Z11)
-	ADD(Z12, Z28, Z24, Z26, Z12)
-	ADD(Z13, Z29, Z24, Z27, Z13)
-	ADD(Z14, Z30, Z24, Z26, Z14)
-	ADD(Z15, Z31, Z24, Z27, Z15)
-	ADD(Z16, Z28, Z24, Z26, Z16)
-	ADD(Z17, Z29, Z24, Z27, Z17)
-	ADD(Z18, Z30, Z24, Z26, Z18)
-	ADD(Z19, Z31, Z24, Z27, Z19)
-	ADD(Z20, Z28, Z24, Z26, Z20)
-	ADD(Z21, Z29, Z24, Z27, Z21)
-	ADD(Z22, Z30, Z24, Z26, Z22)
-	ADD(Z23, Z31, Z24, Z27, Z23)
-	ADDQ         $24, R14
-	JMP          loop_3
-
-done_4:
-	// loop over the partial rounds
-	MOVQ $0x0000000000000015, SI
-
-loop_5:
-	TESTQ        SI, SI
-	JEQ          done_6
-	DECQ         SI
-	MOVQ         0(R14), CX
-	VPBROADCASTD 0(CX), Z26
-	ADD(Z0, Z26, Z24, Z27, Z0)
-	MUL_W(Z0, Z0, Z30, Z31, Z27, Z26, Z28)
-	REDUCE1Q(Z24, Z28, Z30)
-	MUL_W(Z28, Z28, Z31, Z27, Z26, Z30, Z29)
-	MUL_W(Z0, Z29, Z31, Z27, Z26, Z30, Z0)
-	MUL_W(Z0, Z28, Z31, Z27, Z26, Z30, Z0)
-	REDUCE1Q(Z24, Z0, Z31)
-	ADD(Z0, Z1, Z24, Z29, Z30)
-	ADD(Z2, Z3, Z24, Z29, Z31)
-	ADD(Z4, Z5, Z24, Z29, Z28)
-	ADD(Z6, Z7, Z24, Z29, Z27)
-	ADD(Z8, Z30, Z24, Z29, Z30)
-	ADD(Z9, Z31, Z24, Z29, Z31)
-	ADD(Z10, Z28, Z24, Z29, Z28)
-	ADD(Z11, Z27, Z24, Z29, Z27)
-	ADD(Z12, Z30, Z24, Z29, Z30)
-	ADD(Z13, Z31, Z24, Z29, Z31)
-	ADD(Z14, Z28, Z24, Z29, Z28)
-	ADD(Z15, Z27, Z24, Z29, Z27)
-	ADD(Z16, Z30, Z24, Z29, Z30)
-	ADD(Z17, Z31, Z24, Z29, Z31)
-	ADD(Z18, Z28, Z24, Z29, Z28)
-	ADD(Z19, Z27, Z24, Z29, Z27)
-	ADD(Z20, Z30, Z24, Z29, Z30)
-	ADD(Z21, Z31, Z24, Z29, Z31)
-	ADD(Z22, Z28, Z24, Z29, Z28)
-	ADD(Z23, Z27, Z24, Z29, Z27)
-	ADD(Z30, Z31, Z24, Z29, Z30)
-	ADD(Z28, Z27, Z24, Z29, Z28)
-	ADD(Z30, Z28, Z24, Z29, Z27)
-	DOUBLE(Z0, Z24, Z29, Z0)
-	DOUBLE(Z2, Z24, Z29, Z2)
-
-#define HALVE(in0, in1) \
-	MOVD         $1, AX            \
-	VPBROADCASTD AX, in1           \
-	VPTESTMD     in0, in1, K4      \
-	VPADDD       in0, Z24, K4, in0 \
-	VPSRLD       $1, in0, in0      \
-
-	HALVE(Z3, Z29)
-	HALVE(Z6, Z29)
-	DOUBLE(Z4, Z24, Z29, Z30)
-	ADD(Z4, Z30, Z24, Z29, Z4)
-	DOUBLE(Z5, Z24, Z29, Z5)
-	DOUBLE(Z5, Z24, Z29, Z5)
-	DOUBLE(Z7, Z24, Z29, Z26)
-	ADD(Z7, Z26, Z24, Z29, Z7)
-	DOUBLE(Z8, Z24, Z29, Z8)
-	DOUBLE(Z8, Z24, Z29, Z8)
-
-#define MUL_2_EXP_NEG_N(in0, in1, in2, in3, in4, in5, in6, in7, in8) \
-	VPSRLQ    $32, in0, in6 \
-	VPSLLQ    $32, in0, in0 \
-	VPSRLQ    in2, in0, in4 \
-	VPSLLQ    in3, in6, in5 \
-	VPMULUDQ  in4, Z25, in7 \
-	VPMULUDQ  in5, Z25, in8 \
-	VPMULUDQ  in7, Z24, in7 \
-	VPADDQ    in4, in7, in4 \
-	VPMULUDQ  in8, Z24, in8 \
-	VPADDQ    in5, in8, in1 \
-	VMOVSHDUP in4, K3, in1  \
-	VPSUBD    Z24, in1, in8 \
-	VPMINUD   in1, in8, in1 \
-
-	MUL_2_EXP_NEG_N(Z9, Z9, $8, $24, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z10, Z10, $2, $30, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z11, Z11, $3, $29, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z12, Z12, $4, $28, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z13, Z13, $7, $25, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z14, Z14, $9, $23, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z15, Z15, $27, $5, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z16, Z16, $8, $24, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z17, Z17, $2, $30, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z18, Z18, $3, $29, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z19, Z19, $4, $28, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z20, Z20, $5, $27, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z21, Z21, $6, $26, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z22, Z22, $7, $25, Z29, Z26, Z30, Z31, Z28)
-	MUL_2_EXP_NEG_N(Z23, Z23, $27, $5, Z29, Z26, Z30, Z31, Z28)
-
-#define SUB(in0, in1, in2, in3, in4) \
-	VPSUBD  in1, in0, in4 \
-	VPADDD  in2, in4, in3 \
-	VPMINUD in4, in3, in4 \
-
-	SUB(Z27, Z0, Z24, Z29, Z0)
-	ADD(Z27, Z1, Z24, Z26, Z1)
-	ADD(Z2, Z27, Z24, Z30, Z2)
-	ADD(Z3, Z27, Z24, Z31, Z3)
-	ADD(Z4, Z27, Z24, Z28, Z4)
-	ADD(Z5, Z27, Z24, Z29, Z5)
-	SUB(Z27, Z6, Z24, Z26, Z6)
-	SUB(Z27, Z7, Z24, Z30, Z7)
-	SUB(Z27, Z8, Z24, Z31, Z8)
-	ADD(Z9, Z27, Z24, Z28, Z9)
-	ADD(Z10, Z27, Z24, Z29, Z10)
-	ADD(Z11, Z27, Z24, Z26, Z11)
-	ADD(Z12, Z27, Z24, Z30, Z12)
-	ADD(Z13, Z27, Z24, Z31, Z13)
-	ADD(Z14, Z27, Z24, Z28, Z14)
-	ADD(Z15, Z27, Z24, Z29, Z15)
-	SUB(Z27, Z16, Z24, Z26, Z16)
-	SUB(Z27, Z17, Z24, Z30, Z17)
-	SUB(Z27, Z18, Z24, Z31, Z18)
-	SUB(Z27, Z19, Z24, Z28, Z19)
-	SUB(Z27, Z20, Z24, Z29, Z20)
-	SUB(Z27, Z21, Z24, Z26, Z21)
-	SUB(Z27, Z22, Z24, Z30, Z22)
-	SUB(Z27, Z23, Z24, Z31, Z23)
-	ADDQ $24, R14
-	JMP  loop_5
-
-done_6:
-	// loop over the final full rounds
-	MOVQ $0x0000000000000004, DI
-
-loop_7:
-	TESTQ        DI, DI
-	JEQ          done_8
-	DECQ         DI
-	MOVQ         0(R14), CX
-	VPBROADCASTD 0(CX), Z28
-	ADD(Z0, Z28, Z24, Z29, Z0)
-	MUL_W(Z0, Z0, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z0, Z30, Z27, Z29, Z28, Z31, Z0)
-	MUL_W(Z0, Z26, Z27, Z29, Z28, Z31, Z0)
-	REDUCE1Q(Z24, Z0, Z27)
-	VPBROADCASTD 4(CX), Z29
-	ADD(Z1, Z29, Z24, Z28, Z1)
-	MUL_W(Z1, Z1, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z1, Z27, Z30, Z28, Z29, Z26, Z1)
-	MUL_W(Z1, Z31, Z30, Z28, Z29, Z26, Z1)
-	REDUCE1Q(Z24, Z1, Z30)
-	VPBROADCASTD 8(CX), Z28
-	ADD(Z2, Z28, Z24, Z29, Z2)
-	MUL_W(Z2, Z2, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z2, Z30, Z27, Z29, Z28, Z31, Z2)
-	MUL_W(Z2, Z26, Z27, Z29, Z28, Z31, Z2)
-	REDUCE1Q(Z24, Z2, Z27)
-	VPBROADCASTD 12(CX), Z29
-	ADD(Z3, Z29, Z24, Z28, Z3)
-	MUL_W(Z3, Z3, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z3, Z27, Z30, Z28, Z29, Z26, Z3)
-	MUL_W(Z3, Z31, Z30, Z28, Z29, Z26, Z3)
-	REDUCE1Q(Z24, Z3, Z30)
-	VPBROADCASTD 16(CX), Z28
-	ADD(Z4, Z28, Z24, Z29, Z4)
-	MUL_W(Z4, Z4, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z4, Z30, Z27, Z29, Z28, Z31, Z4)
-	MUL_W(Z4, Z26, Z27, Z29, Z28, Z31, Z4)
-	REDUCE1Q(Z24, Z4, Z27)
-	VPBROADCASTD 20(CX), Z29
-	ADD(Z5, Z29, Z24, Z28, Z5)
-	MUL_W(Z5, Z5, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z5, Z27, Z30, Z28, Z29, Z26, Z5)
-	MUL_W(Z5, Z31, Z30, Z28, Z29, Z26, Z5)
-	REDUCE1Q(Z24, Z5, Z30)
-	VPBROADCASTD 24(CX), Z28
-	ADD(Z6, Z28, Z24, Z29, Z6)
-	MUL_W(Z6, Z6, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z6, Z30, Z27, Z29, Z28, Z31, Z6)
-	MUL_W(Z6, Z26, Z27, Z29, Z28, Z31, Z6)
-	REDUCE1Q(Z24, Z6, Z27)
-	VPBROADCASTD 28(CX), Z29
-	ADD(Z7, Z29, Z24, Z28, Z7)
-	MUL_W(Z7, Z7, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z7, Z27, Z30, Z28, Z29, Z26, Z7)
-	MUL_W(Z7, Z31, Z30, Z28, Z29, Z26, Z7)
-	REDUCE1Q(Z24, Z7, Z30)
-	VPBROADCASTD 32(CX), Z28
-	ADD(Z8, Z28, Z24, Z29, Z8)
-	MUL_W(Z8, Z8, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z8, Z30, Z27, Z29, Z28, Z31, Z8)
-	MUL_W(Z8, Z26, Z27, Z29, Z28, Z31, Z8)
-	REDUCE1Q(Z24, Z8, Z27)
-	VPBROADCASTD 36(CX), Z29
-	ADD(Z9, Z29, Z24, Z28, Z9)
-	MUL_W(Z9, Z9, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z9, Z27, Z30, Z28, Z29, Z26, Z9)
-	MUL_W(Z9, Z31, Z30, Z28, Z29, Z26, Z9)
-	REDUCE1Q(Z24, Z9, Z30)
-	VPBROADCASTD 40(CX), Z28
-	ADD(Z10, Z28, Z24, Z29, Z10)
-	MUL_W(Z10, Z10, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z10, Z30, Z27, Z29, Z28, Z31, Z10)
-	MUL_W(Z10, Z26, Z27, Z29, Z28, Z31, Z10)
-	REDUCE1Q(Z24, Z10, Z27)
-	VPBROADCASTD 44(CX), Z29
-	ADD(Z11, Z29, Z24, Z28, Z11)
-	MUL_W(Z11, Z11, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z11, Z27, Z30, Z28, Z29, Z26, Z11)
-	MUL_W(Z11, Z31, Z30, Z28, Z29, Z26, Z11)
-	REDUCE1Q(Z24, Z11, Z30)
-	VPBROADCASTD 48(CX), Z28
-	ADD(Z12, Z28, Z24, Z29, Z12)
-	MUL_W(Z12, Z12, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z12, Z30, Z27, Z29, Z28, Z31, Z12)
-	MUL_W(Z12, Z26, Z27, Z29, Z28, Z31, Z12)
-	REDUCE1Q(Z24, Z12, Z27)
-	VPBROADCASTD 52(CX), Z29
-	ADD(Z13, Z29, Z24, Z28, Z13)
-	MUL_W(Z13, Z13, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z13, Z27, Z30, Z28, Z29, Z26, Z13)
-	MUL_W(Z13, Z31, Z30, Z28, Z29, Z26, Z13)
-	REDUCE1Q(Z24, Z13, Z30)
-	VPBROADCASTD 56(CX), Z28
-	ADD(Z14, Z28, Z24, Z29, Z14)
-	MUL_W(Z14, Z14, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z14, Z30, Z27, Z29, Z28, Z31, Z14)
-	MUL_W(Z14, Z26, Z27, Z29, Z28, Z31, Z14)
-	REDUCE1Q(Z24, Z14, Z27)
-	VPBROADCASTD 60(CX), Z29
-	ADD(Z15, Z29, Z24, Z28, Z15)
-	MUL_W(Z15, Z15, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z15, Z27, Z30, Z28, Z29, Z26, Z15)
-	MUL_W(Z15, Z31, Z30, Z28, Z29, Z26, Z15)
-	REDUCE1Q(Z24, Z15, Z30)
-	VPBROADCASTD 64(CX), Z28
-	ADD(Z16, Z28, Z24, Z29, Z16)
-	MUL_W(Z16, Z16, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z16, Z30, Z27, Z29, Z28, Z31, Z16)
-	MUL_W(Z16, Z26, Z27, Z29, Z28, Z31, Z16)
-	REDUCE1Q(Z24, Z16, Z27)
-	VPBROADCASTD 68(CX), Z29
-	ADD(Z17, Z29, Z24, Z28, Z17)
-	MUL_W(Z17, Z17, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z17, Z27, Z30, Z28, Z29, Z26, Z17)
-	MUL_W(Z17, Z31, Z30, Z28, Z29, Z26, Z17)
-	REDUCE1Q(Z24, Z17, Z30)
-	VPBROADCASTD 72(CX), Z28
-	ADD(Z18, Z28, Z24, Z29, Z18)
-	MUL_W(Z18, Z18, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z18, Z30, Z27, Z29, Z28, Z31, Z18)
-	MUL_W(Z18, Z26, Z27, Z29, Z28, Z31, Z18)
-	REDUCE1Q(Z24, Z18, Z27)
-	VPBROADCASTD 76(CX), Z29
-	ADD(Z19, Z29, Z24, Z28, Z19)
-	MUL_W(Z19, Z19, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z19, Z27, Z30, Z28, Z29, Z26, Z19)
-	MUL_W(Z19, Z31, Z30, Z28, Z29, Z26, Z19)
-	REDUCE1Q(Z24, Z19, Z30)
-	VPBROADCASTD 80(CX), Z28
-	ADD(Z20, Z28, Z24, Z29, Z20)
-	MUL_W(Z20, Z20, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z20, Z30, Z27, Z29, Z28, Z31, Z20)
-	MUL_W(Z20, Z26, Z27, Z29, Z28, Z31, Z20)
-	REDUCE1Q(Z24, Z20, Z27)
-	VPBROADCASTD 84(CX), Z29
-	ADD(Z21, Z29, Z24, Z28, Z21)
-	MUL_W(Z21, Z21, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z21, Z27, Z30, Z28, Z29, Z26, Z21)
-	MUL_W(Z21, Z31, Z30, Z28, Z29, Z26, Z21)
-	REDUCE1Q(Z24, Z21, Z30)
-	VPBROADCASTD 88(CX), Z28
-	ADD(Z22, Z28, Z24, Z29, Z22)
-	MUL_W(Z22, Z22, Z31, Z27, Z29, Z28, Z26)
-	REDUCE1Q(Z24, Z26, Z31)
-	MUL_W(Z26, Z26, Z27, Z29, Z28, Z31, Z30)
-	MUL_W(Z22, Z30, Z27, Z29, Z28, Z31, Z22)
-	MUL_W(Z22, Z26, Z27, Z29, Z28, Z31, Z22)
-	REDUCE1Q(Z24, Z22, Z27)
-	VPBROADCASTD 92(CX), Z29
-	ADD(Z23, Z29, Z24, Z28, Z23)
-	MUL_W(Z23, Z23, Z26, Z30, Z28, Z29, Z31)
-	REDUCE1Q(Z24, Z31, Z26)
-	MUL_W(Z31, Z31, Z30, Z28, Z29, Z26, Z27)
-	MUL_W(Z23, Z27, Z30, Z28, Z29, Z26, Z23)
-	MUL_W(Z23, Z31, Z30, Z28, Z29, Z26, Z23)
-	REDUCE1Q(Z24, Z23, Z30)
-	ADD(Z0, Z1, Z24, Z27, Z28)
-	ADD(Z2, Z3, Z24, Z27, Z29)
-	ADD(Z28, Z29, Z24, Z27, Z26)
-	ADD(Z26, Z1, Z24, Z27, Z30)
-	ADD(Z26, Z3, Z24, Z27, Z31)
-	DOUBLE(Z0, Z24, Z27, Z3)
-	ADD(Z3, Z31, Z24, Z27, Z3)
-	DOUBLE(Z2, Z24, Z27, Z1)
-	ADD(Z1, Z30, Z24, Z27, Z1)
-	ADD(Z28, Z30, Z24, Z27, Z0)
-	ADD(Z29, Z31, Z24, Z27, Z2)
-	ADD(Z4, Z5, Z24, Z27, Z28)
-	ADD(Z6, Z7, Z24, Z27, Z29)
-	ADD(Z28, Z29, Z24, Z27, Z26)
-	ADD(Z26, Z5, Z24, Z27, Z30)
-	ADD(Z26, Z7, Z24, Z27, Z31)
-	DOUBLE(Z4, Z24, Z27, Z7)
-	ADD(Z7, Z31, Z24, Z27, Z7)
-	DOUBLE(Z6, Z24, Z27, Z5)
-	ADD(Z5, Z30, Z24, Z27, Z5)
-	ADD(Z28, Z30, Z24, Z27, Z4)
-	ADD(Z29, Z31, Z24, Z27, Z6)
-	ADD(Z8, Z9, Z24, Z27, Z28)
-	ADD(Z10, Z11, Z24, Z27, Z29)
-	ADD(Z28, Z29, Z24, Z27, Z26)
-	ADD(Z26, Z9, Z24, Z27, Z30)
-	ADD(Z26, Z11, Z24, Z27, Z31)
-	DOUBLE(Z8, Z24, Z27, Z11)
-	ADD(Z11, Z31, Z24, Z27, Z11)
-	DOUBLE(Z10, Z24, Z27, Z9)
-	ADD(Z9, Z30, Z24, Z27, Z9)
-	ADD(Z28, Z30, Z24, Z27, Z8)
-	ADD(Z29, Z31, Z24, Z27, Z10)
-	ADD(Z12, Z13, Z24, Z27, Z28)
-	ADD(Z14, Z15, Z24, Z27, Z29)
-	ADD(Z28, Z29, Z24, Z27, Z26)
-	ADD(Z26, Z13, Z24, Z27, Z30)
-	ADD(Z26, Z15, Z24, Z27, Z31)
-	DOUBLE(Z12, Z24, Z27, Z15)
-	ADD(Z15, Z31, Z24, Z27, Z15)
-	DOUBLE(Z14, Z24, Z27, Z13)
-	ADD(Z13, Z30, Z24, Z27, Z13)
-	ADD(Z28, Z30, Z24, Z27, Z12)
-	ADD(Z29, Z31, Z24, Z27, Z14)
-	ADD(Z16, Z17, Z24, Z27, Z28)
-	ADD(Z18, Z19, Z24, Z27, Z29)
-	ADD(Z28, Z29, Z24, Z27, Z26)
-	ADD(Z26, Z17, Z24, Z27, Z30)
-	ADD(Z26, Z19, Z24, Z27, Z31)
-	DOUBLE(Z16, Z24, Z27, Z19)
-	ADD(Z19, Z31, Z24, Z27, Z19)
-	DOUBLE(Z18, Z24, Z27, Z17)
-	ADD(Z17, Z30, Z24, Z27, Z17)
-	ADD(Z28, Z30, Z24, Z27, Z16)
-	ADD(Z29, Z31, Z24, Z27, Z18)
-	ADD(Z20, Z21, Z24, Z27, Z28)
-	ADD(Z22, Z23, Z24, Z27, Z29)
-	ADD(Z28, Z29, Z24, Z27, Z26)
-	ADD(Z26, Z21, Z24, Z27, Z30)
-	ADD(Z26, Z23, Z24, Z27, Z31)
-	DOUBLE(Z20, Z24, Z27, Z23)
-	ADD(Z23, Z31, Z24, Z27, Z23)
-	DOUBLE(Z22, Z24, Z27, Z21)
-	ADD(Z21, Z30, Z24, Z27, Z21)
-	ADD(Z28, Z30, Z24, Z27, Z20)
-	ADD(Z29, Z31, Z24, Z27, Z22)
-	ADD(Z0, Z4, Z24, Z30, Z27)
-	ADD(Z1, Z5, Z24, Z31, Z28)
-	ADD(Z2, Z6, Z24, Z30, Z29)
-	ADD(Z3, Z7, Z24, Z31, Z26)
-	ADD(Z8, Z27, Z24, Z30, Z27)
-	ADD(Z9, Z28, Z24, Z31, Z28)
-	ADD(Z10, Z29, Z24, Z30, Z29)
-	ADD(Z11, Z26, Z24, Z31, Z26)
-	ADD(Z12, Z27, Z24, Z30, Z27)
-	ADD(Z13, Z28, Z24, Z31, Z28)
-	ADD(Z14, Z29, Z24, Z30, Z29)
-	ADD(Z15, Z26, Z24, Z31, Z26)
-	ADD(Z16, Z27, Z24, Z30, Z27)
-	ADD(Z17, Z28, Z24, Z31, Z28)
-	ADD(Z18, Z29, Z24, Z30, Z29)
-	ADD(Z19, Z26, Z24, Z31, Z26)
-	ADD(Z20, Z27, Z24, Z30, Z27)
-	ADD(Z21, Z28, Z24, Z31, Z28)
-	ADD(Z22, Z29, Z24, Z30, Z29)
-	ADD(Z23, Z26, Z24, Z31, Z26)
-	ADD(Z0, Z27, Z24, Z30, Z0)
-	ADD(Z1, Z28, Z24, Z31, Z1)
-	ADD(Z2, Z29, Z24, Z30, Z2)
-	ADD(Z3, Z26, Z24, Z31, Z3)
-	ADD(Z4, Z27, Z24, Z30, Z4)
-	ADD(Z5, Z28, Z24, Z31, Z5)
-	ADD(Z6, Z29, Z24, Z30, Z6)
-	ADD(Z7, Z26, Z24, Z31, Z7)
-	ADD(Z8, Z27, Z24, Z30, Z8)
-	ADD(Z9, Z28, Z24, Z31, Z9)
-	ADD(Z10, Z29, Z24, Z30, Z10)
-	ADD(Z11, Z26, Z24, Z31, Z11)
-	ADD(Z12, Z27, Z24, Z30, Z12)
-	ADD(Z13, Z28, Z24, Z31, Z13)
-	ADD(Z14, Z29, Z24, Z30, Z14)
-	ADD(Z15, Z26, Z24, Z31, Z15)
-	ADD(Z16, Z27, Z24, Z30, Z16)
-	ADD(Z17, Z28, Z24, Z31, Z17)
-	ADD(Z18, Z29, Z24, Z30, Z18)
-	ADD(Z19, Z26, Z24, Z31, Z19)
-	ADD(Z20, Z27, Z24, Z30, Z20)
-	ADD(Z21, Z28, Z24, Z31, Z21)
-	ADD(Z22, Z29, Z24, Z30, Z22)
-	ADD(Z23, Z26, Z24, Z31, Z23)
-	ADDQ         $24, R14
-	JMP          loop_7
-
-done_8:
-	VMOVDQU32 Z0, 0(R15)
-	VMOVDQU32 Z1, 64(R15)
-	VMOVDQU32 Z2, 128(R15)
-	VMOVDQU32 Z3, 192(R15)
-	VMOVDQU32 Z4, 256(R15)
-	VMOVDQU32 Z5, 320(R15)
-	VMOVDQU32 Z6, 384(R15)
-	VMOVDQU32 Z7, 448(R15)
-	VMOVDQU32 Z8, 512(R15)
-	VMOVDQU32 Z9, 576(R15)
-	VMOVDQU32 Z10, 640(R15)
-	VMOVDQU32 Z11, 704(R15)
-	VMOVDQU32 Z12, 768(R15)
-	VMOVDQU32 Z13, 832(R15)
-	VMOVDQU32 Z14, 896(R15)
-	VMOVDQU32 Z15, 960(R15)
-	VMOVDQU32 Z16, 1024(R15)
-	VMOVDQU32 Z17, 1088(R15)
-	VMOVDQU32 Z18, 1152(R15)
-	VMOVDQU32 Z19, 1216(R15)
-	VMOVDQU32 Z20, 1280(R15)
-	VMOVDQU32 Z21, 1344(R15)
-	VMOVDQU32 Z22, 1408(R15)
-	VMOVDQU32 Z23, 1472(R15)
-	RET
-
-TEXT ·permutation16_avx512(SB), NOSPLIT, $0-48
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         $1, AX
-	KMOVQ        AX, K2
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z0
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z1
-	MOVQ         input+0(FP), R15
-	MOVQ         roundKeys+24(FP), R14
-	VMOVDQU32    0(R15), Z2
-	MOVQ         ·diag16+0(SB), CX
-	VMOVDQU32    0(CX), Z18
-	VPSRLQ       $32, Z18, Z19
-
-#define MAT_MUL_EXTERNAL_16() \
-	MAT_MUL_M4(Z2, Z6, Z7, Z8, Z0, Z11) \
-	VEXTRACTI64X4 $1, Z2, Y16           \
-	ADD(Y16, Y2, Y0, Y11, Y16)          \
-	VSHUFF64X2    $1, Y16, Y16, Y17     \
-	ADD(Y16, Y17, Y0, Y11, Y16)         \
-	VINSERTI64X4  $1, Y16, Z16, Z16     \
-	ADD(Z2, Z16, Z0, Z11, Z2)           \
-
-#define SBOX_FULL_16() \
-	MULD(Z2, Z2, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z8) \
-	REDUCE1Q(Z0, Z8, Z15)                                \
-	MULD(Z8, Z8, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z9) \
-	MULD(Z2, Z8, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z2) \
-	MULD(Z2, Z9, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z2) \
-	REDUCE1Q(Z0, Z2, Z15)                                \
-
-#define SUM_STATE_16() \
-	VEXTRACTI64X4 $1, Z2, Y16                   \
-	ADD(Y16, Y10, Y0, Y11, Y16)                 \
-	VSHUFF64X2    $1, Y16, Y16, Y17             \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VPSHUFD       $0x000000000000004e, Y16, Y17 \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VPSHUFD       $0x00000000000000b1, Y16, Y17 \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VINSERTI64X4  $1, Y16, Z16, Z16             \
-
-#define FULL_ROUND_16() \
-	VMOVDQU32 0(BX), Z4      \
-	ADD(Z2, Z4, Z0, Z11, Z2) \
-	SBOX_FULL_16()           \
-	MAT_MUL_EXTERNAL_16()    \
-
-	MAT_MUL_EXTERNAL_16()
-	MOVQ 0(R14), BX
-	FULL_ROUND_16()
-	MOVQ 24(R14), BX
-	FULL_ROUND_16()
-	MOVQ 48(R14), BX
-	FULL_ROUND_16()
-	MOVQ 72(R14), BX
-	FULL_ROUND_16()
-
-	// loop over the partial rounds
-	MOVQ $0x000000000000000d, SI // nb partial rounds --> 13
-	MOVQ R14, DI
-	ADDQ $0x0000000000000060, DI
-
-loop_9:
-	TESTQ     SI, SI
-	JEQ       done_10
-	DECQ      SI
-	MOVQ      0(DI), BX
-	VMOVD     0(BX), X4
-	VMOVDQA32 Z2, Z10
-	ADD(X10, X4, X0, X14, X5)
-	SBOX_PARTIAL()
-	VPBLENDMD Z5, Z10, K2, Z10
-	VPSRLQ    $32, Z2, Z12
-	VPMULUDQ  Z12, Z19, Z8
-	VPMULUDQ  Z8, Z1, Z15
-	VPMULUDQ  Z15, Z0, Z15
-	VPADDQ    Z8, Z15, Z8
-	SUM_STATE_16()
-	VPMULUDQ  Z10, Z18, Z6
-	VPMULUDQ  Z6, Z1, Z14
-	VPMULUDQ  Z14, Z0, Z14
-	VPADDQ    Z6, Z14, Z6
-	VMOVSHDUP Z6, K3, Z8
-	VPSUBD    Z0, Z8, Z11
-	VPMINUD   Z8, Z11, Z2
-	ADD(Z2, Z16, Z0, Z11, Z2)
-	ADDQ      $24, DI
-	JMP       loop_9
-
-done_10:
-	MOVQ      408(R14), BX
-	FULL_ROUND_16()
-	MOVQ      432(R14), BX
-	FULL_ROUND_16()
-	MOVQ      456(R14), BX
-	FULL_ROUND_16()
-	MOVQ      480(R14), BX
-	FULL_ROUND_16()
-	VMOVDQU32 Z2, 0(R15)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/babybear/sis/sis_amd64.s b/field/babybear/sis/sis_amd64.s
--- a/field/babybear/sis/sis_amd64.s
+++ b/field/babybear/sis/sis_amd64.s
@@ -1,693 +1,1 @@
-//go:build !purego
-
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-// Refer to the generator for more documentation.
-// Some sub-functions are derived from Plonky3:
-// https://github.com/Plonky3/Plonky3/blob/36e619f3c6526ee86e2e5639a24b3224e1c1700f/monty-31/src/x86_64_avx512/packing.rs#L319
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define BUTTERFLYD1Q(in0, in1, in2, in3, in4) \
-	VPADDD  in0, in1, in3 \
-	VPSUBD  in1, in0, in1 \
-	VPSUBD  in2, in3, in0 \
-	VPMINUD in3, in0, in0 \
-	VPADDD  in2, in1, in4 \
-	VPMINUD in4, in1, in1 \
-
-#define BUTTERFLYD2Q(in0, in1, in2, in3, in4) \
-	VPSUBD  in1, in0, in4 \
-	VPADDD  in0, in1, in3 \
-	VPADDD  in2, in4, in1 \
-	VPSUBD  in2, in3, in0 \
-	VPMINUD in3, in0, in0 \
-
-#define BUTTERFLYD2Q2Q(in0, in1, in2, in3) \
-	VPSUBD in1, in0, in3 \
-	VPADDD in0, in1, in0 \
-	VPADDD in2, in3, in1 \
-
-#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9) \
-	VPSRLQ    $32, in0, in2 \
-	VPSRLQ    $32, in1, in3 \
-	VPMULUDQ  in0, in1, in4 \
-	VPMULUDQ  in2, in3, in5 \
-	VPMULUDQ  in4, in9, in6 \
-	VPMULUDQ  in5, in9, in7 \
-	VPMULUDQ  in6, in8, in6 \
-	VPADDQ    in4, in6, in4 \
-	VPMULUDQ  in7, in8, in7 \
-	VPADDQ    in5, in7, in5 \
-	VMOVSHDUP in4, K3, in5  \
-	VPSUBD    in8, in5, in7 \
-	VPMINUD   in5, in7, in0 \
-
-#define PERMUTE8X8(in0, in1, in2) \
-	VSHUFI64X2 $0x000000000000004e, in1, in0, in2 \
-	VPBLENDMQ  in0, in2, K1, in0                  \
-	VPBLENDMQ  in2, in1, K1, in1                  \
-
-#define PERMUTE4X4(in0, in1, in2, in3) \
-	VMOVDQA64 in2, in3          \
-	VPERMI2Q  in1, in0, in3     \
-	VPBLENDMQ in0, in3, K2, in0 \
-	VPBLENDMQ in3, in1, K2, in1 \
-
-#define PERMUTE2X2(in0, in1, in2) \
-	VSHUFPD   $0x0000000000000055, in1, in0, in2 \
-	VPBLENDMQ in0, in2, K3, in0                  \
-	VPBLENDMQ in2, in1, K3, in1                  \
-
-#define PERMUTE1X1(in0, in1, in2) \
-	VPSHRDQ   $32, in1, in0, in2 \
-	VPBLENDMD in0, in2, K3, in0  \
-	VPBLENDMD in2, in1, K3, in1  \
-
-#define LOAD_Q(in0, in1) \
-	MOVD         $const_q, AX       \
-	VPBROADCASTD AX, in0            \
-	MOVD         $const_qInvNeg, AX \
-	VPBROADCASTD AX, in1            \
-
-#define LOAD_MASKS() \
-	MOVQ  $0x0000000000000f0f, AX \
-	KMOVQ AX, K1                  \
-	MOVQ  $0x0000000000000033, AX \
-	KMOVQ AX, K2                  \
-	MOVQ  $0x0000000000005555, AX \
-	KMOVD AX, K3                  \
-
-#define BUTTERFLY_MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11, in12, in13, in14) \
-BUTTERFLYD2Q(in0, in1, in2, in3, in4)                       \
-MULD(in5, in6, in7, in8, in9, in10, in11, in12, in13, in14) \
-
-TEXT ·sis512_16_avx512(SB), $1024-120
-	// refer to the code generator for comments and documentation.
-	LOAD_Q(Z0, Z1)
-	LOAD_MASKS()
-	MOVQ k256+0(FP), R15
-	MOVQ cosets+24(FP), CX
-	MOVQ twiddles+48(FP), SI
-	MOVQ rag+72(FP), R8
-	MOVQ res+96(FP), R9
-	MOVQ 0(SI), DI               // twiddles[0]
-	MOVQ R15, DX
-	MOVQ CX, BX
-	ADDQ $0x0000000000000200, DX
-	ADDQ $0x0000000000000400, BX
-
-#define FROMMONTGOMERY(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11) \
-	VPSRLQ    $32, in0, in3     \
-	VPSRLQ    $32, in1, in7     \
-	VPMULUDQ  in0, in11, in4    \
-	VPMULUDQ  in3, in11, in5    \
-	VPMULUDQ  in1, in11, in8    \
-	VPMULUDQ  in7, in11, in9    \
-	VPMULUDQ  in4, in10, in4    \
-	VPMULUDQ  in8, in10, in8    \
-	VPMULUDQ  in5, in10, in5    \
-	VPMULUDQ  in9, in10, in9    \
-	VPANDD.Z  in0, in0, K3, in2 \
-	VPANDD.Z  in1, in1, K3, in6 \
-	VPADDQ    in2, in4, in2     \
-	VPADDQ    in6, in8, in6     \
-	VPADDQ    in3, in5, in3     \
-	VPADDQ    in7, in9, in7     \
-	VMOVSHDUP in6, K3, in7      \
-	VMOVSHDUP in2, K3, in3      \
-	VPSUBD    in10, in3, in5    \
-	VPSUBD    in10, in7, in9    \
-	VPMINUD   in3, in5, in0     \
-	VPMINUD   in7, in9, in1     \
-
-	VMOVDQU32     0(R15), Z16
-	VMOVDQU32     0(DX), Z14
-	FROMMONTGOMERY(Z16, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z16, Y17
-	VPMOVZXWD     Y16, Z16
-	VPMOVZXWD     Y17, Z17
-	VMOVDQU32     0(CX), Z12
-	MULD(Z16, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     64(CX), Z13
-	MULD(Z17, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     0(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     64(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z16, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z17, Z15, Z0, Z5, Z7)
-	VMOVDQU32     0(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     64(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 0(SP)
-	VMOVDQU32     Z15, 64(SP)
-	VMOVDQU32     64(R15), Z18
-	VMOVDQU32     64(DX), Z14
-	FROMMONTGOMERY(Z18, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z18, Y19
-	VPMOVZXWD     Y18, Z18
-	VPMOVZXWD     Y19, Z19
-	VMOVDQU32     128(CX), Z12
-	MULD(Z18, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     192(CX), Z13
-	MULD(Z19, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     128(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     192(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z18, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z19, Z15, Z0, Z5, Z7)
-	VMOVDQU32     128(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     192(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 128(SP)
-	VMOVDQU32     Z15, 192(SP)
-	VMOVDQU32     128(R15), Z20
-	VMOVDQU32     128(DX), Z14
-	FROMMONTGOMERY(Z20, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z20, Y21
-	VPMOVZXWD     Y20, Z20
-	VPMOVZXWD     Y21, Z21
-	VMOVDQU32     256(CX), Z12
-	MULD(Z20, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     320(CX), Z13
-	MULD(Z21, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     256(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     320(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z20, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z21, Z15, Z0, Z5, Z7)
-	VMOVDQU32     256(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     320(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 256(SP)
-	VMOVDQU32     Z15, 320(SP)
-	VMOVDQU32     192(R15), Z22
-	VMOVDQU32     192(DX), Z14
-	FROMMONTGOMERY(Z22, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z22, Y23
-	VPMOVZXWD     Y22, Z22
-	VPMOVZXWD     Y23, Z23
-	VMOVDQU32     384(CX), Z12
-	MULD(Z22, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     448(CX), Z13
-	MULD(Z23, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     384(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     448(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z22, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z23, Z15, Z0, Z5, Z7)
-	VMOVDQU32     384(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     448(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 384(SP)
-	VMOVDQU32     Z15, 448(SP)
-	VMOVDQU32     256(R15), Z24
-	VMOVDQU32     256(DX), Z14
-	FROMMONTGOMERY(Z24, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z24, Y25
-	VPMOVZXWD     Y24, Z24
-	VPMOVZXWD     Y25, Z25
-	VMOVDQU32     512(CX), Z12
-	MULD(Z24, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     576(CX), Z13
-	MULD(Z25, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     512(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     576(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z24, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z25, Z15, Z0, Z5, Z7)
-	VMOVDQU32     512(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     576(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 512(SP)
-	VMOVDQU32     Z15, 576(SP)
-	VMOVDQU32     320(R15), Z26
-	VMOVDQU32     320(DX), Z14
-	FROMMONTGOMERY(Z26, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z26, Y27
-	VPMOVZXWD     Y26, Z26
-	VPMOVZXWD     Y27, Z27
-	VMOVDQU32     640(CX), Z12
-	MULD(Z26, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     704(CX), Z13
-	MULD(Z27, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     640(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     704(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z26, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z27, Z15, Z0, Z5, Z7)
-	VMOVDQU32     640(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     704(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 640(SP)
-	VMOVDQU32     Z15, 704(SP)
-	VMOVDQU32     384(R15), Z28
-	VMOVDQU32     384(DX), Z14
-	FROMMONTGOMERY(Z28, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z28, Y29
-	VPMOVZXWD     Y28, Z28
-	VPMOVZXWD     Y29, Z29
-	VMOVDQU32     768(CX), Z12
-	MULD(Z28, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     832(CX), Z13
-	MULD(Z29, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     768(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     832(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z28, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z29, Z15, Z0, Z5, Z7)
-	VMOVDQU32     768(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     832(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 768(SP)
-	VMOVDQU32     Z15, 832(SP)
-	VMOVDQU32     448(R15), Z30
-	VMOVDQU32     448(DX), Z14
-	FROMMONTGOMERY(Z30, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z30, Y31
-	VPMOVZXWD     Y30, Z30
-	VPMOVZXWD     Y31, Z31
-	VMOVDQU32     896(CX), Z12
-	MULD(Z30, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     960(CX), Z13
-	MULD(Z31, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     896(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     960(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z30, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z31, Z15, Z0, Z5, Z7)
-	VMOVDQU32     896(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     960(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 896(SP)
-	VMOVDQU32     Z15, 960(SP)
-	ADDQ          $24, SI
-	MOVQ          SI, DI
-	MOVQ          $2, R10
-
-fft256_2:
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Z2
-	VMOVDQU32    64(R11), Z3
-	VMOVDQU32    128(R11), Z4
-	VMOVDQU32    192(R11), Z5
-	VMOVDQU32    256(R11), Z6
-	VMOVDQU32    320(R11), Z7
-	VMOVDQU32    384(R11), Z8
-	VMOVDQU32    448(R11), Z9
-	BUTTERFLYD2Q(Z16, Z24, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z17, Z25, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z18, Z26, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z19, Z27, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z20, Z28, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z21, Z29, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z22, Z30, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z23, Z31, Z0, Z14, Z11)
-	MULD(Z24, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z25, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z26, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z5, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z28, Z6, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z7, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z30, Z8, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z9, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	ADDQ         $24, SI
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Z2
-	VMOVDQU32    64(R11), Z3
-	VMOVDQU32    128(R11), Z4
-	VMOVDQU32    192(R11), Z5
-	BUTTERFLYD2Q(Z16, Z20, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z17, Z21, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z18, Z22, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z19, Z23, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z24, Z28, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z25, Z29, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z26, Z30, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z27, Z31, Z0, Z14, Z11)
-	MULD(Z20, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z21, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z22, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z5, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z28, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z30, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z5, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	ADDQ         $24, SI
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Z2
-	VMOVDQU32    64(R11), Z3
-	BUTTERFLYD2Q(Z16, Z18, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z17, Z19, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z20, Z22, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z21, Z23, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z24, Z26, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z25, Z27, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z28, Z30, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z29, Z31, Z0, Z14, Z11)
-	MULD(Z18, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z19, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z22, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z26, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z30, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	ADDQ         $24, SI
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Z2
-	BUTTERFLYD2Q(Z16, Z17, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z18, Z19, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z20, Z21, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z22, Z23, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z24, Z25, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z26, Z27, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z28, Z29, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z30, Z31, Z0, Z14, Z11)
-	MULD(Z17, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z19, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z21, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z25, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	ADDQ         $24, SI
-	MOVQ         ·vInterleaveIndices+0(SB), R12
-	VMOVDQU64    0(R12), Z6
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Y2
-	VINSERTI64X4 $1, Y2, Z2, Z2
-	MOVQ         24(SI), R11
-	VMOVDQU32    0(R11), X3
-	VINSERTI64X2 $1, X3, Z3, Z3
-	VINSERTI64X2 $0x0000000000000002, X3, Z3, Z3
-	VINSERTI64X2 $0x0000000000000003, X3, Z3, Z3
-	MOVQ         48(SI), R11
-	VPBROADCASTD 0(R11), Z4
-	VPBROADCASTD 4(R11), Z5
-	VPBLENDMD    Z4, Z5, K3, Z4
-	PERMUTE8X8(Z16, Z17, Z10)
-	BUTTERFLYD2Q(Z16, Z17, Z0, Z14, Z11)
-	PERMUTE8X8(Z18, Z19, Z10)
-	BUTTERFLYD2Q(Z18, Z19, Z0, Z14, Z11)
-	PERMUTE8X8(Z20, Z21, Z10)
-	BUTTERFLYD2Q(Z20, Z21, Z0, Z14, Z11)
-	PERMUTE8X8(Z22, Z23, Z10)
-	BUTTERFLYD2Q(Z22, Z23, Z0, Z14, Z11)
-	PERMUTE8X8(Z24, Z25, Z10)
-	BUTTERFLYD2Q(Z24, Z25, Z0, Z14, Z11)
-	PERMUTE8X8(Z26, Z27, Z10)
-	BUTTERFLYD2Q(Z26, Z27, Z0, Z14, Z11)
-	PERMUTE8X8(Z28, Z29, Z10)
-	BUTTERFLYD2Q(Z28, Z29, Z0, Z14, Z11)
-	PERMUTE8X8(Z30, Z31, Z10)
-	BUTTERFLYD2Q(Z30, Z31, Z0, Z14, Z11)
-	MULD(Z17, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z19, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z21, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z25, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE4X4(Z16, Z17, Z6, Z10)
-	BUTTERFLYD2Q(Z16, Z17, Z0, Z14, Z11)
-	PERMUTE4X4(Z18, Z19, Z6, Z10)
-	BUTTERFLYD2Q(Z18, Z19, Z0, Z14, Z11)
-	PERMUTE4X4(Z20, Z21, Z6, Z10)
-	BUTTERFLYD2Q(Z20, Z21, Z0, Z14, Z11)
-	PERMUTE4X4(Z22, Z23, Z6, Z10)
-	BUTTERFLYD2Q(Z22, Z23, Z0, Z14, Z11)
-	PERMUTE4X4(Z24, Z25, Z6, Z10)
-	BUTTERFLYD2Q(Z24, Z25, Z0, Z14, Z11)
-	PERMUTE4X4(Z26, Z27, Z6, Z10)
-	BUTTERFLYD2Q(Z26, Z27, Z0, Z14, Z11)
-	PERMUTE4X4(Z28, Z29, Z6, Z10)
-	BUTTERFLYD2Q(Z28, Z29, Z0, Z14, Z11)
-	PERMUTE4X4(Z30, Z31, Z6, Z10)
-	BUTTERFLYD2Q(Z30, Z31, Z0, Z14, Z11)
-	MULD(Z17, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z19, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z21, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z25, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE2X2(Z16, Z17, Z10)
-	BUTTERFLYD2Q(Z16, Z17, Z0, Z14, Z11)
-	PERMUTE2X2(Z18, Z19, Z10)
-	BUTTERFLYD2Q(Z18, Z19, Z0, Z14, Z11)
-	PERMUTE2X2(Z20, Z21, Z10)
-	BUTTERFLYD2Q(Z20, Z21, Z0, Z14, Z11)
-	PERMUTE2X2(Z22, Z23, Z10)
-	BUTTERFLYD2Q(Z22, Z23, Z0, Z14, Z11)
-	PERMUTE2X2(Z24, Z25, Z10)
-	BUTTERFLYD2Q(Z24, Z25, Z0, Z14, Z11)
-	PERMUTE2X2(Z26, Z27, Z10)
-	BUTTERFLYD2Q(Z26, Z27, Z0, Z14, Z11)
-	PERMUTE2X2(Z28, Z29, Z10)
-	BUTTERFLYD2Q(Z28, Z29, Z0, Z14, Z11)
-	PERMUTE2X2(Z30, Z31, Z10)
-	BUTTERFLYD2Q(Z30, Z31, Z0, Z14, Z11)
-	MULD(Z17, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z16, Z17, Z10)
-	MULD(Z19, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z18, Z19, Z10)
-	MULD(Z21, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z20, Z21, Z10)
-	MULD(Z23, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z22, Z23, Z10)
-	MULD(Z25, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z24, Z25, Z10)
-	MULD(Z27, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z26, Z27, Z10)
-	MULD(Z29, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z28, Z29, Z10)
-	MULD(Z31, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z30, Z31, Z10)
-	BUTTERFLYD2Q2Q(Z16, Z17, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z18, Z19, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z20, Z21, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z22, Z23, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z24, Z25, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z26, Z27, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z28, Z29, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z30, Z31, Z0, Z10)
-	VMOVDQU32    0(R8), Z7
-	MULD(Z16, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    64(R8), Z8
-	MULD(Z17, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    128(R8), Z7
-	MULD(Z18, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    192(R8), Z8
-	MULD(Z19, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    256(R8), Z7
-	MULD(Z20, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    320(R8), Z8
-	MULD(Z21, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    384(R8), Z7
-	MULD(Z22, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    448(R8), Z8
-	MULD(Z23, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    512(R8), Z7
-	MULD(Z24, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    576(R8), Z8
-	MULD(Z25, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    640(R8), Z7
-	MULD(Z26, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    704(R8), Z8
-	MULD(Z27, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    768(R8), Z7
-	MULD(Z28, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    832(R8), Z8
-	MULD(Z29, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    896(R8), Z7
-	MULD(Z30, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    960(R8), Z8
-	MULD(Z31, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VPADDD       0(R9), Z16, Z16
-	VPADDD       64(R9), Z17, Z17
-	VPSUBD       Z0, Z16, Z5
-	VPSUBD       Z0, Z17, Z10
-	VPMINUD      Z5, Z16, Z16
-	VPMINUD      Z10, Z17, Z17
-	VMOVDQU32    Z16, 0(R9)
-	VMOVDQU32    Z17, 64(R9)
-	VPADDD       128(R9), Z18, Z18
-	VPADDD       192(R9), Z19, Z19
-	VPSUBD       Z0, Z18, Z5
-	VPSUBD       Z0, Z19, Z10
-	VPMINUD      Z5, Z18, Z18
-	VPMINUD      Z10, Z19, Z19
-	VMOVDQU32    Z18, 128(R9)
-	VMOVDQU32    Z19, 192(R9)
-	VPADDD       256(R9), Z20, Z20
-	VPADDD       320(R9), Z21, Z21
-	VPSUBD       Z0, Z20, Z5
-	VPSUBD       Z0, Z21, Z10
-	VPMINUD      Z5, Z20, Z20
-	VPMINUD      Z10, Z21, Z21
-	VMOVDQU32    Z20, 256(R9)
-	VMOVDQU32    Z21, 320(R9)
-	VPADDD       384(R9), Z22, Z22
-	VPADDD       448(R9), Z23, Z23
-	VPSUBD       Z0, Z22, Z5
-	VPSUBD       Z0, Z23, Z10
-	VPMINUD      Z5, Z22, Z22
-	VPMINUD      Z10, Z23, Z23
-	VMOVDQU32    Z22, 384(R9)
-	VMOVDQU32    Z23, 448(R9)
-	VPADDD       512(R9), Z24, Z24
-	VPADDD       576(R9), Z25, Z25
-	VPSUBD       Z0, Z24, Z5
-	VPSUBD       Z0, Z25, Z10
-	VPMINUD      Z5, Z24, Z24
-	VPMINUD      Z10, Z25, Z25
-	VMOVDQU32    Z24, 512(R9)
-	VMOVDQU32    Z25, 576(R9)
-	VPADDD       640(R9), Z26, Z26
-	VPADDD       704(R9), Z27, Z27
-	VPSUBD       Z0, Z26, Z5
-	VPSUBD       Z0, Z27, Z10
-	VPMINUD      Z5, Z26, Z26
-	VPMINUD      Z10, Z27, Z27
-	VMOVDQU32    Z26, 640(R9)
-	VMOVDQU32    Z27, 704(R9)
-	VPADDD       768(R9), Z28, Z28
-	VPADDD       832(R9), Z29, Z29
-	VPSUBD       Z0, Z28, Z5
-	VPSUBD       Z0, Z29, Z10
-	VPMINUD      Z5, Z28, Z28
-	VPMINUD      Z10, Z29, Z29
-	VMOVDQU32    Z28, 768(R9)
-	VMOVDQU32    Z29, 832(R9)
-	VPADDD       896(R9), Z30, Z30
-	VPADDD       960(R9), Z31, Z31
-	VPSUBD       Z0, Z30, Z5
-	VPSUBD       Z0, Z31, Z10
-	VPMINUD      Z5, Z30, Z30
-	VPMINUD      Z10, Z31, Z31
-	VMOVDQU32    Z30, 896(R9)
-	VMOVDQU32    Z31, 960(R9)
-	DECQ         R10
-	TESTQ        R10, R10
-	JEQ          done_1
-	MOVQ         DI, SI
-	VMOVDQU32    0(SP), Z16
-	VMOVDQU32    64(SP), Z17
-	VMOVDQU32    128(SP), Z18
-	VMOVDQU32    192(SP), Z19
-	VMOVDQU32    256(SP), Z20
-	VMOVDQU32    320(SP), Z21
-	VMOVDQU32    384(SP), Z22
-	VMOVDQU32    448(SP), Z23
-	VMOVDQU32    512(SP), Z24
-	VMOVDQU32    576(SP), Z25
-	VMOVDQU32    640(SP), Z26
-	VMOVDQU32    704(SP), Z27
-	VMOVDQU32    768(SP), Z28
-	VMOVDQU32    832(SP), Z29
-	VMOVDQU32    896(SP), Z30
-	VMOVDQU32    960(SP), Z31
-	ADDQ         $0x0000000000000400, R8
-	ADDQ         $0x0000000000000400, R9
-	JMP          fft256_2
-
-done_1:
-	RET
-
-TEXT ·sisShuffle_avx512(SB), NOSPLIT, $0-24
-	MOVQ      a+0(FP), R15
-	MOVQ      a_len+8(FP), DX
-	SHRQ      $5, DX
-	LOAD_MASKS()
-	MOVQ      ·vInterleaveIndices+0(SB), CX
-	VMOVDQU64 0(CX), Z3
-
-loop_3:
-	TESTQ     DX, DX
-	JEQ       done_4
-	DECQ      DX
-	VMOVDQU32 0(R15), Z1  // load a[i]
-	VMOVDQU32 64(R15), Z2 // load a[i+16]
-	PERMUTE8X8(Z1, Z2, Z0)
-	PERMUTE4X4(Z1, Z2, Z3, Z0)
-	PERMUTE2X2(Z1, Z2, Z0)
-	PERMUTE1X1(Z1, Z2, Z0)
-	VMOVDQU32 Z1, 0(R15)  // store a[i]
-	VMOVDQU32 Z2, 64(R15) // store a[i+16]
-	ADDQ      $128, R15
-	JMP       loop_3
-
-done_4:
-	RET
-
-TEXT ·sisUnshuffle_avx512(SB), NOSPLIT, $0-24
-	MOVQ      a+0(FP), R15
-	MOVQ      a_len+8(FP), DX
-	SHRQ      $5, DX
-	LOAD_MASKS()
-	MOVQ      ·vInterleaveIndices+0(SB), CX
-	VMOVDQU64 0(CX), Z3
-
-loop_5:
-	TESTQ      DX, DX
-	JEQ        done_6
-	DECQ       DX
-	VMOVDQU32  0(R15), Z1  // load a[i]
-	VMOVDQU32  64(R15), Z2 // load a[i+16]
-	VPUNPCKLDQ Z2, Z1, Z0
-	VPUNPCKHDQ Z2, Z1, Z2
-	VMOVDQA32  Z0, Z1
-	PERMUTE4X4(Z1, Z2, Z3, Z0)
-	PERMUTE8X8(Z1, Z2, Z0)
-	VMOVDQU32  Z1, 0(R15)  // store a[i]
-	VMOVDQU32  Z2, 64(R15) // store a[i+16]
-	ADDQ       $128, R15
-	JMP        loop_5
-
-done_6:
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/koalabear/element_amd64.s b/field/koalabear/element_amd64.s
--- a/field/koalabear/element_amd64.s
+++ b/field/koalabear/element_amd64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 14212064141731379453
-#include "../asm/element_31b/element_31b_amd64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/field/koalabear/element_arm64.s b/field/koalabear/element_arm64.s
--- a/field/koalabear/element_arm64.s
+++ b/field/koalabear/element_arm64.s
@@ -1,10 +1,1 @@
-//go:build  !purego
-
-// Copyright 2020-2025 Consensys Software Inc.
-// Licensed under the Apache License, Version 2.0. See the LICENSE file for details.
-
-// Code generated by consensys/gnark-crypto DO NOT EDIT
-
-// We include the hash to force the Go compiler to recompile: 8620676634583589757
-#include "../asm/element_31b/element_31b_arm64.s"
-
+// assembly disabled for bazel compatibility
diff -ruN a/field/koalabear/extensions/e4_amd64.s b/field/koalabear/extensions/e4_amd64.s
--- a/field/koalabear/extensions/e4_amd64.s
+++ b/field/koalabear/extensions/e4_amd64.s
@@ -1,59 +1,1 @@
-//go:build !purego
-
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-// Refer to the generator for more documentation.
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-TEXT ·mulAccE4_avx512(SB), NOSPLIT, $0-32
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z0
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z1
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         alpha+0(FP), R14
-	MOVQ         scale+8(FP), R15
-	MOVQ         res+16(FP), CX
-	MOVQ         N+24(FP), BX
-	VMOVDQU32    0(R14), X2
-	VINSERTI64X2 $1, X2, Y2, Y2
-	VINSERTI64X4 $1, Y2, Z2, Z2
-	VPSRLQ       $32, Z2, Z3
-	SHRQ         $2, BX
-
-loop_1:
-	TESTQ        BX, BX
-	JEQ          done_2
-	DECQ         BX
-	VMOVDQU32    0(CX), Z4
-	VPBROADCASTD 0(R15), X5
-	VPBROADCASTD 4(R15), X6
-	VPBROADCASTD 8(R15), X7
-	VPBROADCASTD 12(R15), X8
-	VINSERTI64X2 $1, X6, Y5, Y5
-	VINSERTI64X2 $1, X8, Y7, Y7
-	VINSERTI64X4 $1, Y7, Z5, Z5
-	VPMULUDQ     Z5, Z2, Z10
-	VPMULUDQ     Z5, Z3, Z11
-	VPMULUDQ     Z10, Z1, Z12
-	VPMULUDQ     Z11, Z1, Z13
-	VPMULUDQ     Z12, Z0, Z12
-	VPADDQ       Z10, Z12, Z10
-	VPMULUDQ     Z13, Z0, Z13
-	VPADDQ       Z11, Z13, Z11
-	VMOVSHDUP    Z10, K3, Z11
-	VPSUBD       Z0, Z11, Z12
-	VPMINUD      Z11, Z12, Z9
-	VPADDD       Z4, Z9, Z4     // result = result + acc
-	VPSUBD       Z0, Z4, Z9
-	VPMINUD      Z4, Z9, Z4
-	VMOVDQU32    Z4, 0(CX)
-	ADDQ         $64, CX
-	ADDQ         $16, R15
-	JMP          loop_1
-
-done_2:
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/koalabear/fft/kernel_amd64.s b/field/koalabear/fft/kernel_amd64.s
--- a/field/koalabear/fft/kernel_amd64.s
+++ b/field/koalabear/fft/kernel_amd64.s
@@ -1,692 +1,1 @@
-//go:build !purego
-
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-// Refer to the generator for more documentation.
-// Some sub-functions are derived from Plonky3:
-// https://github.com/Plonky3/Plonky3/blob/36e619f3c6526ee86e2e5639a24b3224e1c1700f/monty-31/src/x86_64_avx512/packing.rs#L319
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define BUTTERFLYD1Q(in0, in1, in2, in3, in4) \
-	VPADDD  in0, in1, in3 \
-	VPSUBD  in1, in0, in1 \
-	VPSUBD  in2, in3, in0 \
-	VPMINUD in3, in0, in0 \
-	VPADDD  in2, in1, in4 \
-	VPMINUD in4, in1, in1 \
-
-#define BUTTERFLYD2Q(in0, in1, in2, in3, in4) \
-	VPSUBD  in1, in0, in4 \
-	VPADDD  in0, in1, in3 \
-	VPADDD  in2, in4, in1 \
-	VPSUBD  in2, in3, in0 \
-	VPMINUD in3, in0, in0 \
-
-#define BUTTERFLYD2Q2Q(in0, in1, in2, in3) \
-	VPSUBD in1, in0, in3 \
-	VPADDD in0, in1, in0 \
-	VPADDD in2, in3, in1 \
-
-#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9) \
-	VPSRLQ    $32, in0, in2 \
-	VPSRLQ    $32, in1, in3 \
-	VPMULUDQ  in0, in1, in4 \
-	VPMULUDQ  in2, in3, in5 \
-	VPMULUDQ  in4, in9, in6 \
-	VPMULUDQ  in5, in9, in7 \
-	VPMULUDQ  in6, in8, in6 \
-	VPADDQ    in4, in6, in4 \
-	VPMULUDQ  in7, in8, in7 \
-	VPADDQ    in5, in7, in5 \
-	VMOVSHDUP in4, K3, in5  \
-	VPSUBD    in8, in5, in7 \
-	VPMINUD   in5, in7, in0 \
-
-#define PERMUTE8X8(in0, in1, in2) \
-	VSHUFI64X2 $0x000000000000004e, in1, in0, in2 \
-	VPBLENDMQ  in0, in2, K1, in0                  \
-	VPBLENDMQ  in2, in1, K1, in1                  \
-
-#define PERMUTE4X4(in0, in1, in2, in3) \
-	VMOVDQA64 in2, in3          \
-	VPERMI2Q  in1, in0, in3     \
-	VPBLENDMQ in0, in3, K2, in0 \
-	VPBLENDMQ in3, in1, K2, in1 \
-
-#define PERMUTE2X2(in0, in1, in2) \
-	VSHUFPD   $0x0000000000000055, in1, in0, in2 \
-	VPBLENDMQ in0, in2, K3, in0                  \
-	VPBLENDMQ in2, in1, K3, in1                  \
-
-#define PERMUTE1X1(in0, in1, in2) \
-	VPSHRDQ   $32, in1, in0, in2 \
-	VPBLENDMD in0, in2, K3, in0  \
-	VPBLENDMD in2, in1, K3, in1  \
-
-#define LOAD_Q(in0, in1) \
-	MOVD         $const_q, AX       \
-	VPBROADCASTD AX, in0            \
-	MOVD         $const_qInvNeg, AX \
-	VPBROADCASTD AX, in1            \
-
-#define LOAD_MASKS() \
-	MOVQ  $0x0000000000000f0f, AX \
-	KMOVQ AX, K1                  \
-	MOVQ  $0x0000000000000033, AX \
-	KMOVQ AX, K2                  \
-	MOVQ  $0x0000000000005555, AX \
-	KMOVD AX, K3                  \
-
-#define BUTTERFLY_MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11, in12, in13, in14) \
-BUTTERFLYD2Q(in0, in1, in2, in3, in4)                       \
-MULD(in5, in6, in7, in8, in9, in10, in11, in12, in13, in14) \
-
-TEXT ·innerDITWithTwiddles_avx512(SB), NOSPLIT, $0-40
-	// refer to the code generator for comments and documentation.
-	LOAD_Q(Z4, Z5)
-	LOAD_MASKS()
-	MOVQ a+0(FP), R15
-	MOVQ twiddles+8(FP), DX
-	MOVQ end+24(FP), CX
-	MOVQ m+32(FP), BX
-	SHRQ $4, CX             // we are processing 16 elements at a time
-	SHLQ $2, BX             // offset = m * 4bytes
-	MOVQ R15, SI
-	ADDQ BX, SI
-
-loop_1:
-	TESTQ     CX, CX
-	JEQ       done_2
-	DECQ      CX
-	VMOVDQU32 0(R15), Z0 // load a[i]
-	VMOVDQU32 0(SI), Z1  // load a[i+m]
-	VMOVDQU32 0(DX), Z6  // load twiddles[i]
-	MULD(Z1, Z6, Z7, Z8, Z2, Z3, Z9, Z10, Z4, Z5)
-	BUTTERFLYD1Q(Z0, Z1, Z4, Z2, Z3)
-	VMOVDQU32 Z0, 0(R15) // store a[i]
-	VMOVDQU32 Z1, 0(SI)  // store a[i+m]
-	ADDQ      $64, R15
-	ADDQ      $64, SI
-	ADDQ      $64, DX
-	JMP       loop_1
-
-done_2:
-	RET
-
-TEXT ·innerDIFWithTwiddles_avx512(SB), NOSPLIT, $0-40
-	// refer to the code generator for comments and documentation.
-	MOVQ a+0(FP), R15
-	MOVQ twiddles+8(FP), DX
-	MOVQ end+24(FP), CX
-	MOVQ m+32(FP), BX
-	LOAD_Q(Z2, Z4)
-	LOAD_MASKS()
-	SHLQ $2, BX             // offset = m * 4bytes
-	MOVQ R15, SI
-	ADDQ BX, SI
-	SHRQ $4, CX             // we are processing 16 elements at a time
-
-loop_3:
-	TESTQ     CX, CX
-	JEQ       done_4
-	DECQ      CX
-	VMOVDQU32 0(R15), Z0 // load a[i]
-	VMOVDQU32 0(SI), Z1  // load a[i+m]
-	VMOVDQU32 0(DX), Z5  // load twiddles[i]
-	BUTTERFLY_MULD(Z0, Z1, Z2, Z3, Z8, Z1, Z5, Z6, Z7, Z3, Z8, Z9, Z10, Z2, Z4)
-	VMOVDQU32 Z0, 0(R15) // store a[i]
-	VMOVDQU32 Z1, 0(SI)
-	ADDQ      $64, R15
-	ADDQ      $64, SI
-	ADDQ      $64, DX
-	JMP       loop_3
-
-done_4:
-	RET
-
-TEXT ·kerDIFNP_256_avx512(SB), NOSPLIT, $0-56
-	// refer to the code generator for comments and documentation.
-	LOAD_Q(Z16, Z17)
-	LOAD_MASKS()
-
-	// load arguments
-	MOVQ         a+0(FP), R15
-	MOVQ         twiddles+24(FP), CX
-	MOVQ         stage+48(FP), AX
-	IMULQ        $24, AX
-	ADDQ         AX, CX                             // we want twiddles[stage] as starting point
-	VMOVDQU32    0(R15), Z0                         // load a[0]
-	VMOVDQU32    64(R15), Z1                        // load a[1]
-	VMOVDQU32    128(R15), Z2                       // load a[2]
-	VMOVDQU32    192(R15), Z3                       // load a[3]
-	VMOVDQU32    256(R15), Z4                       // load a[4]
-	VMOVDQU32    320(R15), Z5                       // load a[5]
-	VMOVDQU32    384(R15), Z6                       // load a[6]
-	VMOVDQU32    448(R15), Z7                       // load a[7]
-	VMOVDQU32    512(R15), Z8                       // load a[8]
-	VMOVDQU32    576(R15), Z9                       // load a[9]
-	VMOVDQU32    640(R15), Z10                      // load a[10]
-	VMOVDQU32    704(R15), Z11                      // load a[11]
-	VMOVDQU32    768(R15), Z12                      // load a[12]
-	VMOVDQU32    832(R15), Z13                      // load a[13]
-	VMOVDQU32    896(R15), Z14                      // load a[14]
-	VMOVDQU32    960(R15), Z15                      // load a[15]
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z18
-	VMOVDQU32    64(DI), Z19
-	VMOVDQU32    128(DI), Z20
-	VMOVDQU32    192(DI), Z21
-	VMOVDQU32    256(DI), Z22
-	VMOVDQU32    320(DI), Z23
-	VMOVDQU32    384(DI), Z24
-	VMOVDQU32    448(DI), Z25
-	BUTTERFLYD2Q(Z0, Z8, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z1, Z9, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z2, Z10, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z3, Z11, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z4, Z12, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z5, Z13, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z6, Z14, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z7, Z15, Z16, Z31, Z27)
-	MULD(Z8, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z9, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z10, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z12, Z22, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z23, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z14, Z24, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z25, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	ADDQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z18
-	VMOVDQU32    64(DI), Z19
-	VMOVDQU32    128(DI), Z20
-	VMOVDQU32    192(DI), Z21
-	BUTTERFLYD2Q(Z0, Z4, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z1, Z5, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z2, Z6, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z3, Z7, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z8, Z12, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z9, Z13, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z10, Z14, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z11, Z15, Z16, Z31, Z27)
-	MULD(Z4, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z5, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z6, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z12, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z14, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z21, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	ADDQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z18
-	VMOVDQU32    64(DI), Z19
-	BUTTERFLYD2Q(Z0, Z2, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z1, Z3, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z4, Z6, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z5, Z7, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z8, Z10, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z9, Z11, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z12, Z14, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z13, Z15, Z16, Z31, Z27)
-	MULD(Z2, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z3, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z6, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z10, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z14, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	ADDQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z18
-	BUTTERFLYD2Q(Z0, Z1, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z2, Z3, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z4, Z5, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z6, Z7, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z8, Z9, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z10, Z11, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z12, Z13, Z16, Z31, Z27)
-	BUTTERFLYD2Q(Z14, Z15, Z16, Z31, Z27)
-	MULD(Z1, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z3, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z5, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z9, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	ADDQ         $24, CX
-	MOVQ         ·vInterleaveIndices+0(SB), R8
-	VMOVDQU64    0(R8), Z22
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Y18
-	VINSERTI64X4 $1, Y18, Z18, Z18
-	MOVQ         24(CX), DI
-	VMOVDQU32    0(DI), X19
-	VINSERTI64X2 $1, X19, Z19, Z19
-	VINSERTI64X2 $0x0000000000000002, X19, Z19, Z19
-	VINSERTI64X2 $0x0000000000000003, X19, Z19, Z19
-	MOVQ         48(CX), DI
-	VPBROADCASTD 0(DI), Z20
-	VPBROADCASTD 4(DI), Z21
-	VPBLENDMD    Z20, Z21, K3, Z20
-	PERMUTE8X8(Z0, Z1, Z26)
-	BUTTERFLYD2Q(Z0, Z1, Z16, Z31, Z27)
-	PERMUTE8X8(Z2, Z3, Z26)
-	BUTTERFLYD2Q(Z2, Z3, Z16, Z31, Z27)
-	PERMUTE8X8(Z4, Z5, Z26)
-	BUTTERFLYD2Q(Z4, Z5, Z16, Z31, Z27)
-	PERMUTE8X8(Z6, Z7, Z26)
-	BUTTERFLYD2Q(Z6, Z7, Z16, Z31, Z27)
-	PERMUTE8X8(Z8, Z9, Z26)
-	BUTTERFLYD2Q(Z8, Z9, Z16, Z31, Z27)
-	PERMUTE8X8(Z10, Z11, Z26)
-	BUTTERFLYD2Q(Z10, Z11, Z16, Z31, Z27)
-	PERMUTE8X8(Z12, Z13, Z26)
-	BUTTERFLYD2Q(Z12, Z13, Z16, Z31, Z27)
-	PERMUTE8X8(Z14, Z15, Z26)
-	BUTTERFLYD2Q(Z14, Z15, Z16, Z31, Z27)
-	MULD(Z1, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z3, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z5, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z9, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z18, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE4X4(Z0, Z1, Z22, Z26)
-	BUTTERFLYD2Q(Z0, Z1, Z16, Z31, Z27)
-	PERMUTE4X4(Z2, Z3, Z22, Z26)
-	BUTTERFLYD2Q(Z2, Z3, Z16, Z31, Z27)
-	PERMUTE4X4(Z4, Z5, Z22, Z26)
-	BUTTERFLYD2Q(Z4, Z5, Z16, Z31, Z27)
-	PERMUTE4X4(Z6, Z7, Z22, Z26)
-	BUTTERFLYD2Q(Z6, Z7, Z16, Z31, Z27)
-	PERMUTE4X4(Z8, Z9, Z22, Z26)
-	BUTTERFLYD2Q(Z8, Z9, Z16, Z31, Z27)
-	PERMUTE4X4(Z10, Z11, Z22, Z26)
-	BUTTERFLYD2Q(Z10, Z11, Z16, Z31, Z27)
-	PERMUTE4X4(Z12, Z13, Z22, Z26)
-	BUTTERFLYD2Q(Z12, Z13, Z16, Z31, Z27)
-	PERMUTE4X4(Z14, Z15, Z22, Z26)
-	BUTTERFLYD2Q(Z14, Z15, Z16, Z31, Z27)
-	MULD(Z1, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z3, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z5, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z7, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z9, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z11, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z13, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	MULD(Z15, Z19, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE2X2(Z0, Z1, Z26)
-	BUTTERFLYD2Q(Z0, Z1, Z16, Z31, Z27)
-	PERMUTE2X2(Z2, Z3, Z26)
-	BUTTERFLYD2Q(Z2, Z3, Z16, Z31, Z27)
-	PERMUTE2X2(Z4, Z5, Z26)
-	BUTTERFLYD2Q(Z4, Z5, Z16, Z31, Z27)
-	PERMUTE2X2(Z6, Z7, Z26)
-	BUTTERFLYD2Q(Z6, Z7, Z16, Z31, Z27)
-	PERMUTE2X2(Z8, Z9, Z26)
-	BUTTERFLYD2Q(Z8, Z9, Z16, Z31, Z27)
-	PERMUTE2X2(Z10, Z11, Z26)
-	BUTTERFLYD2Q(Z10, Z11, Z16, Z31, Z27)
-	PERMUTE2X2(Z12, Z13, Z26)
-	BUTTERFLYD2Q(Z12, Z13, Z16, Z31, Z27)
-	PERMUTE2X2(Z14, Z15, Z26)
-	BUTTERFLYD2Q(Z14, Z15, Z16, Z31, Z27)
-	MULD(Z1, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z0, Z1, Z26)
-	MULD(Z3, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z2, Z3, Z26)
-	MULD(Z5, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z4, Z5, Z26)
-	MULD(Z7, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z6, Z7, Z26)
-	MULD(Z9, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z8, Z9, Z26)
-	MULD(Z11, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z10, Z11, Z26)
-	MULD(Z13, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z12, Z13, Z26)
-	MULD(Z15, Z20, Z28, Z29, Z26, Z27, Z30, Z31, Z16, Z17)
-	PERMUTE1X1(Z14, Z15, Z26)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z26, Z27)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z26, Z27)
-	VPUNPCKLDQ   Z1, Z0, Z23
-	VPUNPCKHDQ   Z1, Z0, Z1
-	VMOVDQA32    Z23, Z0
-	PERMUTE4X4(Z0, Z1, Z22, Z23)
-	PERMUTE8X8(Z0, Z1, Z23)
-	VMOVDQU32    Z0, 0(R15)
-	VMOVDQU32    Z1, 64(R15)
-	VPUNPCKLDQ   Z3, Z2, Z23
-	VPUNPCKHDQ   Z3, Z2, Z3
-	VMOVDQA32    Z23, Z2
-	PERMUTE4X4(Z2, Z3, Z22, Z23)
-	PERMUTE8X8(Z2, Z3, Z23)
-	VMOVDQU32    Z2, 128(R15)
-	VMOVDQU32    Z3, 192(R15)
-	VPUNPCKLDQ   Z5, Z4, Z23
-	VPUNPCKHDQ   Z5, Z4, Z5
-	VMOVDQA32    Z23, Z4
-	PERMUTE4X4(Z4, Z5, Z22, Z23)
-	PERMUTE8X8(Z4, Z5, Z23)
-	VMOVDQU32    Z4, 256(R15)
-	VMOVDQU32    Z5, 320(R15)
-	VPUNPCKLDQ   Z7, Z6, Z23
-	VPUNPCKHDQ   Z7, Z6, Z7
-	VMOVDQA32    Z23, Z6
-	PERMUTE4X4(Z6, Z7, Z22, Z23)
-	PERMUTE8X8(Z6, Z7, Z23)
-	VMOVDQU32    Z6, 384(R15)
-	VMOVDQU32    Z7, 448(R15)
-	VPUNPCKLDQ   Z9, Z8, Z23
-	VPUNPCKHDQ   Z9, Z8, Z9
-	VMOVDQA32    Z23, Z8
-	PERMUTE4X4(Z8, Z9, Z22, Z23)
-	PERMUTE8X8(Z8, Z9, Z23)
-	VMOVDQU32    Z8, 512(R15)
-	VMOVDQU32    Z9, 576(R15)
-	VPUNPCKLDQ   Z11, Z10, Z23
-	VPUNPCKHDQ   Z11, Z10, Z11
-	VMOVDQA32    Z23, Z10
-	PERMUTE4X4(Z10, Z11, Z22, Z23)
-	PERMUTE8X8(Z10, Z11, Z23)
-	VMOVDQU32    Z10, 640(R15)
-	VMOVDQU32    Z11, 704(R15)
-	VPUNPCKLDQ   Z13, Z12, Z23
-	VPUNPCKHDQ   Z13, Z12, Z13
-	VMOVDQA32    Z23, Z12
-	PERMUTE4X4(Z12, Z13, Z22, Z23)
-	PERMUTE8X8(Z12, Z13, Z23)
-	VMOVDQU32    Z12, 768(R15)
-	VMOVDQU32    Z13, 832(R15)
-	VPUNPCKLDQ   Z15, Z14, Z23
-	VPUNPCKHDQ   Z15, Z14, Z15
-	VMOVDQA32    Z23, Z14
-	PERMUTE4X4(Z14, Z15, Z22, Z23)
-	PERMUTE8X8(Z14, Z15, Z23)
-	VMOVDQU32    Z14, 896(R15)
-	VMOVDQU32    Z15, 960(R15)
-	RET
-
-TEXT ·kerDITNP_256_avx512(SB), NOSPLIT, $0-56
-	// refer to the code generator for comments and documentation.
-	LOAD_Q(Z16, Z17)
-	LOAD_MASKS()
-
-	// load arguments
-	MOVQ         a+0(FP), R15
-	MOVQ         twiddles+24(FP), CX
-	MOVQ         stage+48(FP), AX
-	IMULQ        $24, AX
-	ADDQ         AX, CX                             // we want twiddles[stage] as starting point
-	VMOVDQU32    0(R15), Z0                         // load a[0]
-	VMOVDQU32    64(R15), Z1                        // load a[1]
-	VMOVDQU32    128(R15), Z2                       // load a[2]
-	VMOVDQU32    192(R15), Z3                       // load a[3]
-	VMOVDQU32    256(R15), Z4                       // load a[4]
-	VMOVDQU32    320(R15), Z5                       // load a[5]
-	VMOVDQU32    384(R15), Z6                       // load a[6]
-	VMOVDQU32    448(R15), Z7                       // load a[7]
-	VMOVDQU32    512(R15), Z8                       // load a[8]
-	VMOVDQU32    576(R15), Z9                       // load a[9]
-	VMOVDQU32    640(R15), Z10                      // load a[10]
-	VMOVDQU32    704(R15), Z11                      // load a[11]
-	VMOVDQU32    768(R15), Z12                      // load a[12]
-	VMOVDQU32    832(R15), Z13                      // load a[13]
-	VMOVDQU32    896(R15), Z14                      // load a[14]
-	VMOVDQU32    960(R15), Z15                      // load a[15]
-	MOVQ         ·vInterleaveIndices+0(SB), R8
-	VMOVDQU64    0(R8), Z28
-	PERMUTE1X1(Z0, Z1, Z22)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	PERMUTE1X1(Z0, Z1, Z22)
-	PERMUTE1X1(Z2, Z3, Z22)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	PERMUTE1X1(Z2, Z3, Z22)
-	PERMUTE1X1(Z4, Z5, Z22)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	PERMUTE1X1(Z4, Z5, Z22)
-	PERMUTE1X1(Z6, Z7, Z22)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	PERMUTE1X1(Z6, Z7, Z22)
-	PERMUTE1X1(Z8, Z9, Z22)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	PERMUTE1X1(Z8, Z9, Z22)
-	PERMUTE1X1(Z10, Z11, Z22)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	PERMUTE1X1(Z10, Z11, Z22)
-	PERMUTE1X1(Z12, Z13, Z22)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	PERMUTE1X1(Z12, Z13, Z22)
-	PERMUTE1X1(Z14, Z15, Z22)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	PERMUTE1X1(Z14, Z15, Z22)
-	MOVQ         $0x0000000000000006, AX
-	IMULQ        $24, AX
-	ADDQ         AX, CX
-	MOVQ         0(CX), DI
-	VPBROADCASTD 0(DI), Z20
-	VPBROADCASTD 4(DI), Z21
-	VPBLENDMD    Z20, Z21, K3, Z20
-	PERMUTE2X2(Z0, Z1, Z22)
-	MULD(Z1, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	PERMUTE2X2(Z0, Z1, Z22)
-	PERMUTE2X2(Z2, Z3, Z22)
-	MULD(Z3, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	PERMUTE2X2(Z2, Z3, Z22)
-	PERMUTE2X2(Z4, Z5, Z22)
-	MULD(Z5, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	PERMUTE2X2(Z4, Z5, Z22)
-	PERMUTE2X2(Z6, Z7, Z22)
-	MULD(Z7, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	PERMUTE2X2(Z6, Z7, Z22)
-	PERMUTE2X2(Z8, Z9, Z22)
-	MULD(Z9, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	PERMUTE2X2(Z8, Z9, Z22)
-	PERMUTE2X2(Z10, Z11, Z22)
-	MULD(Z11, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	PERMUTE2X2(Z10, Z11, Z22)
-	PERMUTE2X2(Z12, Z13, Z22)
-	MULD(Z13, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	PERMUTE2X2(Z12, Z13, Z22)
-	PERMUTE2X2(Z14, Z15, Z22)
-	MULD(Z15, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	PERMUTE2X2(Z14, Z15, Z22)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), X19
-	VINSERTI64X2 $1, X19, Z19, Z19
-	VINSERTI64X2 $0x0000000000000002, X19, Z19, Z19
-	VINSERTI64X2 $0x0000000000000003, X19, Z19, Z19
-	PERMUTE4X4(Z0, Z1, Z28, Z22)
-	MULD(Z1, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	PERMUTE4X4(Z0, Z1, Z28, Z22)
-	PERMUTE4X4(Z2, Z3, Z28, Z22)
-	MULD(Z3, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	PERMUTE4X4(Z2, Z3, Z28, Z22)
-	PERMUTE4X4(Z4, Z5, Z28, Z22)
-	MULD(Z5, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	PERMUTE4X4(Z4, Z5, Z28, Z22)
-	PERMUTE4X4(Z6, Z7, Z28, Z22)
-	MULD(Z7, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	PERMUTE4X4(Z6, Z7, Z28, Z22)
-	PERMUTE4X4(Z8, Z9, Z28, Z22)
-	MULD(Z9, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	PERMUTE4X4(Z8, Z9, Z28, Z22)
-	PERMUTE4X4(Z10, Z11, Z28, Z22)
-	MULD(Z11, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	PERMUTE4X4(Z10, Z11, Z28, Z22)
-	PERMUTE4X4(Z12, Z13, Z28, Z22)
-	MULD(Z13, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	PERMUTE4X4(Z12, Z13, Z28, Z22)
-	PERMUTE4X4(Z14, Z15, Z28, Z22)
-	MULD(Z15, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	PERMUTE4X4(Z14, Z15, Z28, Z22)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Y18
-	VINSERTI64X4 $1, Y18, Z18, Z18
-	PERMUTE8X8(Z0, Z1, Z22)
-	MULD(Z1, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	PERMUTE8X8(Z0, Z1, Z22)
-	PERMUTE8X8(Z2, Z3, Z22)
-	MULD(Z3, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	PERMUTE8X8(Z2, Z3, Z22)
-	PERMUTE8X8(Z4, Z5, Z22)
-	MULD(Z5, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	PERMUTE8X8(Z4, Z5, Z22)
-	PERMUTE8X8(Z6, Z7, Z22)
-	MULD(Z7, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	PERMUTE8X8(Z6, Z7, Z22)
-	PERMUTE8X8(Z8, Z9, Z22)
-	MULD(Z9, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	PERMUTE8X8(Z8, Z9, Z22)
-	PERMUTE8X8(Z10, Z11, Z22)
-	MULD(Z11, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	PERMUTE8X8(Z10, Z11, Z22)
-	PERMUTE8X8(Z12, Z13, Z22)
-	MULD(Z13, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	PERMUTE8X8(Z12, Z13, Z22)
-	PERMUTE8X8(Z14, Z15, Z22)
-	MULD(Z15, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	PERMUTE8X8(Z14, Z15, Z22)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z29
-	MULD(Z1, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z1, Z16, Z22, Z23)
-	MULD(Z3, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z3, Z16, Z22, Z23)
-	MULD(Z5, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z5, Z16, Z22, Z23)
-	MULD(Z7, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z7, Z16, Z22, Z23)
-	MULD(Z9, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z9, Z16, Z22, Z23)
-	MULD(Z11, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z11, Z16, Z22, Z23)
-	MULD(Z13, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z13, Z16, Z22, Z23)
-	MULD(Z15, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z14, Z15, Z16, Z22, Z23)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z29
-	VMOVDQU32    64(DI), Z30
-	MULD(Z2, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z2, Z16, Z22, Z23)
-	MULD(Z3, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z1, Z3, Z16, Z22, Z23)
-	MULD(Z6, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z6, Z16, Z22, Z23)
-	MULD(Z7, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z5, Z7, Z16, Z22, Z23)
-	MULD(Z10, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z10, Z16, Z22, Z23)
-	MULD(Z11, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z9, Z11, Z16, Z22, Z23)
-	MULD(Z14, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z12, Z14, Z16, Z22, Z23)
-	MULD(Z15, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z13, Z15, Z16, Z22, Z23)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z29
-	VMOVDQU32    64(DI), Z30
-	VMOVDQU32    128(DI), Z31
-	VMOVDQU32    192(DI), Z18
-	MULD(Z4, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z4, Z16, Z22, Z23)
-	MULD(Z5, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z1, Z5, Z16, Z22, Z23)
-	MULD(Z6, Z31, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z6, Z16, Z22, Z23)
-	MULD(Z7, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z3, Z7, Z16, Z22, Z23)
-	MULD(Z12, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z8, Z12, Z16, Z22, Z23)
-	MULD(Z13, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z9, Z13, Z16, Z22, Z23)
-	MULD(Z14, Z31, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z10, Z14, Z16, Z22, Z23)
-	MULD(Z15, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z11, Z15, Z16, Z22, Z23)
-	SUBQ         $24, CX
-	MOVQ         0(CX), DI
-	VMOVDQU32    0(DI), Z29
-	VMOVDQU32    64(DI), Z30
-	VMOVDQU32    128(DI), Z31
-	VMOVDQU32    192(DI), Z18
-	VMOVDQU32    256(DI), Z19
-	VMOVDQU32    320(DI), Z20
-	VMOVDQU32    384(DI), Z21
-	VMOVDQU32    448(DI), Z28
-	MULD(Z8, Z29, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z0, Z8, Z16, Z22, Z23)
-	MULD(Z9, Z30, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z1, Z9, Z16, Z22, Z23)
-	MULD(Z10, Z31, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z2, Z10, Z16, Z22, Z23)
-	MULD(Z11, Z18, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z3, Z11, Z16, Z22, Z23)
-	MULD(Z12, Z19, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z4, Z12, Z16, Z22, Z23)
-	MULD(Z13, Z20, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z5, Z13, Z16, Z22, Z23)
-	MULD(Z14, Z21, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z6, Z14, Z16, Z22, Z23)
-	MULD(Z15, Z28, Z24, Z25, Z22, Z23, Z26, Z27, Z16, Z17)
-	BUTTERFLYD1Q(Z7, Z15, Z16, Z22, Z23)
-	VMOVDQU32    Z0, 0(R15)
-	VMOVDQU32    Z1, 64(R15)
-	VMOVDQU32    Z2, 128(R15)
-	VMOVDQU32    Z3, 192(R15)
-	VMOVDQU32    Z4, 256(R15)
-	VMOVDQU32    Z5, 320(R15)
-	VMOVDQU32    Z6, 384(R15)
-	VMOVDQU32    Z7, 448(R15)
-	VMOVDQU32    Z8, 512(R15)
-	VMOVDQU32    Z9, 576(R15)
-	VMOVDQU32    Z10, 640(R15)
-	VMOVDQU32    Z11, 704(R15)
-	VMOVDQU32    Z12, 768(R15)
-	VMOVDQU32    Z13, 832(R15)
-	VMOVDQU32    Z14, 896(R15)
-	VMOVDQU32    Z15, 960(R15)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/koalabear/poseidon2/poseidon2_amd64.s b/field/koalabear/poseidon2/poseidon2_amd64.s
--- a/field/koalabear/poseidon2/poseidon2_amd64.s
+++ b/field/koalabear/poseidon2/poseidon2_amd64.s
@@ -1,1071 +1,1 @@
-//go:build !purego
-
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-// Refer to the generator for more documentation.
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-TEXT ·permutation24_avx512(SB), NOSPLIT, $0-48
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         $1, AX
-	KMOVQ        AX, K2
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z0
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z1
-	MOVQ         input+0(FP), R15
-	MOVQ         roundKeys+24(FP), R14
-	VMOVDQU32    0(R15), Z2
-	VMOVDQU32    64(R15), Y3
-	MOVQ         ·diag24+0(SB), CX
-	VMOVDQU32    0(CX), Z18
-	VMOVDQU32    64(CX), Y20
-	VPSRLQ       $32, Z18, Z19
-	VPSRLQ       $32, Y20, Y21
-
-#define ADD(in0, in1, in2, in3, in4) \
-	VPADDD  in0, in1, in4 \
-	VPSUBD  in2, in4, in3 \
-	VPMINUD in4, in3, in4 \
-
-#define MAT_MUL_M4(in0, in1, in2, in3, in4, in5) \
-	VPSHUFD $0x000000000000004e, in0, in1 \
-	ADD(in1, in0, in4, in5, in1)          \
-	VPSHUFD $0x00000000000000b1, in1, in2 \
-	ADD(in1, in2, in4, in5, in1)          \
-	VPSHUFD $0x0000000000000039, in0, in3 \
-	VPSLLD  $1, in3, in3                  \
-	VPSUBD  in4, in3, in5                 \
-	VPMINUD in3, in5, in3                 \
-	ADD(in0, in1, in4, in5, in0)          \
-	ADD(in0, in3, in4, in5, in0)          \
-
-#define MAT_MUL_EXTERNAL() \
-	MAT_MUL_M4(Z2, Z6, Z7, Z8, Z0, Z11) \
-	MAT_MUL_M4(Y3, Y6, Y7, Y8, Y0, Y11) \
-	VEXTRACTI64X4 $1, Z2, Y16           \
-	ADD(Y16, Y2, Y0, Y11, Y16)          \
-	ADD(Y16, Y3, Y0, Y11, Y16)          \
-	VSHUFF64X2    $1, Y16, Y16, Y17     \
-	ADD(Y16, Y17, Y0, Y11, Y16)         \
-	VINSERTI64X4  $1, Y16, Z16, Z16     \
-	ADD(Y3, Y16, Y0, Y9, Y3)            \
-	ADD(Z2, Z16, Z0, Z11, Z2)           \
-
-#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10) \
-	VPSRLQ    $32, in0, in2  \
-	VPSRLQ    $32, in1, in3  \
-	VPMULUDQ  in0, in1, in4  \
-	VPMULUDQ  in2, in3, in5  \
-	VPMULUDQ  in4, in9, in6  \
-	VPMULUDQ  in5, in9, in7  \
-	VPMULUDQ  in6, in8, in6  \
-	VPADDQ    in4, in6, in4  \
-	VPMULUDQ  in7, in8, in7  \
-	VPADDQ    in5, in7, in10 \
-	VMOVSHDUP in4, K3, in10  \
-
-#define REDUCE1Q(in0, in1, in2) \
-	VPSUBD  in0, in1, in2 \
-	VPMINUD in1, in2, in1 \
-
-#define SBOX_FULL() \
-	MULD(Z2, Z2, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z8) \
-	MULD(Z2, Z8, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z2) \
-	REDUCE1Q(Z0, Z2, Z15)                                \
-	MULD(Y3, Y3, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y7) \
-	MULD(Y3, Y7, Y12, Y13, Y6, Y7, Y14, Y15, Y0, Y1, Y3) \
-	REDUCE1Q(Y0, Y3, Y15)                                \
-
-#define SBOX_PARTIAL() \
-	VPMULUDQ X5, X5, X6   \
-	VPMULUDQ X6, X1, X14  \
-	VPMULUDQ X14, X0, X14 \
-	VPADDQ   X6, X14, X6  \
-	VPSRLQ   $32, X6, X8  \
-	VPMULUDQ X5, X8, X6   \
-	VPMULUDQ X6, X1, X14  \
-	VPMULUDQ X14, X0, X14 \
-	VPADDQ   X6, X14, X6  \
-	VPSRLQ   $32, X6, X5  \
-	VPSUBD   X0, X5, X14  \
-	VPMINUD  X5, X14, X5  \
-
-#define SUM_STATE() \
-	VEXTRACTI64X4 $1, Z2, Y16                   \
-	ADD(Y16, Y3, Y0, Y11, Y16)                  \
-	ADD(Y16, Y10, Y0, Y11, Y16)                 \
-	VSHUFF64X2    $1, Y16, Y16, Y17             \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VPSHUFD       $0x000000000000004e, Y16, Y17 \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VPSHUFD       $0x00000000000000b1, Y16, Y17 \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VINSERTI64X4  $1, Y16, Z16, Z16             \
-
-#define FULL_ROUND() \
-	VMOVDQU32 0(BX), Z4      \
-	VMOVDQU32 64(BX), Y5     \
-	ADD(Z2, Z4, Z0, Z11, Z2) \
-	ADD(Y3, Y5, Y0, Y8, Y3)  \
-	SBOX_FULL()              \
-	MAT_MUL_EXTERNAL()       \
-
-	MAT_MUL_EXTERNAL()
-	MOVQ 0(R14), BX
-	FULL_ROUND()
-	MOVQ 24(R14), BX
-	FULL_ROUND()
-	MOVQ 48(R14), BX
-	FULL_ROUND()
-
-	// loop over the partial rounds
-	MOVQ $0x0000000000000015, SI // nb partial rounds --> 21
-	MOVQ R14, DI
-	ADDQ $0x0000000000000048, DI
-
-loop_1:
-	TESTQ     SI, SI
-	JEQ       done_2
-	DECQ      SI
-	MOVQ      0(DI), BX
-	VMOVD     0(BX), X4
-	VMOVDQA32 Z2, Z10
-	ADD(X10, X4, X0, X14, X5)
-	SBOX_PARTIAL()
-	VPBLENDMD Z5, Z10, K2, Z10
-	VPSRLQ    $32, Y3, Y12
-	VPMULUDQ  Y3, Y20, Y6
-	VPMULUDQ  Y12, Y21, Y7
-	VPMULUDQ  Y6, Y1, Y14
-	VPMULUDQ  Y7, Y1, Y15
-	VPMULUDQ  Y14, Y0, Y14
-	VPADDQ    Y6, Y14, Y6
-	VPMULUDQ  Y15, Y0, Y15
-	VPADDQ    Y7, Y15, Y9
-	VMOVSHDUP Y6, K3, Y9
-	VPSUBD    Y0, Y9, Y11
-	VPMINUD   Y9, Y11, Y9
-	VPSRLQ    $32, Z2, Z12
-	VPMULUDQ  Z12, Z19, Z8
-	VPMULUDQ  Z8, Z1, Z15
-	VPMULUDQ  Z15, Z0, Z15
-	VPADDQ    Z8, Z15, Z8
-	SUM_STATE()
-	VPMULUDQ  Z10, Z18, Z6
-	VPMULUDQ  Z6, Z1, Z14
-	VPMULUDQ  Z14, Z0, Z14
-	VPADDQ    Z6, Z14, Z6
-	VMOVSHDUP Z6, K3, Z8
-	VPSUBD    Z0, Z8, Z11
-	VPMINUD   Z8, Z11, Z2
-	ADD(Z2, Z16, Z0, Z11, Z2)
-	ADD(Y9, Y16, Y0, Y5, Y3)
-	ADDQ      $24, DI
-	JMP       loop_1
-
-done_2:
-	MOVQ      576(R14), BX
-	FULL_ROUND()
-	MOVQ      600(R14), BX
-	FULL_ROUND()
-	MOVQ      624(R14), BX
-	FULL_ROUND()
-	VMOVDQU32 Z2, 0(R15)
-	VMOVDQU32 Y3, 64(R15)
-	RET
-
-TEXT ·permutation16x24_avx512(SB), NOSPLIT, $0-32
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z24
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z25
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         input+0(FP), R15
-	MOVQ         roundKeys+8(FP), R14
-	VMOVDQU32    0(R15), Z0
-	VMOVDQU32    64(R15), Z1
-	VMOVDQU32    128(R15), Z2
-	VMOVDQU32    192(R15), Z3
-	VMOVDQU32    256(R15), Z4
-	VMOVDQU32    320(R15), Z5
-	VMOVDQU32    384(R15), Z6
-	VMOVDQU32    448(R15), Z7
-	VMOVDQU32    512(R15), Z8
-	VMOVDQU32    576(R15), Z9
-	VMOVDQU32    640(R15), Z10
-	VMOVDQU32    704(R15), Z11
-	VMOVDQU32    768(R15), Z12
-	VMOVDQU32    832(R15), Z13
-	VMOVDQU32    896(R15), Z14
-	VMOVDQU32    960(R15), Z15
-	VMOVDQU32    1024(R15), Z16
-	VMOVDQU32    1088(R15), Z17
-	VMOVDQU32    1152(R15), Z18
-	VMOVDQU32    1216(R15), Z19
-	VMOVDQU32    1280(R15), Z20
-	VMOVDQU32    1344(R15), Z21
-	VMOVDQU32    1408(R15), Z22
-	VMOVDQU32    1472(R15), Z23
-	ADD(Z0, Z1, Z24, Z31, Z26)
-	ADD(Z2, Z3, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z1, Z24, Z31, Z29)
-	ADD(Z28, Z3, Z24, Z31, Z30)
-
-#define DOUBLE(in0, in1, in2, in3) \
-	VPSLLD  $1, in0, in3  \
-	VPSUBD  in1, in3, in2 \
-	VPMINUD in3, in2, in3 \
-
-	DOUBLE(Z0, Z24, Z31, Z3)
-	ADD(Z3, Z30, Z24, Z31, Z3)
-	DOUBLE(Z2, Z24, Z31, Z1)
-	ADD(Z1, Z29, Z24, Z31, Z1)
-	ADD(Z26, Z29, Z24, Z31, Z0)
-	ADD(Z27, Z30, Z24, Z31, Z2)
-	ADD(Z4, Z5, Z24, Z31, Z26)
-	ADD(Z6, Z7, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z5, Z24, Z31, Z29)
-	ADD(Z28, Z7, Z24, Z31, Z30)
-	DOUBLE(Z4, Z24, Z31, Z7)
-	ADD(Z7, Z30, Z24, Z31, Z7)
-	DOUBLE(Z6, Z24, Z31, Z5)
-	ADD(Z5, Z29, Z24, Z31, Z5)
-	ADD(Z26, Z29, Z24, Z31, Z4)
-	ADD(Z27, Z30, Z24, Z31, Z6)
-	ADD(Z8, Z9, Z24, Z31, Z26)
-	ADD(Z10, Z11, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z9, Z24, Z31, Z29)
-	ADD(Z28, Z11, Z24, Z31, Z30)
-	DOUBLE(Z8, Z24, Z31, Z11)
-	ADD(Z11, Z30, Z24, Z31, Z11)
-	DOUBLE(Z10, Z24, Z31, Z9)
-	ADD(Z9, Z29, Z24, Z31, Z9)
-	ADD(Z26, Z29, Z24, Z31, Z8)
-	ADD(Z27, Z30, Z24, Z31, Z10)
-	ADD(Z12, Z13, Z24, Z31, Z26)
-	ADD(Z14, Z15, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z13, Z24, Z31, Z29)
-	ADD(Z28, Z15, Z24, Z31, Z30)
-	DOUBLE(Z12, Z24, Z31, Z15)
-	ADD(Z15, Z30, Z24, Z31, Z15)
-	DOUBLE(Z14, Z24, Z31, Z13)
-	ADD(Z13, Z29, Z24, Z31, Z13)
-	ADD(Z26, Z29, Z24, Z31, Z12)
-	ADD(Z27, Z30, Z24, Z31, Z14)
-	ADD(Z16, Z17, Z24, Z31, Z26)
-	ADD(Z18, Z19, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z17, Z24, Z31, Z29)
-	ADD(Z28, Z19, Z24, Z31, Z30)
-	DOUBLE(Z16, Z24, Z31, Z19)
-	ADD(Z19, Z30, Z24, Z31, Z19)
-	DOUBLE(Z18, Z24, Z31, Z17)
-	ADD(Z17, Z29, Z24, Z31, Z17)
-	ADD(Z26, Z29, Z24, Z31, Z16)
-	ADD(Z27, Z30, Z24, Z31, Z18)
-	ADD(Z20, Z21, Z24, Z31, Z26)
-	ADD(Z22, Z23, Z24, Z31, Z27)
-	ADD(Z26, Z27, Z24, Z31, Z28)
-	ADD(Z28, Z21, Z24, Z31, Z29)
-	ADD(Z28, Z23, Z24, Z31, Z30)
-	DOUBLE(Z20, Z24, Z31, Z23)
-	ADD(Z23, Z30, Z24, Z31, Z23)
-	DOUBLE(Z22, Z24, Z31, Z21)
-	ADD(Z21, Z29, Z24, Z31, Z21)
-	ADD(Z26, Z29, Z24, Z31, Z20)
-	ADD(Z27, Z30, Z24, Z31, Z22)
-	ADD(Z0, Z4, Z24, Z29, Z31)
-	ADD(Z1, Z5, Z24, Z30, Z26)
-	ADD(Z2, Z6, Z24, Z29, Z27)
-	ADD(Z3, Z7, Z24, Z30, Z28)
-	ADD(Z8, Z31, Z24, Z29, Z31)
-	ADD(Z9, Z26, Z24, Z30, Z26)
-	ADD(Z10, Z27, Z24, Z29, Z27)
-	ADD(Z11, Z28, Z24, Z30, Z28)
-	ADD(Z12, Z31, Z24, Z29, Z31)
-	ADD(Z13, Z26, Z24, Z30, Z26)
-	ADD(Z14, Z27, Z24, Z29, Z27)
-	ADD(Z15, Z28, Z24, Z30, Z28)
-	ADD(Z16, Z31, Z24, Z29, Z31)
-	ADD(Z17, Z26, Z24, Z30, Z26)
-	ADD(Z18, Z27, Z24, Z29, Z27)
-	ADD(Z19, Z28, Z24, Z30, Z28)
-	ADD(Z20, Z31, Z24, Z29, Z31)
-	ADD(Z21, Z26, Z24, Z30, Z26)
-	ADD(Z22, Z27, Z24, Z29, Z27)
-	ADD(Z23, Z28, Z24, Z30, Z28)
-	ADD(Z0, Z31, Z24, Z29, Z0)
-	ADD(Z1, Z26, Z24, Z30, Z1)
-	ADD(Z2, Z27, Z24, Z29, Z2)
-	ADD(Z3, Z28, Z24, Z30, Z3)
-	ADD(Z4, Z31, Z24, Z29, Z4)
-	ADD(Z5, Z26, Z24, Z30, Z5)
-	ADD(Z6, Z27, Z24, Z29, Z6)
-	ADD(Z7, Z28, Z24, Z30, Z7)
-	ADD(Z8, Z31, Z24, Z29, Z8)
-	ADD(Z9, Z26, Z24, Z30, Z9)
-	ADD(Z10, Z27, Z24, Z29, Z10)
-	ADD(Z11, Z28, Z24, Z30, Z11)
-	ADD(Z12, Z31, Z24, Z29, Z12)
-	ADD(Z13, Z26, Z24, Z30, Z13)
-	ADD(Z14, Z27, Z24, Z29, Z14)
-	ADD(Z15, Z28, Z24, Z30, Z15)
-	ADD(Z16, Z31, Z24, Z29, Z16)
-	ADD(Z17, Z26, Z24, Z30, Z17)
-	ADD(Z18, Z27, Z24, Z29, Z18)
-	ADD(Z19, Z28, Z24, Z30, Z19)
-	ADD(Z20, Z31, Z24, Z29, Z20)
-	ADD(Z21, Z26, Z24, Z30, Z21)
-	ADD(Z22, Z27, Z24, Z29, Z22)
-	ADD(Z23, Z28, Z24, Z30, Z23)
-
-	// loop over the first full rounds
-	MOVQ $0x0000000000000003, BX
-
-loop_3:
-	TESTQ        BX, BX
-	JEQ          done_4
-	DECQ         BX
-	MOVQ         0(R14), CX
-	VPBROADCASTD 0(CX), Z29
-	ADD(Z0, Z29, Z24, Z30, Z0)
-
-#define MUL_W(in0, in1, in2, in3, in4, in5, in6, in7) \
-	VPSRLQ    $32, in0, in2 \
-	VPSRLQ    $32, in1, in3 \
-	VPMULUDQ  in0, in1, in4 \
-	VPMULUDQ  in2, in3, in5 \
-	VPMULUDQ  in4, Z25, in6 \
-	VPMULUDQ  in5, Z25, in3 \
-	VPMULUDQ  in6, Z24, in6 \
-	VPADDQ    in4, in6, in4 \
-	VPMULUDQ  in3, Z24, in3 \
-	VPADDQ    in5, in3, in7 \
-	VMOVSHDUP in4, K3, in7  \
-
-	MUL_W(Z0, Z0, Z26, Z27, Z28, Z30, Z29, Z31)
-	MUL_W(Z0, Z31, Z26, Z27, Z28, Z30, Z29, Z0)
-	REDUCE1Q(Z24, Z0, Z26)
-	VPBROADCASTD 4(CX), Z27
-	ADD(Z1, Z27, Z24, Z28, Z1)
-	MUL_W(Z1, Z1, Z29, Z26, Z31, Z28, Z27, Z30)
-	MUL_W(Z1, Z30, Z29, Z26, Z31, Z28, Z27, Z1)
-	REDUCE1Q(Z24, Z1, Z29)
-	VPBROADCASTD 8(CX), Z26
-	ADD(Z2, Z26, Z24, Z31, Z2)
-	MUL_W(Z2, Z2, Z27, Z29, Z30, Z31, Z26, Z28)
-	MUL_W(Z2, Z28, Z27, Z29, Z30, Z31, Z26, Z2)
-	REDUCE1Q(Z24, Z2, Z27)
-	VPBROADCASTD 12(CX), Z29
-	ADD(Z3, Z29, Z24, Z30, Z3)
-	MUL_W(Z3, Z3, Z26, Z27, Z28, Z30, Z29, Z31)
-	MUL_W(Z3, Z31, Z26, Z27, Z28, Z30, Z29, Z3)
-	REDUCE1Q(Z24, Z3, Z26)
-	VPBROADCASTD 16(CX), Z27
-	ADD(Z4, Z27, Z24, Z28, Z4)
-	MUL_W(Z4, Z4, Z29, Z26, Z31, Z28, Z27, Z30)
-	MUL_W(Z4, Z30, Z29, Z26, Z31, Z28, Z27, Z4)
-	REDUCE1Q(Z24, Z4, Z29)
-	VPBROADCASTD 20(CX), Z26
-	ADD(Z5, Z26, Z24, Z31, Z5)
-	MUL_W(Z5, Z5, Z27, Z29, Z30, Z31, Z26, Z28)
-	MUL_W(Z5, Z28, Z27, Z29, Z30, Z31, Z26, Z5)
-	REDUCE1Q(Z24, Z5, Z27)
-	VPBROADCASTD 24(CX), Z29
-	ADD(Z6, Z29, Z24, Z30, Z6)
-	MUL_W(Z6, Z6, Z26, Z27, Z28, Z30, Z29, Z31)
-	MUL_W(Z6, Z31, Z26, Z27, Z28, Z30, Z29, Z6)
-	REDUCE1Q(Z24, Z6, Z26)
-	VPBROADCASTD 28(CX), Z27
-	ADD(Z7, Z27, Z24, Z28, Z7)
-	MUL_W(Z7, Z7, Z29, Z26, Z31, Z28, Z27, Z30)
-	MUL_W(Z7, Z30, Z29, Z26, Z31, Z28, Z27, Z7)
-	REDUCE1Q(Z24, Z7, Z29)
-	VPBROADCASTD 32(CX), Z26
-	ADD(Z8, Z26, Z24, Z31, Z8)
-	MUL_W(Z8, Z8, Z27, Z29, Z30, Z31, Z26, Z28)
-	MUL_W(Z8, Z28, Z27, Z29, Z30, Z31, Z26, Z8)
-	REDUCE1Q(Z24, Z8, Z27)
-	VPBROADCASTD 36(CX), Z29
-	ADD(Z9, Z29, Z24, Z30, Z9)
-	MUL_W(Z9, Z9, Z26, Z27, Z28, Z30, Z29, Z31)
-	MUL_W(Z9, Z31, Z26, Z27, Z28, Z30, Z29, Z9)
-	REDUCE1Q(Z24, Z9, Z26)
-	VPBROADCASTD 40(CX), Z27
-	ADD(Z10, Z27, Z24, Z28, Z10)
-	MUL_W(Z10, Z10, Z29, Z26, Z31, Z28, Z27, Z30)
-	MUL_W(Z10, Z30, Z29, Z26, Z31, Z28, Z27, Z10)
-	REDUCE1Q(Z24, Z10, Z29)
-	VPBROADCASTD 44(CX), Z26
-	ADD(Z11, Z26, Z24, Z31, Z11)
-	MUL_W(Z11, Z11, Z27, Z29, Z30, Z31, Z26, Z28)
-	MUL_W(Z11, Z28, Z27, Z29, Z30, Z31, Z26, Z11)
-	REDUCE1Q(Z24, Z11, Z27)
-	VPBROADCASTD 48(CX), Z29
-	ADD(Z12, Z29, Z24, Z30, Z12)
-	MUL_W(Z12, Z12, Z26, Z27, Z28, Z30, Z29, Z31)
-	MUL_W(Z12, Z31, Z26, Z27, Z28, Z30, Z29, Z12)
-	REDUCE1Q(Z24, Z12, Z26)
-	VPBROADCASTD 52(CX), Z27
-	ADD(Z13, Z27, Z24, Z28, Z13)
-	MUL_W(Z13, Z13, Z29, Z26, Z31, Z28, Z27, Z30)
-	MUL_W(Z13, Z30, Z29, Z26, Z31, Z28, Z27, Z13)
-	REDUCE1Q(Z24, Z13, Z29)
-	VPBROADCASTD 56(CX), Z26
-	ADD(Z14, Z26, Z24, Z31, Z14)
-	MUL_W(Z14, Z14, Z27, Z29, Z30, Z31, Z26, Z28)
-	MUL_W(Z14, Z28, Z27, Z29, Z30, Z31, Z26, Z14)
-	REDUCE1Q(Z24, Z14, Z27)
-	VPBROADCASTD 60(CX), Z29
-	ADD(Z15, Z29, Z24, Z30, Z15)
-	MUL_W(Z15, Z15, Z26, Z27, Z28, Z30, Z29, Z31)
-	MUL_W(Z15, Z31, Z26, Z27, Z28, Z30, Z29, Z15)
-	REDUCE1Q(Z24, Z15, Z26)
-	VPBROADCASTD 64(CX), Z27
-	ADD(Z16, Z27, Z24, Z28, Z16)
-	MUL_W(Z16, Z16, Z29, Z26, Z31, Z28, Z27, Z30)
-	MUL_W(Z16, Z30, Z29, Z26, Z31, Z28, Z27, Z16)
-	REDUCE1Q(Z24, Z16, Z29)
-	VPBROADCASTD 68(CX), Z26
-	ADD(Z17, Z26, Z24, Z31, Z17)
-	MUL_W(Z17, Z17, Z27, Z29, Z30, Z31, Z26, Z28)
-	MUL_W(Z17, Z28, Z27, Z29, Z30, Z31, Z26, Z17)
-	REDUCE1Q(Z24, Z17, Z27)
-	VPBROADCASTD 72(CX), Z29
-	ADD(Z18, Z29, Z24, Z30, Z18)
-	MUL_W(Z18, Z18, Z26, Z27, Z28, Z30, Z29, Z31)
-	MUL_W(Z18, Z31, Z26, Z27, Z28, Z30, Z29, Z18)
-	REDUCE1Q(Z24, Z18, Z26)
-	VPBROADCASTD 76(CX), Z27
-	ADD(Z19, Z27, Z24, Z28, Z19)
-	MUL_W(Z19, Z19, Z29, Z26, Z31, Z28, Z27, Z30)
-	MUL_W(Z19, Z30, Z29, Z26, Z31, Z28, Z27, Z19)
-	REDUCE1Q(Z24, Z19, Z29)
-	VPBROADCASTD 80(CX), Z26
-	ADD(Z20, Z26, Z24, Z31, Z20)
-	MUL_W(Z20, Z20, Z27, Z29, Z30, Z31, Z26, Z28)
-	MUL_W(Z20, Z28, Z27, Z29, Z30, Z31, Z26, Z20)
-	REDUCE1Q(Z24, Z20, Z27)
-	VPBROADCASTD 84(CX), Z29
-	ADD(Z21, Z29, Z24, Z30, Z21)
-	MUL_W(Z21, Z21, Z26, Z27, Z28, Z30, Z29, Z31)
-	MUL_W(Z21, Z31, Z26, Z27, Z28, Z30, Z29, Z21)
-	REDUCE1Q(Z24, Z21, Z26)
-	VPBROADCASTD 88(CX), Z27
-	ADD(Z22, Z27, Z24, Z28, Z22)
-	MUL_W(Z22, Z22, Z29, Z26, Z31, Z28, Z27, Z30)
-	MUL_W(Z22, Z30, Z29, Z26, Z31, Z28, Z27, Z22)
-	REDUCE1Q(Z24, Z22, Z29)
-	VPBROADCASTD 92(CX), Z26
-	ADD(Z23, Z26, Z24, Z31, Z23)
-	MUL_W(Z23, Z23, Z27, Z29, Z30, Z31, Z26, Z28)
-	MUL_W(Z23, Z28, Z27, Z29, Z30, Z31, Z26, Z23)
-	REDUCE1Q(Z24, Z23, Z27)
-	ADD(Z0, Z1, Z24, Z28, Z29)
-	ADD(Z2, Z3, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z1, Z24, Z28, Z26)
-	ADD(Z31, Z3, Z24, Z28, Z27)
-	DOUBLE(Z0, Z24, Z28, Z3)
-	ADD(Z3, Z27, Z24, Z28, Z3)
-	DOUBLE(Z2, Z24, Z28, Z1)
-	ADD(Z1, Z26, Z24, Z28, Z1)
-	ADD(Z29, Z26, Z24, Z28, Z0)
-	ADD(Z30, Z27, Z24, Z28, Z2)
-	ADD(Z4, Z5, Z24, Z28, Z29)
-	ADD(Z6, Z7, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z5, Z24, Z28, Z26)
-	ADD(Z31, Z7, Z24, Z28, Z27)
-	DOUBLE(Z4, Z24, Z28, Z7)
-	ADD(Z7, Z27, Z24, Z28, Z7)
-	DOUBLE(Z6, Z24, Z28, Z5)
-	ADD(Z5, Z26, Z24, Z28, Z5)
-	ADD(Z29, Z26, Z24, Z28, Z4)
-	ADD(Z30, Z27, Z24, Z28, Z6)
-	ADD(Z8, Z9, Z24, Z28, Z29)
-	ADD(Z10, Z11, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z9, Z24, Z28, Z26)
-	ADD(Z31, Z11, Z24, Z28, Z27)
-	DOUBLE(Z8, Z24, Z28, Z11)
-	ADD(Z11, Z27, Z24, Z28, Z11)
-	DOUBLE(Z10, Z24, Z28, Z9)
-	ADD(Z9, Z26, Z24, Z28, Z9)
-	ADD(Z29, Z26, Z24, Z28, Z8)
-	ADD(Z30, Z27, Z24, Z28, Z10)
-	ADD(Z12, Z13, Z24, Z28, Z29)
-	ADD(Z14, Z15, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z13, Z24, Z28, Z26)
-	ADD(Z31, Z15, Z24, Z28, Z27)
-	DOUBLE(Z12, Z24, Z28, Z15)
-	ADD(Z15, Z27, Z24, Z28, Z15)
-	DOUBLE(Z14, Z24, Z28, Z13)
-	ADD(Z13, Z26, Z24, Z28, Z13)
-	ADD(Z29, Z26, Z24, Z28, Z12)
-	ADD(Z30, Z27, Z24, Z28, Z14)
-	ADD(Z16, Z17, Z24, Z28, Z29)
-	ADD(Z18, Z19, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z17, Z24, Z28, Z26)
-	ADD(Z31, Z19, Z24, Z28, Z27)
-	DOUBLE(Z16, Z24, Z28, Z19)
-	ADD(Z19, Z27, Z24, Z28, Z19)
-	DOUBLE(Z18, Z24, Z28, Z17)
-	ADD(Z17, Z26, Z24, Z28, Z17)
-	ADD(Z29, Z26, Z24, Z28, Z16)
-	ADD(Z30, Z27, Z24, Z28, Z18)
-	ADD(Z20, Z21, Z24, Z28, Z29)
-	ADD(Z22, Z23, Z24, Z28, Z30)
-	ADD(Z29, Z30, Z24, Z28, Z31)
-	ADD(Z31, Z21, Z24, Z28, Z26)
-	ADD(Z31, Z23, Z24, Z28, Z27)
-	DOUBLE(Z20, Z24, Z28, Z23)
-	ADD(Z23, Z27, Z24, Z28, Z23)
-	DOUBLE(Z22, Z24, Z28, Z21)
-	ADD(Z21, Z26, Z24, Z28, Z21)
-	ADD(Z29, Z26, Z24, Z28, Z20)
-	ADD(Z30, Z27, Z24, Z28, Z22)
-	ADD(Z0, Z4, Z24, Z26, Z28)
-	ADD(Z1, Z5, Z24, Z27, Z29)
-	ADD(Z2, Z6, Z24, Z26, Z30)
-	ADD(Z3, Z7, Z24, Z27, Z31)
-	ADD(Z8, Z28, Z24, Z26, Z28)
-	ADD(Z9, Z29, Z24, Z27, Z29)
-	ADD(Z10, Z30, Z24, Z26, Z30)
-	ADD(Z11, Z31, Z24, Z27, Z31)
-	ADD(Z12, Z28, Z24, Z26, Z28)
-	ADD(Z13, Z29, Z24, Z27, Z29)
-	ADD(Z14, Z30, Z24, Z26, Z30)
-	ADD(Z15, Z31, Z24, Z27, Z31)
-	ADD(Z16, Z28, Z24, Z26, Z28)
-	ADD(Z17, Z29, Z24, Z27, Z29)
-	ADD(Z18, Z30, Z24, Z26, Z30)
-	ADD(Z19, Z31, Z24, Z27, Z31)
-	ADD(Z20, Z28, Z24, Z26, Z28)
-	ADD(Z21, Z29, Z24, Z27, Z29)
-	ADD(Z22, Z30, Z24, Z26, Z30)
-	ADD(Z23, Z31, Z24, Z27, Z31)
-	ADD(Z0, Z28, Z24, Z26, Z0)
-	ADD(Z1, Z29, Z24, Z27, Z1)
-	ADD(Z2, Z30, Z24, Z26, Z2)
-	ADD(Z3, Z31, Z24, Z27, Z3)
-	ADD(Z4, Z28, Z24, Z26, Z4)
-	ADD(Z5, Z29, Z24, Z27, Z5)
-	ADD(Z6, Z30, Z24, Z26, Z6)
-	ADD(Z7, Z31, Z24, Z27, Z7)
-	ADD(Z8, Z28, Z24, Z26, Z8)
-	ADD(Z9, Z29, Z24, Z27, Z9)
-	ADD(Z10, Z30, Z24, Z26, Z10)
-	ADD(Z11, Z31, Z24, Z27, Z11)
-	ADD(Z12, Z28, Z24, Z26, Z12)
-	ADD(Z13, Z29, Z24, Z27, Z13)
-	ADD(Z14, Z30, Z24, Z26, Z14)
-	ADD(Z15, Z31, Z24, Z27, Z15)
-	ADD(Z16, Z28, Z24, Z26, Z16)
-	ADD(Z17, Z29, Z24, Z27, Z17)
-	ADD(Z18, Z30, Z24, Z26, Z18)
-	ADD(Z19, Z31, Z24, Z27, Z19)
-	ADD(Z20, Z28, Z24, Z26, Z20)
-	ADD(Z21, Z29, Z24, Z27, Z21)
-	ADD(Z22, Z30, Z24, Z26, Z22)
-	ADD(Z23, Z31, Z24, Z27, Z23)
-	ADDQ         $24, R14
-	JMP          loop_3
-
-done_4:
-	// loop over the partial rounds
-	MOVQ $0x0000000000000015, SI
-
-loop_5:
-	TESTQ        SI, SI
-	JEQ          done_6
-	DECQ         SI
-	MOVQ         0(R14), CX
-	VPBROADCASTD 0(CX), Z26
-	ADD(Z0, Z26, Z24, Z27, Z0)
-	MUL_W(Z0, Z0, Z29, Z30, Z31, Z27, Z26, Z28)
-	MUL_W(Z0, Z28, Z29, Z30, Z31, Z27, Z26, Z0)
-	REDUCE1Q(Z24, Z0, Z29)
-	ADD(Z0, Z1, Z24, Z28, Z27)
-	ADD(Z2, Z3, Z24, Z28, Z26)
-	ADD(Z4, Z5, Z24, Z28, Z29)
-	ADD(Z6, Z7, Z24, Z28, Z30)
-	ADD(Z8, Z27, Z24, Z28, Z27)
-	ADD(Z9, Z26, Z24, Z28, Z26)
-	ADD(Z10, Z29, Z24, Z28, Z29)
-	ADD(Z11, Z30, Z24, Z28, Z30)
-	ADD(Z12, Z27, Z24, Z28, Z27)
-	ADD(Z13, Z26, Z24, Z28, Z26)
-	ADD(Z14, Z29, Z24, Z28, Z29)
-	ADD(Z15, Z30, Z24, Z28, Z30)
-	ADD(Z16, Z27, Z24, Z28, Z27)
-	ADD(Z17, Z26, Z24, Z28, Z26)
-	ADD(Z18, Z29, Z24, Z28, Z29)
-	ADD(Z19, Z30, Z24, Z28, Z30)
-	ADD(Z20, Z27, Z24, Z28, Z27)
-	ADD(Z21, Z26, Z24, Z28, Z26)
-	ADD(Z22, Z29, Z24, Z28, Z29)
-	ADD(Z23, Z30, Z24, Z28, Z30)
-	ADD(Z27, Z26, Z24, Z28, Z27)
-	ADD(Z29, Z30, Z24, Z28, Z29)
-	ADD(Z27, Z29, Z24, Z28, Z30)
-	DOUBLE(Z0, Z24, Z28, Z0)
-	DOUBLE(Z2, Z24, Z28, Z2)
-
-#define HALVE(in0, in1) \
-	MOVD         $1, AX            \
-	VPBROADCASTD AX, in1           \
-	VPTESTMD     in0, in1, K4      \
-	VPADDD       in0, Z24, K4, in0 \
-	VPSRLD       $1, in0, in0      \
-
-	HALVE(Z3, Z28)
-	HALVE(Z6, Z28)
-	DOUBLE(Z4, Z24, Z28, Z27)
-	ADD(Z4, Z27, Z24, Z28, Z4)
-	DOUBLE(Z5, Z24, Z28, Z5)
-	DOUBLE(Z5, Z24, Z28, Z5)
-	DOUBLE(Z7, Z24, Z28, Z31)
-	ADD(Z7, Z31, Z24, Z28, Z7)
-	DOUBLE(Z8, Z24, Z28, Z8)
-	DOUBLE(Z8, Z24, Z28, Z8)
-
-#define MUL_2_EXP_NEG_N(in0, in1, in2, in3, in4, in5, in6, in7, in8) \
-	VPSRLQ    $32, in0, in6 \
-	VPSLLQ    $32, in0, in0 \
-	VPSRLQ    in2, in0, in4 \
-	VPSLLQ    in3, in6, in5 \
-	VPMULUDQ  in4, Z25, in7 \
-	VPMULUDQ  in5, Z25, in8 \
-	VPMULUDQ  in7, Z24, in7 \
-	VPADDQ    in4, in7, in4 \
-	VPMULUDQ  in8, Z24, in8 \
-	VPADDQ    in5, in8, in1 \
-	VMOVSHDUP in4, K3, in1  \
-	VPSUBD    Z24, in1, in8 \
-	VPMINUD   in1, in8, in1 \
-
-	MUL_2_EXP_NEG_N(Z9, Z9, $8, $24, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z10, Z10, $2, $30, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z11, Z11, $3, $29, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z12, Z12, $4, $28, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z13, Z13, $5, $27, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z14, Z14, $6, $26, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z15, Z15, $24, $8, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z16, Z16, $8, $24, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z17, Z17, $3, $29, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z18, Z18, $4, $28, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z19, Z19, $5, $27, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z20, Z20, $6, $26, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z21, Z21, $7, $25, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z22, Z22, $9, $23, Z28, Z31, Z27, Z26, Z29)
-	MUL_2_EXP_NEG_N(Z23, Z23, $24, $8, Z28, Z31, Z27, Z26, Z29)
-
-#define SUB(in0, in1, in2, in3, in4) \
-	VPSUBD  in1, in0, in4 \
-	VPADDD  in2, in4, in3 \
-	VPMINUD in4, in3, in4 \
-
-	SUB(Z30, Z0, Z24, Z28, Z0)
-	ADD(Z30, Z1, Z24, Z31, Z1)
-	ADD(Z2, Z30, Z24, Z27, Z2)
-	ADD(Z3, Z30, Z24, Z26, Z3)
-	ADD(Z4, Z30, Z24, Z29, Z4)
-	ADD(Z5, Z30, Z24, Z28, Z5)
-	SUB(Z30, Z6, Z24, Z31, Z6)
-	SUB(Z30, Z7, Z24, Z27, Z7)
-	SUB(Z30, Z8, Z24, Z26, Z8)
-	ADD(Z9, Z30, Z24, Z29, Z9)
-	ADD(Z10, Z30, Z24, Z28, Z10)
-	ADD(Z11, Z30, Z24, Z31, Z11)
-	ADD(Z12, Z30, Z24, Z27, Z12)
-	ADD(Z13, Z30, Z24, Z26, Z13)
-	ADD(Z14, Z30, Z24, Z29, Z14)
-	ADD(Z15, Z30, Z24, Z28, Z15)
-	SUB(Z30, Z16, Z24, Z31, Z16)
-	SUB(Z30, Z17, Z24, Z27, Z17)
-	SUB(Z30, Z18, Z24, Z26, Z18)
-	SUB(Z30, Z19, Z24, Z29, Z19)
-	SUB(Z30, Z20, Z24, Z28, Z20)
-	SUB(Z30, Z21, Z24, Z31, Z21)
-	SUB(Z30, Z22, Z24, Z27, Z22)
-	SUB(Z30, Z23, Z24, Z26, Z23)
-	ADDQ $24, R14
-	JMP  loop_5
-
-done_6:
-	// loop over the final full rounds
-	MOVQ $0x0000000000000003, DI
-
-loop_7:
-	TESTQ        DI, DI
-	JEQ          done_8
-	DECQ         DI
-	MOVQ         0(R14), CX
-	VPBROADCASTD 0(CX), Z29
-	ADD(Z0, Z29, Z24, Z28, Z0)
-	MUL_W(Z0, Z0, Z27, Z26, Z30, Z28, Z29, Z31)
-	MUL_W(Z0, Z31, Z27, Z26, Z30, Z28, Z29, Z0)
-	REDUCE1Q(Z24, Z0, Z27)
-	VPBROADCASTD 4(CX), Z26
-	ADD(Z1, Z26, Z24, Z30, Z1)
-	MUL_W(Z1, Z1, Z29, Z27, Z31, Z30, Z26, Z28)
-	MUL_W(Z1, Z28, Z29, Z27, Z31, Z30, Z26, Z1)
-	REDUCE1Q(Z24, Z1, Z29)
-	VPBROADCASTD 8(CX), Z27
-	ADD(Z2, Z27, Z24, Z31, Z2)
-	MUL_W(Z2, Z2, Z26, Z29, Z28, Z31, Z27, Z30)
-	MUL_W(Z2, Z30, Z26, Z29, Z28, Z31, Z27, Z2)
-	REDUCE1Q(Z24, Z2, Z26)
-	VPBROADCASTD 12(CX), Z29
-	ADD(Z3, Z29, Z24, Z28, Z3)
-	MUL_W(Z3, Z3, Z27, Z26, Z30, Z28, Z29, Z31)
-	MUL_W(Z3, Z31, Z27, Z26, Z30, Z28, Z29, Z3)
-	REDUCE1Q(Z24, Z3, Z27)
-	VPBROADCASTD 16(CX), Z26
-	ADD(Z4, Z26, Z24, Z30, Z4)
-	MUL_W(Z4, Z4, Z29, Z27, Z31, Z30, Z26, Z28)
-	MUL_W(Z4, Z28, Z29, Z27, Z31, Z30, Z26, Z4)
-	REDUCE1Q(Z24, Z4, Z29)
-	VPBROADCASTD 20(CX), Z27
-	ADD(Z5, Z27, Z24, Z31, Z5)
-	MUL_W(Z5, Z5, Z26, Z29, Z28, Z31, Z27, Z30)
-	MUL_W(Z5, Z30, Z26, Z29, Z28, Z31, Z27, Z5)
-	REDUCE1Q(Z24, Z5, Z26)
-	VPBROADCASTD 24(CX), Z29
-	ADD(Z6, Z29, Z24, Z28, Z6)
-	MUL_W(Z6, Z6, Z27, Z26, Z30, Z28, Z29, Z31)
-	MUL_W(Z6, Z31, Z27, Z26, Z30, Z28, Z29, Z6)
-	REDUCE1Q(Z24, Z6, Z27)
-	VPBROADCASTD 28(CX), Z26
-	ADD(Z7, Z26, Z24, Z30, Z7)
-	MUL_W(Z7, Z7, Z29, Z27, Z31, Z30, Z26, Z28)
-	MUL_W(Z7, Z28, Z29, Z27, Z31, Z30, Z26, Z7)
-	REDUCE1Q(Z24, Z7, Z29)
-	VPBROADCASTD 32(CX), Z27
-	ADD(Z8, Z27, Z24, Z31, Z8)
-	MUL_W(Z8, Z8, Z26, Z29, Z28, Z31, Z27, Z30)
-	MUL_W(Z8, Z30, Z26, Z29, Z28, Z31, Z27, Z8)
-	REDUCE1Q(Z24, Z8, Z26)
-	VPBROADCASTD 36(CX), Z29
-	ADD(Z9, Z29, Z24, Z28, Z9)
-	MUL_W(Z9, Z9, Z27, Z26, Z30, Z28, Z29, Z31)
-	MUL_W(Z9, Z31, Z27, Z26, Z30, Z28, Z29, Z9)
-	REDUCE1Q(Z24, Z9, Z27)
-	VPBROADCASTD 40(CX), Z26
-	ADD(Z10, Z26, Z24, Z30, Z10)
-	MUL_W(Z10, Z10, Z29, Z27, Z31, Z30, Z26, Z28)
-	MUL_W(Z10, Z28, Z29, Z27, Z31, Z30, Z26, Z10)
-	REDUCE1Q(Z24, Z10, Z29)
-	VPBROADCASTD 44(CX), Z27
-	ADD(Z11, Z27, Z24, Z31, Z11)
-	MUL_W(Z11, Z11, Z26, Z29, Z28, Z31, Z27, Z30)
-	MUL_W(Z11, Z30, Z26, Z29, Z28, Z31, Z27, Z11)
-	REDUCE1Q(Z24, Z11, Z26)
-	VPBROADCASTD 48(CX), Z29
-	ADD(Z12, Z29, Z24, Z28, Z12)
-	MUL_W(Z12, Z12, Z27, Z26, Z30, Z28, Z29, Z31)
-	MUL_W(Z12, Z31, Z27, Z26, Z30, Z28, Z29, Z12)
-	REDUCE1Q(Z24, Z12, Z27)
-	VPBROADCASTD 52(CX), Z26
-	ADD(Z13, Z26, Z24, Z30, Z13)
-	MUL_W(Z13, Z13, Z29, Z27, Z31, Z30, Z26, Z28)
-	MUL_W(Z13, Z28, Z29, Z27, Z31, Z30, Z26, Z13)
-	REDUCE1Q(Z24, Z13, Z29)
-	VPBROADCASTD 56(CX), Z27
-	ADD(Z14, Z27, Z24, Z31, Z14)
-	MUL_W(Z14, Z14, Z26, Z29, Z28, Z31, Z27, Z30)
-	MUL_W(Z14, Z30, Z26, Z29, Z28, Z31, Z27, Z14)
-	REDUCE1Q(Z24, Z14, Z26)
-	VPBROADCASTD 60(CX), Z29
-	ADD(Z15, Z29, Z24, Z28, Z15)
-	MUL_W(Z15, Z15, Z27, Z26, Z30, Z28, Z29, Z31)
-	MUL_W(Z15, Z31, Z27, Z26, Z30, Z28, Z29, Z15)
-	REDUCE1Q(Z24, Z15, Z27)
-	VPBROADCASTD 64(CX), Z26
-	ADD(Z16, Z26, Z24, Z30, Z16)
-	MUL_W(Z16, Z16, Z29, Z27, Z31, Z30, Z26, Z28)
-	MUL_W(Z16, Z28, Z29, Z27, Z31, Z30, Z26, Z16)
-	REDUCE1Q(Z24, Z16, Z29)
-	VPBROADCASTD 68(CX), Z27
-	ADD(Z17, Z27, Z24, Z31, Z17)
-	MUL_W(Z17, Z17, Z26, Z29, Z28, Z31, Z27, Z30)
-	MUL_W(Z17, Z30, Z26, Z29, Z28, Z31, Z27, Z17)
-	REDUCE1Q(Z24, Z17, Z26)
-	VPBROADCASTD 72(CX), Z29
-	ADD(Z18, Z29, Z24, Z28, Z18)
-	MUL_W(Z18, Z18, Z27, Z26, Z30, Z28, Z29, Z31)
-	MUL_W(Z18, Z31, Z27, Z26, Z30, Z28, Z29, Z18)
-	REDUCE1Q(Z24, Z18, Z27)
-	VPBROADCASTD 76(CX), Z26
-	ADD(Z19, Z26, Z24, Z30, Z19)
-	MUL_W(Z19, Z19, Z29, Z27, Z31, Z30, Z26, Z28)
-	MUL_W(Z19, Z28, Z29, Z27, Z31, Z30, Z26, Z19)
-	REDUCE1Q(Z24, Z19, Z29)
-	VPBROADCASTD 80(CX), Z27
-	ADD(Z20, Z27, Z24, Z31, Z20)
-	MUL_W(Z20, Z20, Z26, Z29, Z28, Z31, Z27, Z30)
-	MUL_W(Z20, Z30, Z26, Z29, Z28, Z31, Z27, Z20)
-	REDUCE1Q(Z24, Z20, Z26)
-	VPBROADCASTD 84(CX), Z29
-	ADD(Z21, Z29, Z24, Z28, Z21)
-	MUL_W(Z21, Z21, Z27, Z26, Z30, Z28, Z29, Z31)
-	MUL_W(Z21, Z31, Z27, Z26, Z30, Z28, Z29, Z21)
-	REDUCE1Q(Z24, Z21, Z27)
-	VPBROADCASTD 88(CX), Z26
-	ADD(Z22, Z26, Z24, Z30, Z22)
-	MUL_W(Z22, Z22, Z29, Z27, Z31, Z30, Z26, Z28)
-	MUL_W(Z22, Z28, Z29, Z27, Z31, Z30, Z26, Z22)
-	REDUCE1Q(Z24, Z22, Z29)
-	VPBROADCASTD 92(CX), Z27
-	ADD(Z23, Z27, Z24, Z31, Z23)
-	MUL_W(Z23, Z23, Z26, Z29, Z28, Z31, Z27, Z30)
-	MUL_W(Z23, Z30, Z26, Z29, Z28, Z31, Z27, Z23)
-	REDUCE1Q(Z24, Z23, Z26)
-	ADD(Z0, Z1, Z24, Z30, Z29)
-	ADD(Z2, Z3, Z24, Z30, Z28)
-	ADD(Z29, Z28, Z24, Z30, Z31)
-	ADD(Z31, Z1, Z24, Z30, Z27)
-	ADD(Z31, Z3, Z24, Z30, Z26)
-	DOUBLE(Z0, Z24, Z30, Z3)
-	ADD(Z3, Z26, Z24, Z30, Z3)
-	DOUBLE(Z2, Z24, Z30, Z1)
-	ADD(Z1, Z27, Z24, Z30, Z1)
-	ADD(Z29, Z27, Z24, Z30, Z0)
-	ADD(Z28, Z26, Z24, Z30, Z2)
-	ADD(Z4, Z5, Z24, Z30, Z29)
-	ADD(Z6, Z7, Z24, Z30, Z28)
-	ADD(Z29, Z28, Z24, Z30, Z31)
-	ADD(Z31, Z5, Z24, Z30, Z27)
-	ADD(Z31, Z7, Z24, Z30, Z26)
-	DOUBLE(Z4, Z24, Z30, Z7)
-	ADD(Z7, Z26, Z24, Z30, Z7)
-	DOUBLE(Z6, Z24, Z30, Z5)
-	ADD(Z5, Z27, Z24, Z30, Z5)
-	ADD(Z29, Z27, Z24, Z30, Z4)
-	ADD(Z28, Z26, Z24, Z30, Z6)
-	ADD(Z8, Z9, Z24, Z30, Z29)
-	ADD(Z10, Z11, Z24, Z30, Z28)
-	ADD(Z29, Z28, Z24, Z30, Z31)
-	ADD(Z31, Z9, Z24, Z30, Z27)
-	ADD(Z31, Z11, Z24, Z30, Z26)
-	DOUBLE(Z8, Z24, Z30, Z11)
-	ADD(Z11, Z26, Z24, Z30, Z11)
-	DOUBLE(Z10, Z24, Z30, Z9)
-	ADD(Z9, Z27, Z24, Z30, Z9)
-	ADD(Z29, Z27, Z24, Z30, Z8)
-	ADD(Z28, Z26, Z24, Z30, Z10)
-	ADD(Z12, Z13, Z24, Z30, Z29)
-	ADD(Z14, Z15, Z24, Z30, Z28)
-	ADD(Z29, Z28, Z24, Z30, Z31)
-	ADD(Z31, Z13, Z24, Z30, Z27)
-	ADD(Z31, Z15, Z24, Z30, Z26)
-	DOUBLE(Z12, Z24, Z30, Z15)
-	ADD(Z15, Z26, Z24, Z30, Z15)
-	DOUBLE(Z14, Z24, Z30, Z13)
-	ADD(Z13, Z27, Z24, Z30, Z13)
-	ADD(Z29, Z27, Z24, Z30, Z12)
-	ADD(Z28, Z26, Z24, Z30, Z14)
-	ADD(Z16, Z17, Z24, Z30, Z29)
-	ADD(Z18, Z19, Z24, Z30, Z28)
-	ADD(Z29, Z28, Z24, Z30, Z31)
-	ADD(Z31, Z17, Z24, Z30, Z27)
-	ADD(Z31, Z19, Z24, Z30, Z26)
-	DOUBLE(Z16, Z24, Z30, Z19)
-	ADD(Z19, Z26, Z24, Z30, Z19)
-	DOUBLE(Z18, Z24, Z30, Z17)
-	ADD(Z17, Z27, Z24, Z30, Z17)
-	ADD(Z29, Z27, Z24, Z30, Z16)
-	ADD(Z28, Z26, Z24, Z30, Z18)
-	ADD(Z20, Z21, Z24, Z30, Z29)
-	ADD(Z22, Z23, Z24, Z30, Z28)
-	ADD(Z29, Z28, Z24, Z30, Z31)
-	ADD(Z31, Z21, Z24, Z30, Z27)
-	ADD(Z31, Z23, Z24, Z30, Z26)
-	DOUBLE(Z20, Z24, Z30, Z23)
-	ADD(Z23, Z26, Z24, Z30, Z23)
-	DOUBLE(Z22, Z24, Z30, Z21)
-	ADD(Z21, Z27, Z24, Z30, Z21)
-	ADD(Z29, Z27, Z24, Z30, Z20)
-	ADD(Z28, Z26, Z24, Z30, Z22)
-	ADD(Z0, Z4, Z24, Z27, Z30)
-	ADD(Z1, Z5, Z24, Z26, Z29)
-	ADD(Z2, Z6, Z24, Z27, Z28)
-	ADD(Z3, Z7, Z24, Z26, Z31)
-	ADD(Z8, Z30, Z24, Z27, Z30)
-	ADD(Z9, Z29, Z24, Z26, Z29)
-	ADD(Z10, Z28, Z24, Z27, Z28)
-	ADD(Z11, Z31, Z24, Z26, Z31)
-	ADD(Z12, Z30, Z24, Z27, Z30)
-	ADD(Z13, Z29, Z24, Z26, Z29)
-	ADD(Z14, Z28, Z24, Z27, Z28)
-	ADD(Z15, Z31, Z24, Z26, Z31)
-	ADD(Z16, Z30, Z24, Z27, Z30)
-	ADD(Z17, Z29, Z24, Z26, Z29)
-	ADD(Z18, Z28, Z24, Z27, Z28)
-	ADD(Z19, Z31, Z24, Z26, Z31)
-	ADD(Z20, Z30, Z24, Z27, Z30)
-	ADD(Z21, Z29, Z24, Z26, Z29)
-	ADD(Z22, Z28, Z24, Z27, Z28)
-	ADD(Z23, Z31, Z24, Z26, Z31)
-	ADD(Z0, Z30, Z24, Z27, Z0)
-	ADD(Z1, Z29, Z24, Z26, Z1)
-	ADD(Z2, Z28, Z24, Z27, Z2)
-	ADD(Z3, Z31, Z24, Z26, Z3)
-	ADD(Z4, Z30, Z24, Z27, Z4)
-	ADD(Z5, Z29, Z24, Z26, Z5)
-	ADD(Z6, Z28, Z24, Z27, Z6)
-	ADD(Z7, Z31, Z24, Z26, Z7)
-	ADD(Z8, Z30, Z24, Z27, Z8)
-	ADD(Z9, Z29, Z24, Z26, Z9)
-	ADD(Z10, Z28, Z24, Z27, Z10)
-	ADD(Z11, Z31, Z24, Z26, Z11)
-	ADD(Z12, Z30, Z24, Z27, Z12)
-	ADD(Z13, Z29, Z24, Z26, Z13)
-	ADD(Z14, Z28, Z24, Z27, Z14)
-	ADD(Z15, Z31, Z24, Z26, Z15)
-	ADD(Z16, Z30, Z24, Z27, Z16)
-	ADD(Z17, Z29, Z24, Z26, Z17)
-	ADD(Z18, Z28, Z24, Z27, Z18)
-	ADD(Z19, Z31, Z24, Z26, Z19)
-	ADD(Z20, Z30, Z24, Z27, Z20)
-	ADD(Z21, Z29, Z24, Z26, Z21)
-	ADD(Z22, Z28, Z24, Z27, Z22)
-	ADD(Z23, Z31, Z24, Z26, Z23)
-	ADDQ         $24, R14
-	JMP          loop_7
-
-done_8:
-	VMOVDQU32 Z0, 0(R15)
-	VMOVDQU32 Z1, 64(R15)
-	VMOVDQU32 Z2, 128(R15)
-	VMOVDQU32 Z3, 192(R15)
-	VMOVDQU32 Z4, 256(R15)
-	VMOVDQU32 Z5, 320(R15)
-	VMOVDQU32 Z6, 384(R15)
-	VMOVDQU32 Z7, 448(R15)
-	VMOVDQU32 Z8, 512(R15)
-	VMOVDQU32 Z9, 576(R15)
-	VMOVDQU32 Z10, 640(R15)
-	VMOVDQU32 Z11, 704(R15)
-	VMOVDQU32 Z12, 768(R15)
-	VMOVDQU32 Z13, 832(R15)
-	VMOVDQU32 Z14, 896(R15)
-	VMOVDQU32 Z15, 960(R15)
-	VMOVDQU32 Z16, 1024(R15)
-	VMOVDQU32 Z17, 1088(R15)
-	VMOVDQU32 Z18, 1152(R15)
-	VMOVDQU32 Z19, 1216(R15)
-	VMOVDQU32 Z20, 1280(R15)
-	VMOVDQU32 Z21, 1344(R15)
-	VMOVDQU32 Z22, 1408(R15)
-	VMOVDQU32 Z23, 1472(R15)
-	RET
-
-TEXT ·permutation16_avx512(SB), NOSPLIT, $0-48
-	MOVQ         $0x0000000000005555, AX
-	KMOVD        AX, K3
-	MOVQ         $1, AX
-	KMOVQ        AX, K2
-	MOVD         $const_q, AX
-	VPBROADCASTD AX, Z0
-	MOVD         $const_qInvNeg, AX
-	VPBROADCASTD AX, Z1
-	MOVQ         input+0(FP), R15
-	MOVQ         roundKeys+24(FP), R14
-	VMOVDQU32    0(R15), Z2
-	MOVQ         ·diag16+0(SB), CX
-	VMOVDQU32    0(CX), Z18
-	VPSRLQ       $32, Z18, Z19
-
-#define MAT_MUL_EXTERNAL_16() \
-	MAT_MUL_M4(Z2, Z6, Z7, Z8, Z0, Z11) \
-	VEXTRACTI64X4 $1, Z2, Y16           \
-	ADD(Y16, Y2, Y0, Y11, Y16)          \
-	VSHUFF64X2    $1, Y16, Y16, Y17     \
-	ADD(Y16, Y17, Y0, Y11, Y16)         \
-	VINSERTI64X4  $1, Y16, Z16, Z16     \
-	ADD(Z2, Z16, Z0, Z11, Z2)           \
-
-#define SBOX_FULL_16() \
-	MULD(Z2, Z2, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z8) \
-	MULD(Z2, Z8, Z12, Z13, Z6, Z7, Z14, Z15, Z0, Z1, Z2) \
-	REDUCE1Q(Z0, Z2, Z15)                                \
-
-#define SUM_STATE_16() \
-	VEXTRACTI64X4 $1, Z2, Y16                   \
-	ADD(Y16, Y10, Y0, Y11, Y16)                 \
-	VSHUFF64X2    $1, Y16, Y16, Y17             \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VPSHUFD       $0x000000000000004e, Y16, Y17 \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VPSHUFD       $0x00000000000000b1, Y16, Y17 \
-	ADD(Y16, Y17, Y0, Y11, Y16)                 \
-	VINSERTI64X4  $1, Y16, Z16, Z16             \
-
-#define FULL_ROUND_16() \
-	VMOVDQU32 0(BX), Z4      \
-	ADD(Z2, Z4, Z0, Z11, Z2) \
-	SBOX_FULL_16()           \
-	MAT_MUL_EXTERNAL_16()    \
-
-	MAT_MUL_EXTERNAL_16()
-	MOVQ 0(R14), BX
-	FULL_ROUND_16()
-	MOVQ 24(R14), BX
-	FULL_ROUND_16()
-	MOVQ 48(R14), BX
-	FULL_ROUND_16()
-
-	// loop over the partial rounds
-	MOVQ $0x0000000000000015, SI // nb partial rounds --> 21
-	MOVQ R14, DI
-	ADDQ $0x0000000000000048, DI
-
-loop_9:
-	TESTQ     SI, SI
-	JEQ       done_10
-	DECQ      SI
-	MOVQ      0(DI), BX
-	VMOVD     0(BX), X4
-	VMOVDQA32 Z2, Z10
-	ADD(X10, X4, X0, X14, X5)
-	SBOX_PARTIAL()
-	VPBLENDMD Z5, Z10, K2, Z10
-	VPSRLQ    $32, Z2, Z12
-	VPMULUDQ  Z12, Z19, Z8
-	VPMULUDQ  Z8, Z1, Z15
-	VPMULUDQ  Z15, Z0, Z15
-	VPADDQ    Z8, Z15, Z8
-	SUM_STATE_16()
-	VPMULUDQ  Z10, Z18, Z6
-	VPMULUDQ  Z6, Z1, Z14
-	VPMULUDQ  Z14, Z0, Z14
-	VPADDQ    Z6, Z14, Z6
-	VMOVSHDUP Z6, K3, Z8
-	VPSUBD    Z0, Z8, Z11
-	VPMINUD   Z8, Z11, Z2
-	ADD(Z2, Z16, Z0, Z11, Z2)
-	ADDQ      $24, DI
-	JMP       loop_9
-
-done_10:
-	MOVQ      576(R14), BX
-	FULL_ROUND_16()
-	MOVQ      600(R14), BX
-	FULL_ROUND_16()
-	MOVQ      624(R14), BX
-	FULL_ROUND_16()
-	VMOVDQU32 Z2, 0(R15)
-	RET
+// assembly disabled for bazel compatibility
diff -ruN a/field/koalabear/sis/sis_amd64.s b/field/koalabear/sis/sis_amd64.s
--- a/field/koalabear/sis/sis_amd64.s
+++ b/field/koalabear/sis/sis_amd64.s
@@ -1,693 +1,1 @@
-//go:build !purego
-
-// Code generated by gnark-crypto/generator. DO NOT EDIT.
-// Refer to the generator for more documentation.
-// Some sub-functions are derived from Plonky3:
-// https://github.com/Plonky3/Plonky3/blob/36e619f3c6526ee86e2e5639a24b3224e1c1700f/monty-31/src/x86_64_avx512/packing.rs#L319
-
-#include "textflag.h"
-#include "funcdata.h"
-#include "go_asm.h"
-
-#define BUTTERFLYD1Q(in0, in1, in2, in3, in4) \
-	VPADDD  in0, in1, in3 \
-	VPSUBD  in1, in0, in1 \
-	VPSUBD  in2, in3, in0 \
-	VPMINUD in3, in0, in0 \
-	VPADDD  in2, in1, in4 \
-	VPMINUD in4, in1, in1 \
-
-#define BUTTERFLYD2Q(in0, in1, in2, in3, in4) \
-	VPSUBD  in1, in0, in4 \
-	VPADDD  in0, in1, in3 \
-	VPADDD  in2, in4, in1 \
-	VPSUBD  in2, in3, in0 \
-	VPMINUD in3, in0, in0 \
-
-#define BUTTERFLYD2Q2Q(in0, in1, in2, in3) \
-	VPSUBD in1, in0, in3 \
-	VPADDD in0, in1, in0 \
-	VPADDD in2, in3, in1 \
-
-#define MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9) \
-	VPSRLQ    $32, in0, in2 \
-	VPSRLQ    $32, in1, in3 \
-	VPMULUDQ  in0, in1, in4 \
-	VPMULUDQ  in2, in3, in5 \
-	VPMULUDQ  in4, in9, in6 \
-	VPMULUDQ  in5, in9, in7 \
-	VPMULUDQ  in6, in8, in6 \
-	VPADDQ    in4, in6, in4 \
-	VPMULUDQ  in7, in8, in7 \
-	VPADDQ    in5, in7, in5 \
-	VMOVSHDUP in4, K3, in5  \
-	VPSUBD    in8, in5, in7 \
-	VPMINUD   in5, in7, in0 \
-
-#define PERMUTE8X8(in0, in1, in2) \
-	VSHUFI64X2 $0x000000000000004e, in1, in0, in2 \
-	VPBLENDMQ  in0, in2, K1, in0                  \
-	VPBLENDMQ  in2, in1, K1, in1                  \
-
-#define PERMUTE4X4(in0, in1, in2, in3) \
-	VMOVDQA64 in2, in3          \
-	VPERMI2Q  in1, in0, in3     \
-	VPBLENDMQ in0, in3, K2, in0 \
-	VPBLENDMQ in3, in1, K2, in1 \
-
-#define PERMUTE2X2(in0, in1, in2) \
-	VSHUFPD   $0x0000000000000055, in1, in0, in2 \
-	VPBLENDMQ in0, in2, K3, in0                  \
-	VPBLENDMQ in2, in1, K3, in1                  \
-
-#define PERMUTE1X1(in0, in1, in2) \
-	VPSHRDQ   $32, in1, in0, in2 \
-	VPBLENDMD in0, in2, K3, in0  \
-	VPBLENDMD in2, in1, K3, in1  \
-
-#define LOAD_Q(in0, in1) \
-	MOVD         $const_q, AX       \
-	VPBROADCASTD AX, in0            \
-	MOVD         $const_qInvNeg, AX \
-	VPBROADCASTD AX, in1            \
-
-#define LOAD_MASKS() \
-	MOVQ  $0x0000000000000f0f, AX \
-	KMOVQ AX, K1                  \
-	MOVQ  $0x0000000000000033, AX \
-	KMOVQ AX, K2                  \
-	MOVQ  $0x0000000000005555, AX \
-	KMOVD AX, K3                  \
-
-#define BUTTERFLY_MULD(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11, in12, in13, in14) \
-BUTTERFLYD2Q(in0, in1, in2, in3, in4)                       \
-MULD(in5, in6, in7, in8, in9, in10, in11, in12, in13, in14) \
-
-TEXT ·sis512_16_avx512(SB), $1024-120
-	// refer to the code generator for comments and documentation.
-	LOAD_Q(Z0, Z1)
-	LOAD_MASKS()
-	MOVQ k256+0(FP), R15
-	MOVQ cosets+24(FP), CX
-	MOVQ twiddles+48(FP), SI
-	MOVQ rag+72(FP), R8
-	MOVQ res+96(FP), R9
-	MOVQ 0(SI), DI               // twiddles[0]
-	MOVQ R15, DX
-	MOVQ CX, BX
-	ADDQ $0x0000000000000200, DX
-	ADDQ $0x0000000000000400, BX
-
-#define FROMMONTGOMERY(in0, in1, in2, in3, in4, in5, in6, in7, in8, in9, in10, in11) \
-	VPSRLQ    $32, in0, in3     \
-	VPSRLQ    $32, in1, in7     \
-	VPMULUDQ  in0, in11, in4    \
-	VPMULUDQ  in3, in11, in5    \
-	VPMULUDQ  in1, in11, in8    \
-	VPMULUDQ  in7, in11, in9    \
-	VPMULUDQ  in4, in10, in4    \
-	VPMULUDQ  in8, in10, in8    \
-	VPMULUDQ  in5, in10, in5    \
-	VPMULUDQ  in9, in10, in9    \
-	VPANDD.Z  in0, in0, K3, in2 \
-	VPANDD.Z  in1, in1, K3, in6 \
-	VPADDQ    in2, in4, in2     \
-	VPADDQ    in6, in8, in6     \
-	VPADDQ    in3, in5, in3     \
-	VPADDQ    in7, in9, in7     \
-	VMOVSHDUP in6, K3, in7      \
-	VMOVSHDUP in2, K3, in3      \
-	VPSUBD    in10, in3, in5    \
-	VPSUBD    in10, in7, in9    \
-	VPMINUD   in3, in5, in0     \
-	VPMINUD   in7, in9, in1     \
-
-	VMOVDQU32     0(R15), Z16
-	VMOVDQU32     0(DX), Z14
-	FROMMONTGOMERY(Z16, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z16, Y17
-	VPMOVZXWD     Y16, Z16
-	VPMOVZXWD     Y17, Z17
-	VMOVDQU32     0(CX), Z12
-	MULD(Z16, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     64(CX), Z13
-	MULD(Z17, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     0(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     64(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z16, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z17, Z15, Z0, Z5, Z7)
-	VMOVDQU32     0(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     64(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 0(SP)
-	VMOVDQU32     Z15, 64(SP)
-	VMOVDQU32     64(R15), Z18
-	VMOVDQU32     64(DX), Z14
-	FROMMONTGOMERY(Z18, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z18, Y19
-	VPMOVZXWD     Y18, Z18
-	VPMOVZXWD     Y19, Z19
-	VMOVDQU32     128(CX), Z12
-	MULD(Z18, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     192(CX), Z13
-	MULD(Z19, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     128(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     192(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z18, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z19, Z15, Z0, Z5, Z7)
-	VMOVDQU32     128(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     192(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 128(SP)
-	VMOVDQU32     Z15, 192(SP)
-	VMOVDQU32     128(R15), Z20
-	VMOVDQU32     128(DX), Z14
-	FROMMONTGOMERY(Z20, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z20, Y21
-	VPMOVZXWD     Y20, Z20
-	VPMOVZXWD     Y21, Z21
-	VMOVDQU32     256(CX), Z12
-	MULD(Z20, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     320(CX), Z13
-	MULD(Z21, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     256(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     320(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z20, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z21, Z15, Z0, Z5, Z7)
-	VMOVDQU32     256(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     320(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 256(SP)
-	VMOVDQU32     Z15, 320(SP)
-	VMOVDQU32     192(R15), Z22
-	VMOVDQU32     192(DX), Z14
-	FROMMONTGOMERY(Z22, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z22, Y23
-	VPMOVZXWD     Y22, Z22
-	VPMOVZXWD     Y23, Z23
-	VMOVDQU32     384(CX), Z12
-	MULD(Z22, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     448(CX), Z13
-	MULD(Z23, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     384(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     448(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z22, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z23, Z15, Z0, Z5, Z7)
-	VMOVDQU32     384(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     448(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 384(SP)
-	VMOVDQU32     Z15, 448(SP)
-	VMOVDQU32     256(R15), Z24
-	VMOVDQU32     256(DX), Z14
-	FROMMONTGOMERY(Z24, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z24, Y25
-	VPMOVZXWD     Y24, Z24
-	VPMOVZXWD     Y25, Z25
-	VMOVDQU32     512(CX), Z12
-	MULD(Z24, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     576(CX), Z13
-	MULD(Z25, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     512(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     576(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z24, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z25, Z15, Z0, Z5, Z7)
-	VMOVDQU32     512(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     576(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 512(SP)
-	VMOVDQU32     Z15, 576(SP)
-	VMOVDQU32     320(R15), Z26
-	VMOVDQU32     320(DX), Z14
-	FROMMONTGOMERY(Z26, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z26, Y27
-	VPMOVZXWD     Y26, Z26
-	VPMOVZXWD     Y27, Z27
-	VMOVDQU32     640(CX), Z12
-	MULD(Z26, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     704(CX), Z13
-	MULD(Z27, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     640(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     704(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z26, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z27, Z15, Z0, Z5, Z7)
-	VMOVDQU32     640(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     704(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 640(SP)
-	VMOVDQU32     Z15, 704(SP)
-	VMOVDQU32     384(R15), Z28
-	VMOVDQU32     384(DX), Z14
-	FROMMONTGOMERY(Z28, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z28, Y29
-	VPMOVZXWD     Y28, Z28
-	VPMOVZXWD     Y29, Z29
-	VMOVDQU32     768(CX), Z12
-	MULD(Z28, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     832(CX), Z13
-	MULD(Z29, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     768(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     832(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z28, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z29, Z15, Z0, Z5, Z7)
-	VMOVDQU32     768(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     832(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 768(SP)
-	VMOVDQU32     Z15, 832(SP)
-	VMOVDQU32     448(R15), Z30
-	VMOVDQU32     448(DX), Z14
-	FROMMONTGOMERY(Z30, Z14, Z4, Z5, Z6, Z7, Z8, Z9, Z10, Z11, Z0, Z1)
-	VEXTRACTI64X4 $1, Z30, Y31
-	VPMOVZXWD     Y30, Z30
-	VPMOVZXWD     Y31, Z31
-	VMOVDQU32     896(CX), Z12
-	MULD(Z30, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     960(CX), Z13
-	MULD(Z31, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VEXTRACTI64X4 $1, Z14, Y15
-	VPMOVZXWD     Y14, Z14
-	VPMOVZXWD     Y15, Z15
-	VMOVDQU32     896(BX), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     960(BX), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	BUTTERFLYD2Q(Z30, Z14, Z0, Z4, Z6)
-	BUTTERFLYD2Q(Z31, Z15, Z0, Z5, Z7)
-	VMOVDQU32     896(DI), Z12
-	MULD(Z14, Z12, Z2, Z3, Z4, Z5, Z6, Z7, Z0, Z1)
-	VMOVDQU32     960(DI), Z13
-	MULD(Z15, Z13, Z8, Z9, Z10, Z11, Z6, Z7, Z0, Z1)
-	VMOVDQU32     Z14, 896(SP)
-	VMOVDQU32     Z15, 960(SP)
-	ADDQ          $24, SI
-	MOVQ          SI, DI
-	MOVQ          $2, R10
-
-fft256_2:
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Z2
-	VMOVDQU32    64(R11), Z3
-	VMOVDQU32    128(R11), Z4
-	VMOVDQU32    192(R11), Z5
-	VMOVDQU32    256(R11), Z6
-	VMOVDQU32    320(R11), Z7
-	VMOVDQU32    384(R11), Z8
-	VMOVDQU32    448(R11), Z9
-	BUTTERFLYD2Q(Z16, Z24, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z17, Z25, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z18, Z26, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z19, Z27, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z20, Z28, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z21, Z29, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z22, Z30, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z23, Z31, Z0, Z14, Z11)
-	MULD(Z24, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z25, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z26, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z5, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z28, Z6, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z7, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z30, Z8, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z9, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	ADDQ         $24, SI
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Z2
-	VMOVDQU32    64(R11), Z3
-	VMOVDQU32    128(R11), Z4
-	VMOVDQU32    192(R11), Z5
-	BUTTERFLYD2Q(Z16, Z20, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z17, Z21, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z18, Z22, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z19, Z23, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z24, Z28, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z25, Z29, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z26, Z30, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z27, Z31, Z0, Z14, Z11)
-	MULD(Z20, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z21, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z22, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z5, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z28, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z30, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z5, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	ADDQ         $24, SI
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Z2
-	VMOVDQU32    64(R11), Z3
-	BUTTERFLYD2Q(Z16, Z18, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z17, Z19, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z20, Z22, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z21, Z23, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z24, Z26, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z25, Z27, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z28, Z30, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z29, Z31, Z0, Z14, Z11)
-	MULD(Z18, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z19, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z22, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z26, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z30, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	ADDQ         $24, SI
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Z2
-	BUTTERFLYD2Q(Z16, Z17, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z18, Z19, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z20, Z21, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z22, Z23, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z24, Z25, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z26, Z27, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z28, Z29, Z0, Z14, Z11)
-	BUTTERFLYD2Q(Z30, Z31, Z0, Z14, Z11)
-	MULD(Z17, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z19, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z21, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z25, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	ADDQ         $24, SI
-	MOVQ         ·vInterleaveIndices+0(SB), R12
-	VMOVDQU64    0(R12), Z6
-	MOVQ         0(SI), R11
-	VMOVDQU32    0(R11), Y2
-	VINSERTI64X4 $1, Y2, Z2, Z2
-	MOVQ         24(SI), R11
-	VMOVDQU32    0(R11), X3
-	VINSERTI64X2 $1, X3, Z3, Z3
-	VINSERTI64X2 $0x0000000000000002, X3, Z3, Z3
-	VINSERTI64X2 $0x0000000000000003, X3, Z3, Z3
-	MOVQ         48(SI), R11
-	VPBROADCASTD 0(R11), Z4
-	VPBROADCASTD 4(R11), Z5
-	VPBLENDMD    Z4, Z5, K3, Z4
-	PERMUTE8X8(Z16, Z17, Z10)
-	BUTTERFLYD2Q(Z16, Z17, Z0, Z14, Z11)
-	PERMUTE8X8(Z18, Z19, Z10)
-	BUTTERFLYD2Q(Z18, Z19, Z0, Z14, Z11)
-	PERMUTE8X8(Z20, Z21, Z10)
-	BUTTERFLYD2Q(Z20, Z21, Z0, Z14, Z11)
-	PERMUTE8X8(Z22, Z23, Z10)
-	BUTTERFLYD2Q(Z22, Z23, Z0, Z14, Z11)
-	PERMUTE8X8(Z24, Z25, Z10)
-	BUTTERFLYD2Q(Z24, Z25, Z0, Z14, Z11)
-	PERMUTE8X8(Z26, Z27, Z10)
-	BUTTERFLYD2Q(Z26, Z27, Z0, Z14, Z11)
-	PERMUTE8X8(Z28, Z29, Z10)
-	BUTTERFLYD2Q(Z28, Z29, Z0, Z14, Z11)
-	PERMUTE8X8(Z30, Z31, Z10)
-	BUTTERFLYD2Q(Z30, Z31, Z0, Z14, Z11)
-	MULD(Z17, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z19, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z21, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z25, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z2, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE4X4(Z16, Z17, Z6, Z10)
-	BUTTERFLYD2Q(Z16, Z17, Z0, Z14, Z11)
-	PERMUTE4X4(Z18, Z19, Z6, Z10)
-	BUTTERFLYD2Q(Z18, Z19, Z0, Z14, Z11)
-	PERMUTE4X4(Z20, Z21, Z6, Z10)
-	BUTTERFLYD2Q(Z20, Z21, Z0, Z14, Z11)
-	PERMUTE4X4(Z22, Z23, Z6, Z10)
-	BUTTERFLYD2Q(Z22, Z23, Z0, Z14, Z11)
-	PERMUTE4X4(Z24, Z25, Z6, Z10)
-	BUTTERFLYD2Q(Z24, Z25, Z0, Z14, Z11)
-	PERMUTE4X4(Z26, Z27, Z6, Z10)
-	BUTTERFLYD2Q(Z26, Z27, Z0, Z14, Z11)
-	PERMUTE4X4(Z28, Z29, Z6, Z10)
-	BUTTERFLYD2Q(Z28, Z29, Z0, Z14, Z11)
-	PERMUTE4X4(Z30, Z31, Z6, Z10)
-	BUTTERFLYD2Q(Z30, Z31, Z0, Z14, Z11)
-	MULD(Z17, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z19, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z21, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z23, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z25, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z27, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z29, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	MULD(Z31, Z3, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE2X2(Z16, Z17, Z10)
-	BUTTERFLYD2Q(Z16, Z17, Z0, Z14, Z11)
-	PERMUTE2X2(Z18, Z19, Z10)
-	BUTTERFLYD2Q(Z18, Z19, Z0, Z14, Z11)
-	PERMUTE2X2(Z20, Z21, Z10)
-	BUTTERFLYD2Q(Z20, Z21, Z0, Z14, Z11)
-	PERMUTE2X2(Z22, Z23, Z10)
-	BUTTERFLYD2Q(Z22, Z23, Z0, Z14, Z11)
-	PERMUTE2X2(Z24, Z25, Z10)
-	BUTTERFLYD2Q(Z24, Z25, Z0, Z14, Z11)
-	PERMUTE2X2(Z26, Z27, Z10)
-	BUTTERFLYD2Q(Z26, Z27, Z0, Z14, Z11)
-	PERMUTE2X2(Z28, Z29, Z10)
-	BUTTERFLYD2Q(Z28, Z29, Z0, Z14, Z11)
-	PERMUTE2X2(Z30, Z31, Z10)
-	BUTTERFLYD2Q(Z30, Z31, Z0, Z14, Z11)
-	MULD(Z17, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z16, Z17, Z10)
-	MULD(Z19, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z18, Z19, Z10)
-	MULD(Z21, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z20, Z21, Z10)
-	MULD(Z23, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z22, Z23, Z10)
-	MULD(Z25, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z24, Z25, Z10)
-	MULD(Z27, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z26, Z27, Z10)
-	MULD(Z29, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z28, Z29, Z10)
-	MULD(Z31, Z4, Z13, Z15, Z10, Z11, Z12, Z14, Z0, Z1)
-	PERMUTE1X1(Z30, Z31, Z10)
-	BUTTERFLYD2Q2Q(Z16, Z17, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z18, Z19, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z20, Z21, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z22, Z23, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z24, Z25, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z26, Z27, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z28, Z29, Z0, Z10)
-	BUTTERFLYD2Q2Q(Z30, Z31, Z0, Z10)
-	VMOVDQU32    0(R8), Z7
-	MULD(Z16, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    64(R8), Z8
-	MULD(Z17, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    128(R8), Z7
-	MULD(Z18, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    192(R8), Z8
-	MULD(Z19, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    256(R8), Z7
-	MULD(Z20, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    320(R8), Z8
-	MULD(Z21, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    384(R8), Z7
-	MULD(Z22, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    448(R8), Z8
-	MULD(Z23, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    512(R8), Z7
-	MULD(Z24, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    576(R8), Z8
-	MULD(Z25, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    640(R8), Z7
-	MULD(Z26, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    704(R8), Z8
-	MULD(Z27, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    768(R8), Z7
-	MULD(Z28, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    832(R8), Z8
-	MULD(Z29, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VMOVDQU32    896(R8), Z7
-	MULD(Z30, Z7, Z9, Z2, Z3, Z4, Z5, Z10, Z0, Z1)
-	VMOVDQU32    960(R8), Z8
-	MULD(Z31, Z8, Z11, Z13, Z15, Z12, Z5, Z10, Z0, Z1)
-	VPADDD       0(R9), Z16, Z16
-	VPADDD       64(R9), Z17, Z17
-	VPSUBD       Z0, Z16, Z5
-	VPSUBD       Z0, Z17, Z10
-	VPMINUD      Z5, Z16, Z16
-	VPMINUD      Z10, Z17, Z17
-	VMOVDQU32    Z16, 0(R9)
-	VMOVDQU32    Z17, 64(R9)
-	VPADDD       128(R9), Z18, Z18
-	VPADDD       192(R9), Z19, Z19
-	VPSUBD       Z0, Z18, Z5
-	VPSUBD       Z0, Z19, Z10
-	VPMINUD      Z5, Z18, Z18
-	VPMINUD      Z10, Z19, Z19
-	VMOVDQU32    Z18, 128(R9)
-	VMOVDQU32    Z19, 192(R9)
-	VPADDD       256(R9), Z20, Z20
-	VPADDD       320(R9), Z21, Z21
-	VPSUBD       Z0, Z20, Z5
-	VPSUBD       Z0, Z21, Z10
-	VPMINUD      Z5, Z20, Z20
-	VPMINUD      Z10, Z21, Z21
-	VMOVDQU32    Z20, 256(R9)
-	VMOVDQU32    Z21, 320(R9)
-	VPADDD       384(R9), Z22, Z22
-	VPADDD       448(R9), Z23, Z23
-	VPSUBD       Z0, Z22, Z5
-	VPSUBD       Z0, Z23, Z10
-	VPMINUD      Z5, Z22, Z22
-	VPMINUD      Z10, Z23, Z23
-	VMOVDQU32    Z22, 384(R9)
-	VMOVDQU32    Z23, 448(R9)
-	VPADDD       512(R9), Z24, Z24
-	VPADDD       576(R9), Z25, Z25
-	VPSUBD       Z0, Z24, Z5
-	VPSUBD       Z0, Z25, Z10
-	VPMINUD      Z5, Z24, Z24
-	VPMINUD      Z10, Z25, Z25
-	VMOVDQU32    Z24, 512(R9)
-	VMOVDQU32    Z25, 576(R9)
-	VPADDD       640(R9), Z26, Z26
-	VPADDD       704(R9), Z27, Z27
-	VPSUBD       Z0, Z26, Z5
-	VPSUBD       Z0, Z27, Z10
-	VPMINUD      Z5, Z26, Z26
-	VPMINUD      Z10, Z27, Z27
-	VMOVDQU32    Z26, 640(R9)
-	VMOVDQU32    Z27, 704(R9)
-	VPADDD       768(R9), Z28, Z28
-	VPADDD       832(R9), Z29, Z29
-	VPSUBD       Z0, Z28, Z5
-	VPSUBD       Z0, Z29, Z10
-	VPMINUD      Z5, Z28, Z28
-	VPMINUD      Z10, Z29, Z29
-	VMOVDQU32    Z28, 768(R9)
-	VMOVDQU32    Z29, 832(R9)
-	VPADDD       896(R9), Z30, Z30
-	VPADDD       960(R9), Z31, Z31
-	VPSUBD       Z0, Z30, Z5
-	VPSUBD       Z0, Z31, Z10
-	VPMINUD      Z5, Z30, Z30
-	VPMINUD      Z10, Z31, Z31
-	VMOVDQU32    Z30, 896(R9)
-	VMOVDQU32    Z31, 960(R9)
-	DECQ         R10
-	TESTQ        R10, R10
-	JEQ          done_1
-	MOVQ         DI, SI
-	VMOVDQU32    0(SP), Z16
-	VMOVDQU32    64(SP), Z17
-	VMOVDQU32    128(SP), Z18
-	VMOVDQU32    192(SP), Z19
-	VMOVDQU32    256(SP), Z20
-	VMOVDQU32    320(SP), Z21
-	VMOVDQU32    384(SP), Z22
-	VMOVDQU32    448(SP), Z23
-	VMOVDQU32    512(SP), Z24
-	VMOVDQU32    576(SP), Z25
-	VMOVDQU32    640(SP), Z26
-	VMOVDQU32    704(SP), Z27
-	VMOVDQU32    768(SP), Z28
-	VMOVDQU32    832(SP), Z29
-	VMOVDQU32    896(SP), Z30
-	VMOVDQU32    960(SP), Z31
-	ADDQ         $0x0000000000000400, R8
-	ADDQ         $0x0000000000000400, R9
-	JMP          fft256_2
-
-done_1:
-	RET
-
-TEXT ·sisShuffle_avx512(SB), NOSPLIT, $0-24
-	MOVQ      a+0(FP), R15
-	MOVQ      a_len+8(FP), DX
-	SHRQ      $5, DX
-	LOAD_MASKS()
-	MOVQ      ·vInterleaveIndices+0(SB), CX
-	VMOVDQU64 0(CX), Z3
-
-loop_3:
-	TESTQ     DX, DX
-	JEQ       done_4
-	DECQ      DX
-	VMOVDQU32 0(R15), Z1  // load a[i]
-	VMOVDQU32 64(R15), Z2 // load a[i+16]
-	PERMUTE8X8(Z1, Z2, Z0)
-	PERMUTE4X4(Z1, Z2, Z3, Z0)
-	PERMUTE2X2(Z1, Z2, Z0)
-	PERMUTE1X1(Z1, Z2, Z0)
-	VMOVDQU32 Z1, 0(R15)  // store a[i]
-	VMOVDQU32 Z2, 64(R15) // store a[i+16]
-	ADDQ      $128, R15
-	JMP       loop_3
-
-done_4:
-	RET
-
-TEXT ·sisUnshuffle_avx512(SB), NOSPLIT, $0-24
-	MOVQ      a+0(FP), R15
-	MOVQ      a_len+8(FP), DX
-	SHRQ      $5, DX
-	LOAD_MASKS()
-	MOVQ      ·vInterleaveIndices+0(SB), CX
-	VMOVDQU64 0(CX), Z3
-
-loop_5:
-	TESTQ      DX, DX
-	JEQ        done_6
-	DECQ       DX
-	VMOVDQU32  0(R15), Z1  // load a[i]
-	VMOVDQU32  64(R15), Z2 // load a[i+16]
-	VPUNPCKLDQ Z2, Z1, Z0
-	VPUNPCKHDQ Z2, Z1, Z2
-	VMOVDQA32  Z0, Z1
-	PERMUTE4X4(Z1, Z2, Z3, Z0)
-	PERMUTE8X8(Z1, Z2, Z0)
-	VMOVDQU32  Z1, 0(R15)  // store a[i]
-	VMOVDQU32  Z2, 64(R15) // store a[i+16]
-	ADDQ       $128, R15
-	JMP        loop_5
-
-done_6:
-	RET
+// assembly disabled for bazel compatibility
